{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe2395b-1a5d-404d-aeaf-b7848d5d7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "import utils.data_processing_bronze_table\n",
    "import utils.data_processing_silver_table\n",
    "import utils.data_processing_gold_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e646ff59-d527-439b-a057-1b97a972ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/27 19:24:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/27 19:24:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"dev\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19bdc9c-06f1-45a0-a806-16f88347a3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== datamart/bronze/lms/ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/27 19:25:07 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\n",
      "Caused by: java.lang.RuntimeException: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv is not a Parquet file. Expected magic number at tail, but found [45, 48, 49, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
      "\t... 14 more\n",
      "25/09/27 19:25:07 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o26.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (88c6508da389 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\nCaused by: java.lang.RuntimeException: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv is not a Parquet file. Expected magic number at tail, but found [45, 48, 49, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\nCaused by: java.lang.RuntimeException: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv is not a Parquet file. Expected magic number at tail, but found [45, 48, 49, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdatamart/bronze/lms/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdatamart/silver/loan_daily/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdatamart/gold/label_store/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m ]:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===\u001b[39m\u001b[33m\"\u001b[39m, p, \u001b[33m\"\u001b[39m\u001b[33m===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mrows:\u001b[39m\u001b[33m\"\u001b[39m, df.count())\n\u001b[32m     10\u001b[39m     df.printSchema()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    533\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    535\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    536\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    542\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o26.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (88c6508da389 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\nCaused by: java.lang.RuntimeException: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv is not a Parquet file. Expected magic number at tail, but found [45, 48, 49, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\nCaused by: java.lang.RuntimeException: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv is not a Parquet file. Expected magic number at tail, but found [45, 48, 49, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "# Bronze / Silver / Gold row counts & schemas\n",
    "for p in [\n",
    "    \"datamart/bronze/lms/\",\n",
    "    \"datamart/silver/loan_daily/\",\n",
    "    \"datamart/gold/label_store/\",\n",
    "]:\n",
    "    print(\"\\n===\", p, \"===\")\n",
    "    df = spark.read.parquet(p)\n",
    "    print(\"rows:\", df.count())\n",
    "    df.printSchema()\n",
    "    df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8288bf-4ae7-4990-a0b4-2043c0d9a5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bronze_loan_daily_2023_01_01.csv', 'bronze_loan_daily_2023_02_01.csv', 'bronze_loan_daily_2023_03_01.csv', 'bronze_loan_daily_2023_04_01.csv', 'bronze_loan_daily_2023_05_01.csv', 'bronze_loan_daily_2023_06_01.csv', 'bronze_loan_daily_2023_07_01.csv', 'bronze_loan_daily_2023_08_01.csv', 'bronze_loan_daily_2023_09_01.csv', 'bronze_loan_daily_2023_10_01.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print([f for f in os.listdir(\"datamart/bronze/lms/\")][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7dbe8a4-99cf-4dba-bd28-1276e318d916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze rows: 104288\n",
      "root\n",
      " |-- loan_id: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- loan_start_date: date (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- installment_num: integer (nullable = true)\n",
      " |-- loan_amt: integer (nullable = true)\n",
      " |-- due_amt: double (nullable = true)\n",
      " |-- paid_amt: double (nullable = true)\n",
      " |-- overdue_amt: double (nullable = true)\n",
      " |-- balance: double (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbronze rows:\u001b[39m\u001b[33m\"\u001b[39m, bronze.count()); bronze.printSchema()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Silver (Parquet)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m silver = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatamart/silver/loan_daily/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msilver rows:\u001b[39m\u001b[33m\"\u001b[39m, silver.count()); silver.printSchema()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Gold (Parquet)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    533\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    535\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    536\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    542\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Bronze (CSV)\n",
    "bronze = (spark.read\n",
    "          .option(\"header\", \"true\")\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "          .csv(\"datamart/bronze/lms/\"))\n",
    "print(\"bronze rows:\", bronze.count()); bronze.printSchema()\n",
    "\n",
    "# Silver (Parquet)\n",
    "silver = spark.read.parquet(\"datamart/silver/loan_daily/\")\n",
    "print(\"silver rows:\", silver.count()); silver.printSchema()\n",
    "\n",
    "# Gold (Parquet)\n",
    "gold = spark.read.parquet(\"datamart/gold/label_store/\")\n",
    "print(\"gold rows:\", gold.count()); gold.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dc7b47c-8dab-432d-b57b-4db4755702bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver dir exists?  True\n",
      "Silver contents:  ['silver_loan_daily_2023_01_01.parquet', 'silver_loan_daily_2023_02_01.parquet', 'silver_loan_daily_2023_03_01.parquet', 'silver_loan_daily_2023_04_01.parquet', 'silver_loan_daily_2023_05_01.parquet', 'silver_loan_daily_2023_06_01.parquet', 'silver_loan_daily_2023_07_01.parquet', 'silver_loan_daily_2023_08_01.parquet', 'silver_loan_daily_2023_09_01.parquet', 'silver_loan_daily_2023_10_01.parquet']\n",
      "Gold dir exists?  True\n",
      "Gold contents:  ['gold_label_store_2023_01_01.parquet', 'gold_label_store_2023_02_01.parquet', 'gold_label_store_2023_03_01.parquet', 'gold_label_store_2023_04_01.parquet', 'gold_label_store_2023_05_01.parquet', 'gold_label_store_2023_06_01.parquet', 'gold_label_store_2023_07_01.parquet', 'gold_label_store_2023_08_01.parquet', 'gold_label_store_2023_09_01.parquet', 'gold_label_store_2023_10_01.parquet']\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "print(\"Silver dir exists? \", os.path.exists(\"datamart/silver/loan_daily/\"))\n",
    "print(\"Silver contents: \", os.listdir(\"datamart/silver/loan_daily/\")[:10])\n",
    "\n",
    "print(\"Gold dir exists? \", os.path.exists(\"datamart/gold/label_store/\"))\n",
    "print(\"Gold contents: \", os.listdir(\"datamart/gold/label_store/\")[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5cdc4e7-ba84-4043-bc18-7796764bf131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver rows: 104288\n",
      "root\n",
      " |-- loan_id: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- loan_start_date: date (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- installment_num: integer (nullable = true)\n",
      " |-- loan_amt: float (nullable = true)\n",
      " |-- due_amt: float (nullable = true)\n",
      " |-- paid_amt: float (nullable = true)\n",
      " |-- overdue_amt: float (nullable = true)\n",
      " |-- balance: float (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- mob: integer (nullable = true)\n",
      " |-- installments_missed: integer (nullable = true)\n",
      " |-- first_missed_date: date (nullable = true)\n",
      " |-- dpd: integer (nullable = true)\n",
      "\n",
      "gold rows: 8974\n",
      "root\n",
      " |-- loan_id: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- label_def: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# SILVER\n",
    "silver = spark.read.parquet(\"datamart/silver/loan_daily/*.parquet\")\n",
    "print(\"silver rows:\", silver.count()); silver.printSchema()\n",
    "\n",
    "# GOLD\n",
    "gold = spark.read.parquet(\"datamart/gold/label_store/*.parquet\")\n",
    "print(\"gold rows:\", gold.count()); gold.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e9851d-88b6-42d6-b647-2c5f5d7188f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2023-01-01'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv\")[\"snapshot_date\"].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5787973-326a-4796-84fb-6b7098d21f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fe_1</th>\n",
       "      <th>fe_2</th>\n",
       "      <th>fe_3</th>\n",
       "      <th>fe_4</th>\n",
       "      <th>fe_5</th>\n",
       "      <th>fe_6</th>\n",
       "      <th>fe_7</th>\n",
       "      <th>fe_8</th>\n",
       "      <th>fe_9</th>\n",
       "      <th>fe_10</th>\n",
       "      <th>...</th>\n",
       "      <th>fe_16</th>\n",
       "      <th>fe_17</th>\n",
       "      <th>fe_18</th>\n",
       "      <th>fe_19</th>\n",
       "      <th>fe_20</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>ingest_dt</th>\n",
       "      <th>snap_ym</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>118</td>\n",
       "      <td>80</td>\n",
       "      <td>121</td>\n",
       "      <td>55</td>\n",
       "      <td>193</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>-101</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>114</td>\n",
       "      <td>35</td>\n",
       "      <td>85</td>\n",
       "      <td>-73</td>\n",
       "      <td>76</td>\n",
       "      <td>CUS_0x1037</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>feature_clickstream.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-108</td>\n",
       "      <td>182</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>-56</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>-6</td>\n",
       "      <td>284</td>\n",
       "      <td>222</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>130</td>\n",
       "      <td>94</td>\n",
       "      <td>111</td>\n",
       "      <td>75</td>\n",
       "      <td>CUS_0x1069</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>feature_clickstream.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-13</td>\n",
       "      <td>8</td>\n",
       "      <td>87</td>\n",
       "      <td>166</td>\n",
       "      <td>214</td>\n",
       "      <td>-98</td>\n",
       "      <td>215</td>\n",
       "      <td>152</td>\n",
       "      <td>129</td>\n",
       "      <td>139</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>-130</td>\n",
       "      <td>354</td>\n",
       "      <td>17</td>\n",
       "      <td>302</td>\n",
       "      <td>CUS_0x114a</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>feature_clickstream.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-85</td>\n",
       "      <td>45</td>\n",
       "      <td>200</td>\n",
       "      <td>89</td>\n",
       "      <td>128</td>\n",
       "      <td>54</td>\n",
       "      <td>76</td>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>139</td>\n",
       "      <td>...</td>\n",
       "      <td>163</td>\n",
       "      <td>37</td>\n",
       "      <td>207</td>\n",
       "      <td>180</td>\n",
       "      <td>118</td>\n",
       "      <td>CUS_0x1184</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>feature_clickstream.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>120</td>\n",
       "      <td>226</td>\n",
       "      <td>-86</td>\n",
       "      <td>253</td>\n",
       "      <td>97</td>\n",
       "      <td>107</td>\n",
       "      <td>68</td>\n",
       "      <td>103</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>159</td>\n",
       "      <td>-26</td>\n",
       "      <td>104</td>\n",
       "      <td>118</td>\n",
       "      <td>184</td>\n",
       "      <td>CUS_0x1297</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>feature_clickstream.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fe_1  fe_2  fe_3  fe_4  fe_5  fe_6  fe_7  fe_8  fe_9  fe_10  ...  fe_16  \\\n",
       "0    63   118    80   121    55   193   111   112  -101     83  ...    114   \n",
       "1  -108   182   123     4   -56    27    25    -6   284    222  ...     35   \n",
       "2   -13     8    87   166   214   -98   215   152   129    139  ...    125   \n",
       "3   -85    45   200    89   128    54    76    51    61    139  ...    163   \n",
       "4    55   120   226   -86   253    97   107    68   103    126  ...    159   \n",
       "\n",
       "   fe_17  fe_18  fe_19  fe_20  Customer_ID  snapshot_date   ingest_dt  \\\n",
       "0     35     85    -73     76   CUS_0x1037     2023-01-01  2023-01-01   \n",
       "1    130     94    111     75   CUS_0x1069     2023-01-01  2023-01-01   \n",
       "2   -130    354     17    302   CUS_0x114a     2023-01-01  2023-01-01   \n",
       "3     37    207    180    118   CUS_0x1184     2023-01-01  2023-01-01   \n",
       "4    -26    104    118    184   CUS_0x1297     2023-01-01  2023-01-01   \n",
       "\n",
       "   snap_ym              source_file  \n",
       "0  2023-01  feature_clickstream.csv  \n",
       "1  2023-01  feature_clickstream.csv  \n",
       "2  2023-01  feature_clickstream.csv  \n",
       "3  2023-01  feature_clickstream.csv  \n",
       "4  2023-01  feature_clickstream.csv  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"datamart/bronze/features/clickstream/bronze_clickstream_2023_01_01.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ad4113-407b-46e6-bc1a-24e011dbb181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "len(glob.glob(\"datamart/bronze/features/clickstream/bronze_clickstream_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6be47b-a333-40cc-857e-14b31648c759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob(\"datamart/bronze/lms/bronze_loan_daily_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99983c7e-8a7e-4f4a-b556-31a3abf957a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickstream files: 24\n",
      "attributes files : 24\n",
      "financials files : 24\n",
      "lms files        : 24\n",
      "2023-06-01 snapshot unique: ['2023-06-01']\n"
     ]
    }
   ],
   "source": [
    "# quick_verify.py\n",
    "import glob, pandas as pd\n",
    "\n",
    "def count_files(pattern): return len(glob.glob(pattern))\n",
    "\n",
    "print(\"clickstream files:\", count_files(\"datamart/bronze/features/clickstream/bronze_clickstream_*.csv\"))\n",
    "print(\"attributes files :\", count_files(\"datamart/bronze/features/attributes/bronze_attributes_*.csv\"))\n",
    "print(\"financials files :\", count_files(\"datamart/bronze/features/financials/bronze_financials_*.csv\"))\n",
    "print(\"lms files        :\", count_files(\"datamart/bronze/lms/bronze_loan_daily_*.csv\"))\n",
    "\n",
    "# spot-check one month\n",
    "df = pd.read_csv(\"datamart/bronze/lms/bronze_loan_daily_2023_06_01.csv\")\n",
    "print(\"2023-06-01 snapshot unique:\", df[\"snapshot_date\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0faca526-b259-4e22-bc1b-84f7f6256b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/03 17:25:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- ingest_dt: date (nullable = true)\n",
      " |-- snap_ym: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n",
      "+-----------+----------+-------------+----------+-------------------+-----------------------+\n",
      "|customer_id|occupation|snapshot_date|ingest_dt |snap_ym            |source_file            |\n",
      "+-----------+----------+-------------+----------+-------------------+-----------------------+\n",
      "|CUS_0x1037 |ACCOUNTANT|2023-01-01   |2025-10-03|2023-01-01 00:00:00|features_attributes.csv|\n",
      "|CUS_0x1069 |ACCOUNTANT|2023-01-01   |2025-10-03|2023-01-01 00:00:00|features_attributes.csv|\n",
      "|CUS_0x114a |DEVELOPER |2023-01-01   |2025-10-03|2023-01-01 00:00:00|features_attributes.csv|\n",
      "|CUS_0x1184 |LAWYER    |2023-01-01   |2025-10-03|2023-01-01 00:00:00|features_attributes.csv|\n",
      "|CUS_0x1297 |MANAGER   |2023-01-01   |2025-10-03|2023-01-01 00:00:00|features_attributes.csv|\n",
      "+-----------+----------+-------------+----------+-------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/03 17:25:47 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadSilver\").getOrCreate()\n",
    "\n",
    "# Example: read attributes Silver for Jan 2023\n",
    "df = spark.read.parquet(\"datamart/silver/features/attributes/silver_attributes_2023_01_01.parquet\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3293a00-b7ef-4437-a004-419b8d2699dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|occupation   |\n",
      "+-------------+\n",
      "|ACCOUNTANT   |\n",
      "|DEVELOPER    |\n",
      "|MECHANIC     |\n",
      "|ARCHITECT    |\n",
      "|MEDIA_MANAGER|\n",
      "|ENGINEER     |\n",
      "|TEACHER      |\n",
      "|MUSICIAN     |\n",
      "|LAWYER       |\n",
      "|SCIENTIST    |\n",
      "|WRITER       |\n",
      "|JOURNALIST   |\n",
      "|MANAGER      |\n",
      "|DOCTOR       |\n",
      "|ENTREPRENEUR |\n",
      "|NULL         |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"occupation\").distinct().show(100, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d187a565-95ef-4184-bd85-f450425a254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541f3156-6d67-40cc-90af-0c91f80e66c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/03 19:39:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Loaded months=24 rows=11974\n",
      "\n",
      "[COUNT] rows per ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|ds_token  |count|\n",
      "+----------+-----+\n",
      "|2023-01-01|530  |\n",
      "|2023-02-01|501  |\n",
      "|2023-03-01|506  |\n",
      "|2023-04-01|510  |\n",
      "|2023-05-01|521  |\n",
      "|2023-06-01|517  |\n",
      "|2023-07-01|471  |\n",
      "|2023-08-01|481  |\n",
      "|2023-09-01|454  |\n",
      "|2023-10-01|487  |\n",
      "|2023-11-01|491  |\n",
      "|2023-12-01|489  |\n",
      "|2024-01-01|485  |\n",
      "|2024-02-01|518  |\n",
      "|2024-03-01|511  |\n",
      "|2024-04-01|513  |\n",
      "|2024-05-01|491  |\n",
      "|2024-06-01|498  |\n",
      "|2024-07-01|505  |\n",
      "|2024-08-01|543  |\n",
      "|2024-09-01|493  |\n",
      "|2024-10-01|456  |\n",
      "|2024-11-01|488  |\n",
      "|2024-12-01|515  |\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "[DUP] duplicate (customer_id, snapshot_date) rows (should be 0):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate groups: 0\n",
      "\n",
      "[NULL%] key columns (by month)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "|ds_token  |annual_income|monthly_inhand_salary|outstanding_debt|interest_rate|credit_utilization_ratio|num_of_loan|num_of_delayed_payment|credit_history_age|credit_mix        |payment_behaviour |payment_of_min_amount|\n",
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "|2023-01-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.754716981132077|7.169811320754717 |0.0                  |\n",
      "|2023-02-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |19.36127744510978 |6.986027944111776 |0.0                  |\n",
      "|2023-03-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |24.703557312252965|8.49802371541502  |0.0                  |\n",
      "|2023-04-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.588235294117645|10.392156862745098|0.0                  |\n",
      "|2023-05-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.072936660268713|8.253358925143955 |0.0                  |\n",
      "|2023-06-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.889748549323016|8.317214700193423 |0.0                  |\n",
      "|2023-07-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.080679405520172|8.70488322717622  |0.0                  |\n",
      "|2023-08-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |19.95841995841996 |9.563409563409564 |0.0                  |\n",
      "|2023-09-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |24.88986784140969 |7.929515418502203 |0.0                  |\n",
      "|2023-10-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.533880903490758|7.802874743326489 |0.0                  |\n",
      "|2023-11-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |15.478615071283095|6.720977596741344 |0.0                  |\n",
      "|2023-12-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.813905930470348|7.361963190184049 |0.0                  |\n",
      "|2024-01-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |21.443298969072163|7.216494845360824 |0.0                  |\n",
      "|2024-02-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.91891891891892 |7.5289575289575295|0.0                  |\n",
      "|2024-03-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.504892367906066|7.240704500978473 |0.0                  |\n",
      "|2024-04-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.22222222222222 |9.161793372319687 |0.0                  |\n",
      "|2024-05-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.606924643584524|7.128309572301425 |0.0                  |\n",
      "|2024-06-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.690763052208833|6.626506024096386 |0.0                  |\n",
      "|2024-07-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.574257425742577|6.732673267326733 |0.0                  |\n",
      "|2024-08-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |15.101289134438305|9.760589318600369 |0.0                  |\n",
      "|2024-09-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.515212981744423|6.896551724137931 |0.0                  |\n",
      "|2024-10-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.640350877192983|7.894736842105263 |0.0                  |\n",
      "|2024-11-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.74590163934426 |7.991803278688526 |0.0                  |\n",
      "|2024-12-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.58252427184466 |8.932038834951456 |0.0                  |\n",
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "\n",
      "\n",
      "[RANGE] interest_rate % (expect ~0..100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------+\n",
      "|min|max   |mean             |\n",
      "+---+------+-----------------+\n",
      "|2.0|5789.0|75.49189911474862|\n",
      "+---+------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest_rate out-of-range rows: 258\n",
      "\n",
      "[RANGE] credit_history_age (months) (expect non-negative & plausible)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "|min|max|mean             |\n",
      "+---+---+-----------------+\n",
      "|8  |404|224.5880240521129|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit_history_age implausible rows (>100y or negative): 0\n",
      "\n",
      "[2DP%] fraction not rounded to 2 d.p. (should be ~0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "|annual_income|monthly_inhand_salary|outstanding_debt|amount_invested_monthly|monthly_balance|\n",
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "|0.0          |0.0                  |0.0             |0.0                    |0.0            |\n",
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "\n",
      "\n",
      "[CREDIT_MIX] distinct values (expect Good/Standard/Bad or NULL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|credit_mix|count|\n",
      "+----------+-----+\n",
      "|Standard  |4287 |\n",
      "|Good      |2918 |\n",
      "|NULL      |2505 |\n",
      "|Bad       |2264 |\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "[PAYMENT_BEHAVIOUR] invalid token '!@9#%8' (should be 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows with bad token: 0\n",
      "\n",
      "[CONSISTENCY] num_of_loan vs parsed type_of_loan mismatches:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mismatch rows: 2314\n",
      "+----------+-----------+-----------------------------------------------------------------------------------------------------+-----------+-------------+\n",
      "|ds_token  |customer_id|type_of_loan                                                                                         |num_of_loan|_num_from_tol|\n",
      "+----------+-----------+-----------------------------------------------------------------------------------------------------+-----------+-------------+\n",
      "|2023-01-01|CUS_0x1325 |Not Specified, Personal Loan, Auto Loan, Not Specified, and Debt Consolidation Loan                  |3          |4            |\n",
      "|2023-01-01|CUS_0x13a8 |NULL                                                                                                 |0          |-1           |\n",
      "|2023-01-01|CUS_0x1630 |Not Specified, Student Loan, Personal Loan, Not Specified, Not Specified, and Debt Consolidation Loan|3          |4            |\n",
      "|2023-01-01|CUS_0x169c |NULL                                                                                                 |0          |-1           |\n",
      "|2023-01-01|CUS_0x182b |Credit-Builder Loan, Home Equity Loan, Not Specified, and Payday Loan                                |3          |4            |\n",
      "|2023-01-01|CUS_0x190e |NULL                                                                                                 |0          |-1           |\n",
      "|2023-01-01|CUS_0x1920 |NULL                                                                                                 |0          |-1           |\n",
      "|2023-01-01|CUS_0x19e1 |Personal Loan, Personal Loan, Not Specified, and Auto Loan                                           |3          |4            |\n",
      "|2023-01-01|CUS_0x1bc4 |Student Loan, Not Specified, and Payday Loan                                                         |2          |3            |\n",
      "|2023-01-01|CUS_0x1c09 |NULL                                                                                                 |0          |-1           |\n",
      "+----------+-----------+-----------------------------------------------------------------------------------------------------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[NEG%] numeric columns with negatives (should be ~0 except credit_change can be ±)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+-------------------+--------------------+------------------+------------------------+\n",
      "|annual_income|monthly_inhand_salary|outstanding_debt|total_emi_per_month|amount_invested_monthly|monthly_balance|num_of_loan|num_of_delayed_payment|delay_from_due_date|num_credit_inquiries|credit_history_age|credit_utilization_ratio|\n",
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+-------------------+--------------------+------------------+------------------------+\n",
      "|0.0          |0.0                  |0.0             |0.0                |0.0                    |0.0            |0.0        |0.0                   |0.7098713880073493 |0.0                 |0.0               |0.0                     |\n",
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+-------------------+--------------------+------------------+------------------------+\n",
      "\n",
      "\n",
      "[SAMPLE] interest_rate out-of-range examples\n",
      "+----------+-----------+-------------+\n",
      "|ds_token  |customer_id|interest_rate|\n",
      "+----------+-----------+-------------+\n",
      "|2023-01-01|CUS_0x1567 |5059.0       |\n",
      "|2023-01-01|CUS_0x397b |1884.0       |\n",
      "|2023-01-01|CUS_0x4cdd |4689.0       |\n",
      "|2023-01-01|CUS_0x5ae6 |4349.0       |\n",
      "|2023-01-01|CUS_0x5f02 |2617.0       |\n",
      "|2023-01-01|CUS_0x5f10 |2915.0       |\n",
      "|2023-01-01|CUS_0x7dfb |2565.0       |\n",
      "|2023-01-01|CUS_0x8956 |4230.0       |\n",
      "|2023-01-01|CUS_0xa9a  |4140.0       |\n",
      "|2023-01-01|CUS_0xac46 |1060.0       |\n",
      "+----------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[SAMPLE] 2dp non-conforming amounts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|ds_token|customer_id|annual_income|\n",
      "+--------+-----------+-------------+\n",
      "+--------+-----------+-------------+\n",
      "\n",
      "\n",
      "[OK] Sanity checks complete.\n"
     ]
    }
   ],
   "source": [
    "# sanity_financials.py\n",
    "# Quick checks for Silver financials across a date range.\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SILVER_FIN_DIR = \"datamart/silver/features/financials\"   # no trailing slash pls\n",
    "\n",
    "\n",
    "def month_range(start_str: str, end_str: str):\n",
    "    start = datetime.strptime(start_str, \"%Y-%m-%d\")\n",
    "    end   = datetime.strptime(end_str,   \"%Y-%m-%d\")\n",
    "    cur = datetime(start.year, start.month, 1)\n",
    "    out = []\n",
    "    while cur <= end:\n",
    "        out.append(cur.strftime(\"%Y-%m-%d\"))\n",
    "        cur += relativedelta(months=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_month_df(spark, silver_dir, ds):\n",
    "    token = datetime.strptime(ds, \"%Y-%m-%d\").strftime(\"%Y_%m_%d\")\n",
    "    path = os.path.join(silver_dir, f\"silver_financials_{token}.parquet\")\n",
    "    return spark.read.parquet(path).withColumn(\"ds_token\", F.lit(ds))\n",
    "\n",
    "\n",
    "def dq_for_range(spark, silver_dir, start=\"2023-01-01\", end=\"2024-12-01\"):\n",
    "    months = month_range(start, end)\n",
    "\n",
    "    # ---------- Load & union ----------\n",
    "    dfs = []\n",
    "    for ds in months:\n",
    "        try:\n",
    "            dfs.append(load_month_df(spark, silver_dir, ds))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Missing or unreadable: {ds} ({e})\")\n",
    "    if not dfs:\n",
    "        print(\"[ERROR] No Silver financials found.\")\n",
    "        return\n",
    "    df = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        df = df.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "    print(f\"\\n[INFO] Loaded months={len(dfs)} rows={df.count()}\")\n",
    "\n",
    "    # ---------- Basic shape & duplicates ----------\n",
    "    print(\"\\n[COUNT] rows per ds\")\n",
    "    df.groupBy(\"ds_token\").count().orderBy(\"ds_token\").show(50, False)\n",
    "\n",
    "    print(\"\\n[DUP] duplicate (customer_id, snapshot_date) rows (should be 0):\")\n",
    "    dup = (\n",
    "        df.groupBy(\"customer_id\", \"snapshot_date\", \"ds_token\")\n",
    "          .count()\n",
    "          .filter(F.col(\"count\") > 1)\n",
    "    )\n",
    "    print(f\"duplicate groups: {dup.count()}\")\n",
    "\n",
    "    # ---------- Type compliance & null rates ----------\n",
    "    expected_cols = [\n",
    "        \"customer_id\",\"snapshot_date\",\"ingest_dt\",\"snap_ym\",\"source_file\",\n",
    "        \"annual_income\",\"monthly_inhand_salary\",\"interest_rate\",\n",
    "        \"num_of_loan\",\"delay_from_due_date\",\"num_of_delayed_payment\",\n",
    "        \"changed_credit_limit\",\"num_credit_inquiries\",\"credit_mix\",\n",
    "        \"outstanding_debt\",\"credit_utilization_ratio\",\"credit_history_age\",\n",
    "        \"payment_of_min_amount\",\"total_emi_per_month\",\"amount_invested_monthly\",\n",
    "        \"payment_behaviour\",\"monthly_balance\",\"type_of_loan\"\n",
    "    ]\n",
    "    missing = [c for c in expected_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"[SCHEMA] Missing columns:\", missing)\n",
    "\n",
    "    print(\"\\n[NULL%] key columns (by month)\")\n",
    "    key_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "                \"interest_rate\",\"credit_utilization_ratio\",\"num_of_loan\",\n",
    "                \"num_of_delayed_payment\",\"credit_history_age\",\"credit_mix\",\n",
    "                \"payment_behaviour\",\"payment_of_min_amount\"]\n",
    "    exprs = [\n",
    "        (F.avg(F.when(F.col(c).isNull(), 1.0).otherwise(0.0))*100.0).alias(c)\n",
    "        for c in key_cols if c in df.columns\n",
    "    ]\n",
    "    df.groupBy(\"ds_token\").agg(*exprs).orderBy(\"ds_token\").show(50, False)\n",
    "\n",
    "    # ---------- Numeric ranges & “2dp” checks ----------\n",
    "    def show_min_max(name):\n",
    "        if name in df.columns:\n",
    "            df.select(\n",
    "                F.min(F.col(name)).alias(\"min\"),\n",
    "                F.max(F.col(name)).alias(\"max\"),\n",
    "                F.avg(F.col(name)).alias(\"mean\")\n",
    "            ).show(truncate=False)\n",
    "\n",
    "    print(\"\\n[RANGE] interest_rate % (expect ~0..100)\")\n",
    "    show_min_max(\"interest_rate\")\n",
    "    out_ir = df.filter((F.col(\"interest_rate\") < 0) | (F.col(\"interest_rate\") > 100))\n",
    "    print(f\"interest_rate out-of-range rows: {out_ir.count()}\")\n",
    "\n",
    "    print(\"\\n[RANGE] credit_history_age (months) (expect non-negative & plausible)\")\n",
    "    show_min_max(\"credit_history_age\")\n",
    "    out_hist = df.filter((F.col(\"credit_history_age\") < 0) | (F.col(\"credit_history_age\") > 1200))\n",
    "    print(f\"credit_history_age implausible rows (>100y or negative): {out_hist.count()}\")\n",
    "\n",
    "    # “2dp” conformance (allow tiny FP error)\n",
    "    def two_dp_nonconforming(colname):\n",
    "        return F.avg(\n",
    "            F.when(F.abs(F.col(colname) - F.round(F.col(colname), 2)) > F.lit(1e-9), 1.0).otherwise(0.0)\n",
    "        ).alias(colname)\n",
    "\n",
    "    dp_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "               \"amount_invested_monthly\",\"monthly_balance\"]\n",
    "    print(\"\\n[2DP%] fraction not rounded to 2 d.p. (should be ~0)\")\n",
    "    df.select([two_dp_nonconforming(c) for c in dp_cols if c in df.columns]).show(truncate=False)\n",
    "\n",
    "    # ---------- Category hygiene ----------\n",
    "    if \"credit_mix\" in df.columns:\n",
    "        print(\"\\n[CREDIT_MIX] distinct values (expect Good/Standard/Bad or NULL)\")\n",
    "        df.groupBy(\"credit_mix\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "    if \"payment_behaviour\" in df.columns:\n",
    "        print(\"\\n[PAYMENT_BEHAVIOUR] invalid token '!@9#%8' (should be 0)\")\n",
    "        bad_pb = df.filter(F.col(\"payment_behaviour\") == \"!@9#%8\")\n",
    "        print(f\"rows with bad token: {bad_pb.count()}\")\n",
    "\n",
    "    # ---------- Cross-field consistency ----------\n",
    "    # Recompute num_of_loan from type_of_loan to verify\n",
    "    if \"type_of_loan\" in df.columns and \"num_of_loan\" in df.columns:\n",
    "        tol_norm = F.upper(F.col(\"type_of_loan\").cast(\"string\"))\n",
    "        tol_norm = F.regexp_replace(tol_norm, r\"\\bAND\\b\", \",\")\n",
    "        tol_norm = F.regexp_replace(tol_norm, r\"NOT SPECIFIED\", \"\")\n",
    "        tol_norm = F.regexp_replace(tol_norm, r\"\\s+\", \" \")\n",
    "        tol_norm = F.regexp_replace(tol_norm, r\",\\s*,\", \",\")\n",
    "        tol_norm = F.regexp_replace(tol_norm, r\"(^\\s*,)|(,\\s*$)\", \"\")\n",
    "        df_check = df.withColumn(\n",
    "            \"_num_from_tol\",\n",
    "            F.when(F.length(F.trim(tol_norm)) == 0, F.lit(0))\n",
    "             .otherwise(F.size(F.split(tol_norm, r\"\\s*,\\s*\"))).cast(\"int\")\n",
    "        )\n",
    "        mism = df_check.filter(F.col(\"_num_from_tol\") != F.col(\"num_of_loan\"))\n",
    "        print(\"\\n[CONSISTENCY] num_of_loan vs parsed type_of_loan mismatches:\")\n",
    "        print(f\"mismatch rows: {mism.count()}\")\n",
    "        mism.select(\"ds_token\",\"customer_id\",\"type_of_loan\",\"num_of_loan\",\"_num_from_tol\").show(10, False)\n",
    "\n",
    "    # ---------- Value reasonableness ----------\n",
    "    nonneg_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "                   \"total_emi_per_month\",\"amount_invested_monthly\",\"monthly_balance\",\n",
    "                   \"num_of_loan\",\"num_of_delayed_payment\",\"delay_from_due_date\",\n",
    "                   \"num_credit_inquiries\",\"credit_history_age\",\"credit_utilization_ratio\"]\n",
    "    print(\"\\n[NEG%] numeric columns with negatives (should be ~0 except credit_change can be ±)\")\n",
    "    exprs2 = [\n",
    "        (F.avg(F.when(F.col(c) < 0, 1.0).otherwise(0.0))*100.0).alias(c)\n",
    "        for c in nonneg_cols if c in df.columns\n",
    "    ]\n",
    "    if exprs2:\n",
    "        df.select(*exprs2).show(truncate=False)\n",
    "\n",
    "    # ---------- Spot check examples ----------\n",
    "    print(\"\\n[SAMPLE] interest_rate out-of-range examples\")\n",
    "    out_ir.select(\"ds_token\",\"customer_id\",\"interest_rate\").show(10, False)\n",
    "\n",
    "    print(\"\\n[SAMPLE] 2dp non-conforming amounts\")\n",
    "    expr_flag = (F.abs(F.col(\"annual_income\") - F.round(F.col(\"annual_income\"), 2)) > F.lit(1e-9))\n",
    "    if \"annual_income\" in df.columns:\n",
    "        df.filter(expr_flag).select(\"ds_token\",\"customer_id\",\"annual_income\").show(10, False)\n",
    "\n",
    "    print(\"\\n[OK] Sanity checks complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"DQ-Silver-Financials\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    dq_for_range(\n",
    "        spark,\n",
    "        silver_dir=SILVER_FIN_DIR,\n",
    "        start=\"2023-01-01\",\n",
    "        end=\"2024-12-01\"\n",
    "    )\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe920e0-4d3d-443d-9caa-50f984843a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Loaded months=24 rows=11974\n",
      "\n",
      "[COUNT] rows per ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|ds_token  |count|\n",
      "+----------+-----+\n",
      "|2023-01-01|530  |\n",
      "|2023-02-01|501  |\n",
      "|2023-03-01|506  |\n",
      "|2023-04-01|510  |\n",
      "|2023-05-01|521  |\n",
      "|2023-06-01|517  |\n",
      "|2023-07-01|471  |\n",
      "|2023-08-01|481  |\n",
      "|2023-09-01|454  |\n",
      "|2023-10-01|487  |\n",
      "|2023-11-01|491  |\n",
      "|2023-12-01|489  |\n",
      "|2024-01-01|485  |\n",
      "|2024-02-01|518  |\n",
      "|2024-03-01|511  |\n",
      "|2024-04-01|513  |\n",
      "|2024-05-01|491  |\n",
      "|2024-06-01|498  |\n",
      "|2024-07-01|505  |\n",
      "|2024-08-01|543  |\n",
      "|2024-09-01|493  |\n",
      "|2024-10-01|456  |\n",
      "|2024-11-01|488  |\n",
      "|2024-12-01|515  |\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "[DUP] duplicate (customer_id, snapshot_date) groups (should be 0):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate groups: 0\n",
      "\n",
      "[NULL%] key columns (by month)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "|ds_token  |annual_income|monthly_inhand_salary|outstanding_debt|interest_rate|credit_utilization_ratio|num_of_loan|num_of_delayed_payment|credit_history_age|credit_mix        |payment_behaviour |payment_of_min_amount|\n",
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "|2023-01-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.754716981132077|7.169811320754717 |0.0                  |\n",
      "|2023-02-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |19.36127744510978 |6.986027944111776 |0.0                  |\n",
      "|2023-03-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |24.703557312252965|8.49802371541502  |0.0                  |\n",
      "|2023-04-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.588235294117645|10.392156862745098|0.0                  |\n",
      "|2023-05-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.072936660268713|8.253358925143955 |0.0                  |\n",
      "|2023-06-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.889748549323016|8.317214700193423 |0.0                  |\n",
      "|2023-07-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.080679405520172|8.70488322717622  |0.0                  |\n",
      "|2023-08-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |19.95841995841996 |9.563409563409564 |0.0                  |\n",
      "|2023-09-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |24.88986784140969 |7.929515418502203 |0.0                  |\n",
      "|2023-10-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.533880903490758|7.802874743326489 |0.0                  |\n",
      "|2023-11-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |15.478615071283095|6.720977596741344 |0.0                  |\n",
      "|2023-12-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.813905930470348|7.361963190184049 |0.0                  |\n",
      "|2024-01-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |21.443298969072163|7.216494845360824 |0.0                  |\n",
      "|2024-02-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.91891891891892 |7.5289575289575295|0.0                  |\n",
      "|2024-03-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.504892367906066|7.240704500978473 |0.0                  |\n",
      "|2024-04-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.22222222222222 |9.161793372319687 |0.0                  |\n",
      "|2024-05-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.606924643584524|7.128309572301425 |0.0                  |\n",
      "|2024-06-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.690763052208833|6.626506024096386 |0.0                  |\n",
      "|2024-07-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.574257425742577|6.732673267326733 |0.0                  |\n",
      "|2024-08-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |15.101289134438305|9.760589318600369 |0.0                  |\n",
      "|2024-09-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.515212981744423|6.896551724137931 |0.0                  |\n",
      "|2024-10-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.640350877192983|7.894736842105263 |0.0                  |\n",
      "|2024-11-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.74590163934426 |7.991803278688526 |0.0                  |\n",
      "|2024-12-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.58252427184466 |8.932038834951456 |0.0                  |\n",
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "\n",
      "\n",
      "[RANGE] interest_rate % (expect 0..100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------------+\n",
      "|min |max  |mean             |\n",
      "+----+-----+-----------------+\n",
      "|1.01|100.0|17.48090863537665|\n",
      "+----+-----+-----------------+\n",
      "\n",
      "interest_rate out-of-range rows: 0\n",
      "\n",
      "[RANGE] credit_history_age months (expect >=0 & <=1200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "|min|max|mean             |\n",
      "+---+---+-----------------+\n",
      "|8  |404|224.5880240521129|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit_history_age implausible rows (>100y or negative): 0\n",
      "\n",
      "[2DP%] fraction not rounded to 2 d.p. (should be ~0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "|annual_income|monthly_inhand_salary|outstanding_debt|amount_invested_monthly|monthly_balance|\n",
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "|0.0          |0.0                  |0.0             |0.0                    |0.0            |\n",
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "\n",
      "\n",
      "[CREDIT_MIX] distinct values (expect Good/Standard/Bad or NULL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|credit_mix|count|\n",
      "+----------+-----+\n",
      "|Standard  |4287 |\n",
      "|Good      |2918 |\n",
      "|NULL      |2505 |\n",
      "|Bad       |2264 |\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "[PAYMENT_BEHAVIOUR] invalid token '!@9#%8' (should be 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows with bad token: 0\n",
      "\n",
      "[CONSISTENCY] num_of_loan vs (commas+1) mismatches:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mismatch rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+-----------+-------------+\n",
      "|ds_token|customer_id|type_of_loan|num_of_loan|_num_from_tol|\n",
      "+--------+-----------+------------+-----------+-------------+\n",
      "+--------+-----------+------------+-----------+-------------+\n",
      "\n",
      "\n",
      "[NEG%] numeric columns with negatives (should be ~0; delay_from_due_date may be ±)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+--------------------+------------------+------------------------+\n",
      "|annual_income|monthly_inhand_salary|outstanding_debt|total_emi_per_month|amount_invested_monthly|monthly_balance|num_of_loan|num_of_delayed_payment|num_credit_inquiries|credit_history_age|credit_utilization_ratio|\n",
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+--------------------+------------------+------------------------+\n",
      "|0.0          |0.0                  |0.0             |0.0                |0.0                    |0.0            |0.0        |0.0                   |0.0                 |0.0               |0.0                     |\n",
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+--------------------+------------------+------------------------+\n",
      "\n",
      "\n",
      "[DELAY_FROM_DUE_DATE] share negative (early payments)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|neg_%             |\n",
      "+------------------+\n",
      "|0.7098713880073493|\n",
      "+------------------+\n",
      "\n",
      "\n",
      "[SAMPLE] interest_rate out-of-range examples\n",
      "+--------+-----------+-------------+\n",
      "|ds_token|customer_id|interest_rate|\n",
      "+--------+-----------+-------------+\n",
      "+--------+-----------+-------------+\n",
      "\n",
      "\n",
      "[OK] Sanity checks complete.\n"
     ]
    }
   ],
   "source": [
    "# sanity_financials.py\n",
    "# Quick checks for Silver financials across a date range (aligned to final Silver rules).\n",
    "#\n",
    "# - Loads each month's silver_financials_<YYYY_MM_DD>.parquet and unions them\n",
    "# - Counts rows/month, duplicate (customer_id, snapshot_date), null rates\n",
    "# - Range checks: interest_rate in 0..100 (%), credit_history_age months plausible\n",
    "# - 2 d.p. conformance for money fields\n",
    "# - Category hygiene for credit_mix and payment_behaviour\n",
    "# - Consistency: num_of_loan == (raw comma count + 1), with NULL/empty -> 0\n",
    "# - Reasonableness: negatives for non-negative numeric fields\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SILVER_FIN_DIR = \"datamart/silver/features/financials\"   # no trailing slash\n",
    "\n",
    "\n",
    "def month_range(start_str: str, end_str: str):\n",
    "    start = datetime.strptime(start_str, \"%Y-%m-%d\")\n",
    "    end   = datetime.strptime(end_str,   \"%Y-%m-%d\")\n",
    "    cur = datetime(start.year, start.month, 1)\n",
    "    out = []\n",
    "    while cur <= end:\n",
    "        out.append(cur.strftime(\"%Y-%m-%d\"))\n",
    "        cur += relativedelta(months=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_month_df(spark, silver_dir, ds):\n",
    "    token = datetime.strptime(ds, \"%Y-%m-%d\").strftime(\"%Y_%m_%d\")\n",
    "    path = os.path.join(silver_dir, f\"silver_financials_{token}.parquet\")\n",
    "    return spark.read.parquet(path).withColumn(\"ds_token\", F.lit(ds))\n",
    "\n",
    "\n",
    "def dq_for_range(spark, silver_dir, start=\"2023-01-01\", end=\"2024-12-01\"):\n",
    "    months = month_range(start, end)\n",
    "\n",
    "    # ---------- Load & union ----------\n",
    "    dfs = []\n",
    "    for ds in months:\n",
    "        try:\n",
    "            dfs.append(load_month_df(spark, silver_dir, ds))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Missing or unreadable: {ds} ({e})\")\n",
    "    if not dfs:\n",
    "        print(\"[ERROR] No Silver financials found.\")\n",
    "        return\n",
    "\n",
    "    df = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        df = df.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "    total_rows = df.count()\n",
    "    print(f\"\\n[INFO] Loaded months={len(dfs)} rows={total_rows}\")\n",
    "\n",
    "    # ---------- Basic shape & duplicates ----------\n",
    "    print(\"\\n[COUNT] rows per ds\")\n",
    "    df.groupBy(\"ds_token\").count().orderBy(\"ds_token\").show(50, False)\n",
    "\n",
    "    print(\"\\n[DUP] duplicate (customer_id, snapshot_date) groups (should be 0):\")\n",
    "    dup = (\n",
    "        df.groupBy(\"customer_id\", \"snapshot_date\", \"ds_token\")\n",
    "          .count()\n",
    "          .filter(F.col(\"count\") > 1)\n",
    "    )\n",
    "    print(f\"duplicate groups: {dup.count()}\")\n",
    "\n",
    "    # ---------- Schema presence ----------\n",
    "    expected_cols = [\n",
    "        \"customer_id\",\"snapshot_date\",\"ingest_dt\",\"snap_ym\",\"source_file\",\n",
    "        \"annual_income\",\"monthly_inhand_salary\",\"interest_rate\",\n",
    "        \"num_of_loan\",\"delay_from_due_date\",\"num_of_delayed_payment\",\n",
    "        \"changed_credit_limit\",\"num_credit_inquiries\",\"credit_mix\",\n",
    "        \"outstanding_debt\",\"credit_utilization_ratio\",\"credit_history_age\",\n",
    "        \"payment_of_min_amount\",\"total_emi_per_month\",\"amount_invested_monthly\",\n",
    "        \"payment_behaviour\",\"monthly_balance\",\"type_of_loan\"\n",
    "    ]\n",
    "    missing = [c for c in expected_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"[SCHEMA] Missing columns:\", missing)\n",
    "\n",
    "    # ---------- Null rates (by month) ----------\n",
    "    print(\"\\n[NULL%] key columns (by month)\")\n",
    "    key_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "                \"interest_rate\",\"credit_utilization_ratio\",\"num_of_loan\",\n",
    "                \"num_of_delayed_payment\",\"credit_history_age\",\"credit_mix\",\n",
    "                \"payment_behaviour\",\"payment_of_min_amount\"]\n",
    "    exprs = [\n",
    "        (F.avg(F.when(F.col(c).isNull(), 1.0).otherwise(0.0))*100.0).alias(c)\n",
    "        for c in key_cols if c in df.columns\n",
    "    ]\n",
    "    if exprs:\n",
    "        df.groupBy(\"ds_token\").agg(*exprs).orderBy(\"ds_token\").show(50, False)\n",
    "\n",
    "    # ---------- Ranges ----------\n",
    "    def show_min_max_mean(name, title=None):\n",
    "        if name in df.columns:\n",
    "            print(f\"\\n[RANGE] {title or name}\")\n",
    "            df.select(\n",
    "                F.min(F.col(name)).alias(\"min\"),\n",
    "                F.max(F.col(name)).alias(\"max\"),\n",
    "                F.avg(F.col(name)).alias(\"mean\")\n",
    "            ).show(truncate=False)\n",
    "\n",
    "    # interest_rate in 0..100\n",
    "    show_min_max_mean(\"interest_rate\", \"interest_rate % (expect 0..100)\")\n",
    "    out_ir = df.filter((F.col(\"interest_rate\") < 0) | (F.col(\"interest_rate\") > 100))\n",
    "    print(f\"interest_rate out-of-range rows: {out_ir.count()}\")\n",
    "\n",
    "    # credit_history_age months plausible (>=0, <= 1200 ~ 100y)\n",
    "    show_min_max_mean(\"credit_history_age\", \"credit_history_age months (expect >=0 & <=1200)\")\n",
    "    out_hist = df.filter((F.col(\"credit_history_age\") < 0) | (F.col(\"credit_history_age\") > 1200))\n",
    "    print(f\"credit_history_age implausible rows (>100y or negative): {out_hist.count()}\")\n",
    "\n",
    "    # ---------- “2dp” conformance for money fields ----------\n",
    "    def two_dp_nonconforming(colname):\n",
    "        return F.avg(\n",
    "            F.when(F.abs(F.col(colname) - F.round(F.col(colname), 2)) > F.lit(1e-9), 1.0).otherwise(0.0)\n",
    "        ).alias(colname)\n",
    "\n",
    "    dp_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "               \"amount_invested_monthly\",\"monthly_balance\"]\n",
    "    print(\"\\n[2DP%] fraction not rounded to 2 d.p. (should be ~0)\")\n",
    "    df.select([two_dp_nonconforming(c) for c in dp_cols if c in df.columns]).show(truncate=False)\n",
    "\n",
    "    # ---------- Category hygiene ----------\n",
    "    if \"credit_mix\" in df.columns:\n",
    "        print(\"\\n[CREDIT_MIX] distinct values (expect Good/Standard/Bad or NULL)\")\n",
    "        df.groupBy(\"credit_mix\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "    if \"payment_behaviour\" in df.columns:\n",
    "        print(\"\\n[PAYMENT_BEHAVIOUR] invalid token '!@9#%8' (should be 0)\")\n",
    "        bad_pb = df.filter(F.col(\"payment_behaviour\") == \"!@9#%8\")\n",
    "        print(f\"rows with bad token: {bad_pb.count()}\")\n",
    "\n",
    "    # ---------- Consistency: num_of_loan vs (commas + 1) on RAW type_of_loan ----------\n",
    "    if \"type_of_loan\" in df.columns and \"num_of_loan\" in df.columns:\n",
    "        tol_raw = F.col(\"type_of_loan\").cast(\"string\")\n",
    "        is_zero = (\n",
    "            tol_raw.isNull() |\n",
    "            (F.length(tol_raw) == 0) |\n",
    "            (F.upper(F.trim(tol_raw)) == F.lit(\"NULL\"))\n",
    "        )\n",
    "        comma_count = F.length(tol_raw) - F.length(F.regexp_replace(tol_raw, \",\", \"\"))\n",
    "        df_check = df.withColumn(\n",
    "            \"_num_from_tol\",\n",
    "            F.when(is_zero, F.lit(0)).otherwise((comma_count + F.lit(1)).cast(\"int\"))\n",
    "        )\n",
    "        mism = df_check.filter(F.col(\"_num_from_tol\") != F.col(\"num_of_loan\"))\n",
    "        print(\"\\n[CONSISTENCY] num_of_loan vs (commas+1) mismatches:\")\n",
    "        print(f\"mismatch rows: {mism.count()}\")\n",
    "        mism.select(\"ds_token\",\"customer_id\",\"type_of_loan\",\"num_of_loan\",\"_num_from_tol\").show(10, False)\n",
    "\n",
    "    # ---------- Reasonableness: non-negative numeric fields ----------\n",
    "    nonneg_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "                   \"total_emi_per_month\",\"amount_invested_monthly\",\"monthly_balance\",\n",
    "                   \"num_of_loan\",\"num_of_delayed_payment\",\"num_credit_inquiries\",\n",
    "                   \"credit_history_age\",\"credit_utilization_ratio\"]\n",
    "    print(\"\\n[NEG%] numeric columns with negatives (should be ~0; delay_from_due_date may be ±)\")\n",
    "    exprs2 = [\n",
    "        (F.avg(F.when(F.col(c) < 0, 1.0).otherwise(0.0))*100.0).alias(c)\n",
    "        for c in nonneg_cols if c in df.columns\n",
    "    ]\n",
    "    if exprs2:\n",
    "        df.select(*exprs2).show(truncate=False)\n",
    "\n",
    "    # delay_from_due_date separate (can be negative for early payments)\n",
    "    if \"delay_from_due_date\" in df.columns:\n",
    "        print(\"\\n[DELAY_FROM_DUE_DATE] share negative (early payments)\")\n",
    "        df.select((F.avg(F.when(F.col(\"delay_from_due_date\") < 0, 1.0).otherwise(0.0))*100.0)\n",
    "                  .alias(\"neg_%\")).show(truncate=False)\n",
    "\n",
    "    # ---------- Spot samples ----------\n",
    "    print(\"\\n[SAMPLE] interest_rate out-of-range examples\")\n",
    "    out_ir.select(\"ds_token\",\"customer_id\",\"interest_rate\").show(10, False)\n",
    "\n",
    "    print(\"\\n[OK] Sanity checks complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"DQ-Silver-Financials\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    dq_for_range(\n",
    "        spark,\n",
    "        silver_dir=SILVER_FIN_DIR,\n",
    "        start=\"2023-01-01\",\n",
    "        end=\"2024-12-01\"\n",
    "    )\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e9d79-1297-40d3-9dd6-6daf02dc5ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
