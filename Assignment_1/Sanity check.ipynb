{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe2395b-1a5d-404d-aeaf-b7848d5d7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "import utils.data_processing_bronze_table\n",
    "import utils.data_processing_silver_table\n",
    "import utils.data_processing_gold_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e646ff59-d527-439b-a057-1b97a972ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/27 19:24:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/27 19:24:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"dev\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19bdc9c-06f1-45a0-a806-16f88347a3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== datamart/bronze/lms/ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/27 19:25:07 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\n",
      "Caused by: java.lang.RuntimeException: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv is not a Parquet file. Expected magic number at tail, but found [45, 48, 49, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
      "\t... 14 more\n",
      "25/09/27 19:25:07 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o26.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (88c6508da389 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\nCaused by: java.lang.RuntimeException: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv is not a Parquet file. Expected magic number at tail, but found [45, 48, 49, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\nCaused by: java.lang.RuntimeException: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv is not a Parquet file. Expected magic number at tail, but found [45, 48, 49, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdatamart/bronze/lms/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdatamart/silver/loan_daily/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdatamart/gold/label_store/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m ]:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===\u001b[39m\u001b[33m\"\u001b[39m, p, \u001b[33m\"\u001b[39m\u001b[33m===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mrows:\u001b[39m\u001b[33m\"\u001b[39m, df.count())\n\u001b[32m     10\u001b[39m     df.printSchema()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    533\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    535\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    536\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    542\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o26.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (88c6508da389 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\nCaused by: java.lang.RuntimeException: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv is not a Parquet file. Expected magic number at tail, but found [45, 48, 49, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1057)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1423)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)\nCaused by: java.lang.RuntimeException: file:/app/datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv is not a Parquet file. Expected magic number at tail, but found [45, 48, 49, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "# Bronze / Silver / Gold row counts & schemas\n",
    "for p in [\n",
    "    \"datamart/bronze/lms/\",\n",
    "    \"datamart/silver/loan_daily/\",\n",
    "    \"datamart/gold/label_store/\",\n",
    "]:\n",
    "    print(\"\\n===\", p, \"===\")\n",
    "    df = spark.read.parquet(p)\n",
    "    print(\"rows:\", df.count())\n",
    "    df.printSchema()\n",
    "    df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8288bf-4ae7-4990-a0b4-2043c0d9a5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bronze_loan_daily_2023_01_01.csv', 'bronze_loan_daily_2023_02_01.csv', 'bronze_loan_daily_2023_03_01.csv', 'bronze_loan_daily_2023_04_01.csv', 'bronze_loan_daily_2023_05_01.csv', 'bronze_loan_daily_2023_06_01.csv', 'bronze_loan_daily_2023_07_01.csv', 'bronze_loan_daily_2023_08_01.csv', 'bronze_loan_daily_2023_09_01.csv', 'bronze_loan_daily_2023_10_01.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print([f for f in os.listdir(\"datamart/bronze/lms/\")][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7dbe8a4-99cf-4dba-bd28-1276e318d916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze rows: 104288\n",
      "root\n",
      " |-- loan_id: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- loan_start_date: date (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- installment_num: integer (nullable = true)\n",
      " |-- loan_amt: integer (nullable = true)\n",
      " |-- due_amt: double (nullable = true)\n",
      " |-- paid_amt: double (nullable = true)\n",
      " |-- overdue_amt: double (nullable = true)\n",
      " |-- balance: double (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbronze rows:\u001b[39m\u001b[33m\"\u001b[39m, bronze.count()); bronze.printSchema()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Silver (Parquet)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m silver = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatamart/silver/loan_daily/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msilver rows:\u001b[39m\u001b[33m\"\u001b[39m, silver.count()); silver.printSchema()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Gold (Parquet)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    533\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    535\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    536\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    542\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Bronze (CSV)\n",
    "bronze = (spark.read\n",
    "          .option(\"header\", \"true\")\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "          .csv(\"datamart/bronze/lms/\"))\n",
    "print(\"bronze rows:\", bronze.count()); bronze.printSchema()\n",
    "\n",
    "# Silver (Parquet)\n",
    "silver = spark.read.parquet(\"datamart/silver/loan_daily/\")\n",
    "print(\"silver rows:\", silver.count()); silver.printSchema()\n",
    "\n",
    "# Gold (Parquet)\n",
    "gold = spark.read.parquet(\"datamart/gold/label_store/\")\n",
    "print(\"gold rows:\", gold.count()); gold.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dc7b47c-8dab-432d-b57b-4db4755702bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver dir exists?  True\n",
      "Silver contents:  ['silver_loan_daily_2023_01_01.parquet', 'silver_loan_daily_2023_02_01.parquet', 'silver_loan_daily_2023_03_01.parquet', 'silver_loan_daily_2023_04_01.parquet', 'silver_loan_daily_2023_05_01.parquet', 'silver_loan_daily_2023_06_01.parquet', 'silver_loan_daily_2023_07_01.parquet', 'silver_loan_daily_2023_08_01.parquet', 'silver_loan_daily_2023_09_01.parquet', 'silver_loan_daily_2023_10_01.parquet']\n",
      "Gold dir exists?  True\n",
      "Gold contents:  ['gold_label_store_2023_01_01.parquet', 'gold_label_store_2023_02_01.parquet', 'gold_label_store_2023_03_01.parquet', 'gold_label_store_2023_04_01.parquet', 'gold_label_store_2023_05_01.parquet', 'gold_label_store_2023_06_01.parquet', 'gold_label_store_2023_07_01.parquet', 'gold_label_store_2023_08_01.parquet', 'gold_label_store_2023_09_01.parquet', 'gold_label_store_2023_10_01.parquet']\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "print(\"Silver dir exists? \", os.path.exists(\"datamart/silver/loan_daily/\"))\n",
    "print(\"Silver contents: \", os.listdir(\"datamart/silver/loan_daily/\")[:10])\n",
    "\n",
    "print(\"Gold dir exists? \", os.path.exists(\"datamart/gold/label_store/\"))\n",
    "print(\"Gold contents: \", os.listdir(\"datamart/gold/label_store/\")[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5cdc4e7-ba84-4043-bc18-7796764bf131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver rows: 104288\n",
      "root\n",
      " |-- loan_id: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- loan_start_date: date (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- installment_num: integer (nullable = true)\n",
      " |-- loan_amt: float (nullable = true)\n",
      " |-- due_amt: float (nullable = true)\n",
      " |-- paid_amt: float (nullable = true)\n",
      " |-- overdue_amt: float (nullable = true)\n",
      " |-- balance: float (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- mob: integer (nullable = true)\n",
      " |-- installments_missed: integer (nullable = true)\n",
      " |-- first_missed_date: date (nullable = true)\n",
      " |-- dpd: integer (nullable = true)\n",
      "\n",
      "gold rows: 8974\n",
      "root\n",
      " |-- loan_id: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- label_def: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# SILVER\n",
    "silver = spark.read.parquet(\"datamart/silver/loan_daily/*.parquet\")\n",
    "print(\"silver rows:\", silver.count()); silver.printSchema()\n",
    "\n",
    "# GOLD\n",
    "gold = spark.read.parquet(\"datamart/gold/label_store/*.parquet\")\n",
    "print(\"gold rows:\", gold.count()); gold.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e9851d-88b6-42d6-b647-2c5f5d7188f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2023-01-01'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"datamart/bronze/lms/bronze_loan_daily_2023_01_01.csv\")[\"snapshot_date\"].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5787973-326a-4796-84fb-6b7098d21f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fe_1</th>\n",
       "      <th>fe_2</th>\n",
       "      <th>fe_3</th>\n",
       "      <th>fe_4</th>\n",
       "      <th>fe_5</th>\n",
       "      <th>fe_6</th>\n",
       "      <th>fe_7</th>\n",
       "      <th>fe_8</th>\n",
       "      <th>fe_9</th>\n",
       "      <th>fe_10</th>\n",
       "      <th>...</th>\n",
       "      <th>fe_16</th>\n",
       "      <th>fe_17</th>\n",
       "      <th>fe_18</th>\n",
       "      <th>fe_19</th>\n",
       "      <th>fe_20</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>ingest_dt</th>\n",
       "      <th>snap_ym</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>118</td>\n",
       "      <td>80</td>\n",
       "      <td>121</td>\n",
       "      <td>55</td>\n",
       "      <td>193</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>-101</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>114</td>\n",
       "      <td>35</td>\n",
       "      <td>85</td>\n",
       "      <td>-73</td>\n",
       "      <td>76</td>\n",
       "      <td>CUS_0x1037</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>feature_clickstream.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-108</td>\n",
       "      <td>182</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>-56</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>-6</td>\n",
       "      <td>284</td>\n",
       "      <td>222</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>130</td>\n",
       "      <td>94</td>\n",
       "      <td>111</td>\n",
       "      <td>75</td>\n",
       "      <td>CUS_0x1069</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>feature_clickstream.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-13</td>\n",
       "      <td>8</td>\n",
       "      <td>87</td>\n",
       "      <td>166</td>\n",
       "      <td>214</td>\n",
       "      <td>-98</td>\n",
       "      <td>215</td>\n",
       "      <td>152</td>\n",
       "      <td>129</td>\n",
       "      <td>139</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>-130</td>\n",
       "      <td>354</td>\n",
       "      <td>17</td>\n",
       "      <td>302</td>\n",
       "      <td>CUS_0x114a</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>feature_clickstream.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-85</td>\n",
       "      <td>45</td>\n",
       "      <td>200</td>\n",
       "      <td>89</td>\n",
       "      <td>128</td>\n",
       "      <td>54</td>\n",
       "      <td>76</td>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>139</td>\n",
       "      <td>...</td>\n",
       "      <td>163</td>\n",
       "      <td>37</td>\n",
       "      <td>207</td>\n",
       "      <td>180</td>\n",
       "      <td>118</td>\n",
       "      <td>CUS_0x1184</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>feature_clickstream.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>120</td>\n",
       "      <td>226</td>\n",
       "      <td>-86</td>\n",
       "      <td>253</td>\n",
       "      <td>97</td>\n",
       "      <td>107</td>\n",
       "      <td>68</td>\n",
       "      <td>103</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>159</td>\n",
       "      <td>-26</td>\n",
       "      <td>104</td>\n",
       "      <td>118</td>\n",
       "      <td>184</td>\n",
       "      <td>CUS_0x1297</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>feature_clickstream.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fe_1  fe_2  fe_3  fe_4  fe_5  fe_6  fe_7  fe_8  fe_9  fe_10  ...  fe_16  \\\n",
       "0    63   118    80   121    55   193   111   112  -101     83  ...    114   \n",
       "1  -108   182   123     4   -56    27    25    -6   284    222  ...     35   \n",
       "2   -13     8    87   166   214   -98   215   152   129    139  ...    125   \n",
       "3   -85    45   200    89   128    54    76    51    61    139  ...    163   \n",
       "4    55   120   226   -86   253    97   107    68   103    126  ...    159   \n",
       "\n",
       "   fe_17  fe_18  fe_19  fe_20  Customer_ID  snapshot_date   ingest_dt  \\\n",
       "0     35     85    -73     76   CUS_0x1037     2023-01-01  2023-01-01   \n",
       "1    130     94    111     75   CUS_0x1069     2023-01-01  2023-01-01   \n",
       "2   -130    354     17    302   CUS_0x114a     2023-01-01  2023-01-01   \n",
       "3     37    207    180    118   CUS_0x1184     2023-01-01  2023-01-01   \n",
       "4    -26    104    118    184   CUS_0x1297     2023-01-01  2023-01-01   \n",
       "\n",
       "   snap_ym              source_file  \n",
       "0  2023-01  feature_clickstream.csv  \n",
       "1  2023-01  feature_clickstream.csv  \n",
       "2  2023-01  feature_clickstream.csv  \n",
       "3  2023-01  feature_clickstream.csv  \n",
       "4  2023-01  feature_clickstream.csv  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"datamart/bronze/features/clickstream/bronze_clickstream_2023_01_01.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ad4113-407b-46e6-bc1a-24e011dbb181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "len(glob.glob(\"datamart/bronze/features/clickstream/bronze_clickstream_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6be47b-a333-40cc-857e-14b31648c759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob(\"datamart/bronze/lms/bronze_loan_daily_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99983c7e-8a7e-4f4a-b556-31a3abf957a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickstream files: 24\n",
      "attributes files : 24\n",
      "financials files : 24\n",
      "lms files        : 24\n",
      "2023-06-01 snapshot unique: ['2023-06-01']\n"
     ]
    }
   ],
   "source": [
    "# quick_verify.py\n",
    "import glob, pandas as pd\n",
    "\n",
    "def count_files(pattern): return len(glob.glob(pattern))\n",
    "\n",
    "print(\"clickstream files:\", count_files(\"datamart/bronze/features/clickstream/bronze_clickstream_*.csv\"))\n",
    "print(\"attributes files :\", count_files(\"datamart/bronze/features/attributes/bronze_attributes_*.csv\"))\n",
    "print(\"financials files :\", count_files(\"datamart/bronze/features/financials/bronze_financials_*.csv\"))\n",
    "print(\"lms files        :\", count_files(\"datamart/bronze/lms/bronze_loan_daily_*.csv\"))\n",
    "\n",
    "# spot-check one month\n",
    "df = pd.read_csv(\"datamart/bronze/lms/bronze_loan_daily_2023_06_01.csv\")\n",
    "print(\"2023-06-01 snapshot unique:\", df[\"snapshot_date\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0faca526-b259-4e22-bc1b-84f7f6256b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/03 17:25:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- ingest_dt: date (nullable = true)\n",
      " |-- snap_ym: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n",
      "+-----------+----------+-------------+----------+-------------------+-----------------------+\n",
      "|customer_id|occupation|snapshot_date|ingest_dt |snap_ym            |source_file            |\n",
      "+-----------+----------+-------------+----------+-------------------+-----------------------+\n",
      "|CUS_0x1037 |ACCOUNTANT|2023-01-01   |2025-10-03|2023-01-01 00:00:00|features_attributes.csv|\n",
      "|CUS_0x1069 |ACCOUNTANT|2023-01-01   |2025-10-03|2023-01-01 00:00:00|features_attributes.csv|\n",
      "|CUS_0x114a |DEVELOPER |2023-01-01   |2025-10-03|2023-01-01 00:00:00|features_attributes.csv|\n",
      "|CUS_0x1184 |LAWYER    |2023-01-01   |2025-10-03|2023-01-01 00:00:00|features_attributes.csv|\n",
      "|CUS_0x1297 |MANAGER   |2023-01-01   |2025-10-03|2023-01-01 00:00:00|features_attributes.csv|\n",
      "+-----------+----------+-------------+----------+-------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/03 17:25:47 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadSilver\").getOrCreate()\n",
    "\n",
    "# Example: read attributes Silver for Jan 2023\n",
    "df = spark.read.parquet(\"datamart/silver/features/attributes/silver_attributes_2023_01_01.parquet\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3293a00-b7ef-4437-a004-419b8d2699dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|occupation   |\n",
      "+-------------+\n",
      "|ACCOUNTANT   |\n",
      "|DEVELOPER    |\n",
      "|MECHANIC     |\n",
      "|ARCHITECT    |\n",
      "|MEDIA_MANAGER|\n",
      "|ENGINEER     |\n",
      "|TEACHER      |\n",
      "|MUSICIAN     |\n",
      "|LAWYER       |\n",
      "|SCIENTIST    |\n",
      "|WRITER       |\n",
      "|JOURNALIST   |\n",
      "|MANAGER      |\n",
      "|DOCTOR       |\n",
      "|ENTREPRENEUR |\n",
      "|NULL         |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"occupation\").distinct().show(100, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d187a565-95ef-4184-bd85-f450425a254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541f3156-6d67-40cc-90af-0c91f80e66c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/03 19:39:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Loaded months=24 rows=11974\n",
      "\n",
      "[COUNT] rows per ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|ds_token  |count|\n",
      "+----------+-----+\n",
      "|2023-01-01|530  |\n",
      "|2023-02-01|501  |\n",
      "|2023-03-01|506  |\n",
      "|2023-04-01|510  |\n",
      "|2023-05-01|521  |\n",
      "|2023-06-01|517  |\n",
      "|2023-07-01|471  |\n",
      "|2023-08-01|481  |\n",
      "|2023-09-01|454  |\n",
      "|2023-10-01|487  |\n",
      "|2023-11-01|491  |\n",
      "|2023-12-01|489  |\n",
      "|2024-01-01|485  |\n",
      "|2024-02-01|518  |\n",
      "|2024-03-01|511  |\n",
      "|2024-04-01|513  |\n",
      "|2024-05-01|491  |\n",
      "|2024-06-01|498  |\n",
      "|2024-07-01|505  |\n",
      "|2024-08-01|543  |\n",
      "|2024-09-01|493  |\n",
      "|2024-10-01|456  |\n",
      "|2024-11-01|488  |\n",
      "|2024-12-01|515  |\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "[DUP] duplicate (customer_id, snapshot_date) rows (should be 0):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate groups: 0\n",
      "\n",
      "[NULL%] key columns (by month)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "|ds_token  |annual_income|monthly_inhand_salary|outstanding_debt|interest_rate|credit_utilization_ratio|num_of_loan|num_of_delayed_payment|credit_history_age|credit_mix        |payment_behaviour |payment_of_min_amount|\n",
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "|2023-01-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.754716981132077|7.169811320754717 |0.0                  |\n",
      "|2023-02-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |19.36127744510978 |6.986027944111776 |0.0                  |\n",
      "|2023-03-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |24.703557312252965|8.49802371541502  |0.0                  |\n",
      "|2023-04-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.588235294117645|10.392156862745098|0.0                  |\n",
      "|2023-05-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.072936660268713|8.253358925143955 |0.0                  |\n",
      "|2023-06-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.889748549323016|8.317214700193423 |0.0                  |\n",
      "|2023-07-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.080679405520172|8.70488322717622  |0.0                  |\n",
      "|2023-08-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |19.95841995841996 |9.563409563409564 |0.0                  |\n",
      "|2023-09-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |24.88986784140969 |7.929515418502203 |0.0                  |\n",
      "|2023-10-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.533880903490758|7.802874743326489 |0.0                  |\n",
      "|2023-11-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |15.478615071283095|6.720977596741344 |0.0                  |\n",
      "|2023-12-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.813905930470348|7.361963190184049 |0.0                  |\n",
      "|2024-01-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |21.443298969072163|7.216494845360824 |0.0                  |\n",
      "|2024-02-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.91891891891892 |7.5289575289575295|0.0                  |\n",
      "|2024-03-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.504892367906066|7.240704500978473 |0.0                  |\n",
      "|2024-04-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.22222222222222 |9.161793372319687 |0.0                  |\n",
      "|2024-05-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.606924643584524|7.128309572301425 |0.0                  |\n",
      "|2024-06-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.690763052208833|6.626506024096386 |0.0                  |\n",
      "|2024-07-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.574257425742577|6.732673267326733 |0.0                  |\n",
      "|2024-08-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |15.101289134438305|9.760589318600369 |0.0                  |\n",
      "|2024-09-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.515212981744423|6.896551724137931 |0.0                  |\n",
      "|2024-10-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.640350877192983|7.894736842105263 |0.0                  |\n",
      "|2024-11-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.74590163934426 |7.991803278688526 |0.0                  |\n",
      "|2024-12-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.58252427184466 |8.932038834951456 |0.0                  |\n",
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "\n",
      "\n",
      "[RANGE] interest_rate % (expect ~0..100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------+\n",
      "|min|max   |mean             |\n",
      "+---+------+-----------------+\n",
      "|2.0|5789.0|75.49189911474862|\n",
      "+---+------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest_rate out-of-range rows: 258\n",
      "\n",
      "[RANGE] credit_history_age (months) (expect non-negative & plausible)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "|min|max|mean             |\n",
      "+---+---+-----------------+\n",
      "|8  |404|224.5880240521129|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit_history_age implausible rows (>100y or negative): 0\n",
      "\n",
      "[2DP%] fraction not rounded to 2 d.p. (should be ~0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "|annual_income|monthly_inhand_salary|outstanding_debt|amount_invested_monthly|monthly_balance|\n",
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "|0.0          |0.0                  |0.0             |0.0                    |0.0            |\n",
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "\n",
      "\n",
      "[CREDIT_MIX] distinct values (expect Good/Standard/Bad or NULL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|credit_mix|count|\n",
      "+----------+-----+\n",
      "|Standard  |4287 |\n",
      "|Good      |2918 |\n",
      "|NULL      |2505 |\n",
      "|Bad       |2264 |\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "[PAYMENT_BEHAVIOUR] invalid token '!@9#%8' (should be 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows with bad token: 0\n",
      "\n",
      "[CONSISTENCY] num_of_loan vs parsed type_of_loan mismatches:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mismatch rows: 2314\n",
      "+----------+-----------+-----------------------------------------------------------------------------------------------------+-----------+-------------+\n",
      "|ds_token  |customer_id|type_of_loan                                                                                         |num_of_loan|_num_from_tol|\n",
      "+----------+-----------+-----------------------------------------------------------------------------------------------------+-----------+-------------+\n",
      "|2023-01-01|CUS_0x1325 |Not Specified, Personal Loan, Auto Loan, Not Specified, and Debt Consolidation Loan                  |3          |4            |\n",
      "|2023-01-01|CUS_0x13a8 |NULL                                                                                                 |0          |-1           |\n",
      "|2023-01-01|CUS_0x1630 |Not Specified, Student Loan, Personal Loan, Not Specified, Not Specified, and Debt Consolidation Loan|3          |4            |\n",
      "|2023-01-01|CUS_0x169c |NULL                                                                                                 |0          |-1           |\n",
      "|2023-01-01|CUS_0x182b |Credit-Builder Loan, Home Equity Loan, Not Specified, and Payday Loan                                |3          |4            |\n",
      "|2023-01-01|CUS_0x190e |NULL                                                                                                 |0          |-1           |\n",
      "|2023-01-01|CUS_0x1920 |NULL                                                                                                 |0          |-1           |\n",
      "|2023-01-01|CUS_0x19e1 |Personal Loan, Personal Loan, Not Specified, and Auto Loan                                           |3          |4            |\n",
      "|2023-01-01|CUS_0x1bc4 |Student Loan, Not Specified, and Payday Loan                                                         |2          |3            |\n",
      "|2023-01-01|CUS_0x1c09 |NULL                                                                                                 |0          |-1           |\n",
      "+----------+-----------+-----------------------------------------------------------------------------------------------------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[NEG%] numeric columns with negatives (should be ~0 except credit_change can be ±)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+-------------------+--------------------+------------------+------------------------+\n",
      "|annual_income|monthly_inhand_salary|outstanding_debt|total_emi_per_month|amount_invested_monthly|monthly_balance|num_of_loan|num_of_delayed_payment|delay_from_due_date|num_credit_inquiries|credit_history_age|credit_utilization_ratio|\n",
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+-------------------+--------------------+------------------+------------------------+\n",
      "|0.0          |0.0                  |0.0             |0.0                |0.0                    |0.0            |0.0        |0.0                   |0.7098713880073493 |0.0                 |0.0               |0.0                     |\n",
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+-------------------+--------------------+------------------+------------------------+\n",
      "\n",
      "\n",
      "[SAMPLE] interest_rate out-of-range examples\n",
      "+----------+-----------+-------------+\n",
      "|ds_token  |customer_id|interest_rate|\n",
      "+----------+-----------+-------------+\n",
      "|2023-01-01|CUS_0x1567 |5059.0       |\n",
      "|2023-01-01|CUS_0x397b |1884.0       |\n",
      "|2023-01-01|CUS_0x4cdd |4689.0       |\n",
      "|2023-01-01|CUS_0x5ae6 |4349.0       |\n",
      "|2023-01-01|CUS_0x5f02 |2617.0       |\n",
      "|2023-01-01|CUS_0x5f10 |2915.0       |\n",
      "|2023-01-01|CUS_0x7dfb |2565.0       |\n",
      "|2023-01-01|CUS_0x8956 |4230.0       |\n",
      "|2023-01-01|CUS_0xa9a  |4140.0       |\n",
      "|2023-01-01|CUS_0xac46 |1060.0       |\n",
      "+----------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[SAMPLE] 2dp non-conforming amounts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|ds_token|customer_id|annual_income|\n",
      "+--------+-----------+-------------+\n",
      "+--------+-----------+-------------+\n",
      "\n",
      "\n",
      "[OK] Sanity checks complete.\n"
     ]
    }
   ],
   "source": [
    "# sanity_financials.py\n",
    "# Quick checks for Silver financials across a date range.\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SILVER_FIN_DIR = \"datamart/silver/features/financials\"   # no trailing slash pls\n",
    "\n",
    "\n",
    "def month_range(start_str: str, end_str: str):\n",
    "    start = datetime.strptime(start_str, \"%Y-%m-%d\")\n",
    "    end   = datetime.strptime(end_str,   \"%Y-%m-%d\")\n",
    "    cur = datetime(start.year, start.month, 1)\n",
    "    out = []\n",
    "    while cur <= end:\n",
    "        out.append(cur.strftime(\"%Y-%m-%d\"))\n",
    "        cur += relativedelta(months=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_month_df(spark, silver_dir, ds):\n",
    "    token = datetime.strptime(ds, \"%Y-%m-%d\").strftime(\"%Y_%m_%d\")\n",
    "    path = os.path.join(silver_dir, f\"silver_financials_{token}.parquet\")\n",
    "    return spark.read.parquet(path).withColumn(\"ds_token\", F.lit(ds))\n",
    "\n",
    "\n",
    "def dq_for_range(spark, silver_dir, start=\"2023-01-01\", end=\"2024-12-01\"):\n",
    "    months = month_range(start, end)\n",
    "\n",
    "    # ---------- Load & union ----------\n",
    "    dfs = []\n",
    "    for ds in months:\n",
    "        try:\n",
    "            dfs.append(load_month_df(spark, silver_dir, ds))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Missing or unreadable: {ds} ({e})\")\n",
    "    if not dfs:\n",
    "        print(\"[ERROR] No Silver financials found.\")\n",
    "        return\n",
    "    df = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        df = df.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "    print(f\"\\n[INFO] Loaded months={len(dfs)} rows={df.count()}\")\n",
    "\n",
    "    # ---------- Basic shape & duplicates ----------\n",
    "    print(\"\\n[COUNT] rows per ds\")\n",
    "    df.groupBy(\"ds_token\").count().orderBy(\"ds_token\").show(50, False)\n",
    "\n",
    "    print(\"\\n[DUP] duplicate (customer_id, snapshot_date) rows (should be 0):\")\n",
    "    dup = (\n",
    "        df.groupBy(\"customer_id\", \"snapshot_date\", \"ds_token\")\n",
    "          .count()\n",
    "          .filter(F.col(\"count\") > 1)\n",
    "    )\n",
    "    print(f\"duplicate groups: {dup.count()}\")\n",
    "\n",
    "    # ---------- Type compliance & null rates ----------\n",
    "    expected_cols = [\n",
    "        \"customer_id\",\"snapshot_date\",\"ingest_dt\",\"snap_ym\",\"source_file\",\n",
    "        \"annual_income\",\"monthly_inhand_salary\",\"interest_rate\",\n",
    "        \"num_of_loan\",\"delay_from_due_date\",\"num_of_delayed_payment\",\n",
    "        \"changed_credit_limit\",\"num_credit_inquiries\",\"credit_mix\",\n",
    "        \"outstanding_debt\",\"credit_utilization_ratio\",\"credit_history_age\",\n",
    "        \"payment_of_min_amount\",\"total_emi_per_month\",\"amount_invested_monthly\",\n",
    "        \"payment_behaviour\",\"monthly_balance\",\"type_of_loan\"\n",
    "    ]\n",
    "    missing = [c for c in expected_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"[SCHEMA] Missing columns:\", missing)\n",
    "\n",
    "    print(\"\\n[NULL%] key columns (by month)\")\n",
    "    key_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "                \"interest_rate\",\"credit_utilization_ratio\",\"num_of_loan\",\n",
    "                \"num_of_delayed_payment\",\"credit_history_age\",\"credit_mix\",\n",
    "                \"payment_behaviour\",\"payment_of_min_amount\"]\n",
    "    exprs = [\n",
    "        (F.avg(F.when(F.col(c).isNull(), 1.0).otherwise(0.0))*100.0).alias(c)\n",
    "        for c in key_cols if c in df.columns\n",
    "    ]\n",
    "    df.groupBy(\"ds_token\").agg(*exprs).orderBy(\"ds_token\").show(50, False)\n",
    "\n",
    "    # ---------- Numeric ranges & “2dp” checks ----------\n",
    "    def show_min_max(name):\n",
    "        if name in df.columns:\n",
    "            df.select(\n",
    "                F.min(F.col(name)).alias(\"min\"),\n",
    "                F.max(F.col(name)).alias(\"max\"),\n",
    "                F.avg(F.col(name)).alias(\"mean\")\n",
    "            ).show(truncate=False)\n",
    "\n",
    "    print(\"\\n[RANGE] interest_rate % (expect ~0..100)\")\n",
    "    show_min_max(\"interest_rate\")\n",
    "    out_ir = df.filter((F.col(\"interest_rate\") < 0) | (F.col(\"interest_rate\") > 100))\n",
    "    print(f\"interest_rate out-of-range rows: {out_ir.count()}\")\n",
    "\n",
    "    print(\"\\n[RANGE] credit_history_age (months) (expect non-negative & plausible)\")\n",
    "    show_min_max(\"credit_history_age\")\n",
    "    out_hist = df.filter((F.col(\"credit_history_age\") < 0) | (F.col(\"credit_history_age\") > 1200))\n",
    "    print(f\"credit_history_age implausible rows (>100y or negative): {out_hist.count()}\")\n",
    "\n",
    "    # “2dp” conformance (allow tiny FP error)\n",
    "    def two_dp_nonconforming(colname):\n",
    "        return F.avg(\n",
    "            F.when(F.abs(F.col(colname) - F.round(F.col(colname), 2)) > F.lit(1e-9), 1.0).otherwise(0.0)\n",
    "        ).alias(colname)\n",
    "\n",
    "    dp_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "               \"amount_invested_monthly\",\"monthly_balance\"]\n",
    "    print(\"\\n[2DP%] fraction not rounded to 2 d.p. (should be ~0)\")\n",
    "    df.select([two_dp_nonconforming(c) for c in dp_cols if c in df.columns]).show(truncate=False)\n",
    "\n",
    "    # ---------- Category hygiene ----------\n",
    "    if \"credit_mix\" in df.columns:\n",
    "        print(\"\\n[CREDIT_MIX] distinct values (expect Good/Standard/Bad or NULL)\")\n",
    "        df.groupBy(\"credit_mix\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "    if \"payment_behaviour\" in df.columns:\n",
    "        print(\"\\n[PAYMENT_BEHAVIOUR] invalid token '!@9#%8' (should be 0)\")\n",
    "        bad_pb = df.filter(F.col(\"payment_behaviour\") == \"!@9#%8\")\n",
    "        print(f\"rows with bad token: {bad_pb.count()}\")\n",
    "\n",
    "    # ---------- Cross-field consistency ----------\n",
    "    # Recompute num_of_loan from type_of_loan to verify\n",
    "    if \"type_of_loan\" in df.columns and \"num_of_loan\" in df.columns:\n",
    "        tol_norm = F.upper(F.col(\"type_of_loan\").cast(\"string\"))\n",
    "        tol_norm = F.regexp_replace(tol_norm, r\"\\bAND\\b\", \",\")\n",
    "        tol_norm = F.regexp_replace(tol_norm, r\"NOT SPECIFIED\", \"\")\n",
    "        tol_norm = F.regexp_replace(tol_norm, r\"\\s+\", \" \")\n",
    "        tol_norm = F.regexp_replace(tol_norm, r\",\\s*,\", \",\")\n",
    "        tol_norm = F.regexp_replace(tol_norm, r\"(^\\s*,)|(,\\s*$)\", \"\")\n",
    "        df_check = df.withColumn(\n",
    "            \"_num_from_tol\",\n",
    "            F.when(F.length(F.trim(tol_norm)) == 0, F.lit(0))\n",
    "             .otherwise(F.size(F.split(tol_norm, r\"\\s*,\\s*\"))).cast(\"int\")\n",
    "        )\n",
    "        mism = df_check.filter(F.col(\"_num_from_tol\") != F.col(\"num_of_loan\"))\n",
    "        print(\"\\n[CONSISTENCY] num_of_loan vs parsed type_of_loan mismatches:\")\n",
    "        print(f\"mismatch rows: {mism.count()}\")\n",
    "        mism.select(\"ds_token\",\"customer_id\",\"type_of_loan\",\"num_of_loan\",\"_num_from_tol\").show(10, False)\n",
    "\n",
    "    # ---------- Value reasonableness ----------\n",
    "    nonneg_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "                   \"total_emi_per_month\",\"amount_invested_monthly\",\"monthly_balance\",\n",
    "                   \"num_of_loan\",\"num_of_delayed_payment\",\"delay_from_due_date\",\n",
    "                   \"num_credit_inquiries\",\"credit_history_age\",\"credit_utilization_ratio\"]\n",
    "    print(\"\\n[NEG%] numeric columns with negatives (should be ~0 except credit_change can be ±)\")\n",
    "    exprs2 = [\n",
    "        (F.avg(F.when(F.col(c) < 0, 1.0).otherwise(0.0))*100.0).alias(c)\n",
    "        for c in nonneg_cols if c in df.columns\n",
    "    ]\n",
    "    if exprs2:\n",
    "        df.select(*exprs2).show(truncate=False)\n",
    "\n",
    "    # ---------- Spot check examples ----------\n",
    "    print(\"\\n[SAMPLE] interest_rate out-of-range examples\")\n",
    "    out_ir.select(\"ds_token\",\"customer_id\",\"interest_rate\").show(10, False)\n",
    "\n",
    "    print(\"\\n[SAMPLE] 2dp non-conforming amounts\")\n",
    "    expr_flag = (F.abs(F.col(\"annual_income\") - F.round(F.col(\"annual_income\"), 2)) > F.lit(1e-9))\n",
    "    if \"annual_income\" in df.columns:\n",
    "        df.filter(expr_flag).select(\"ds_token\",\"customer_id\",\"annual_income\").show(10, False)\n",
    "\n",
    "    print(\"\\n[OK] Sanity checks complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"DQ-Silver-Financials\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    dq_for_range(\n",
    "        spark,\n",
    "        silver_dir=SILVER_FIN_DIR,\n",
    "        start=\"2023-01-01\",\n",
    "        end=\"2024-12-01\"\n",
    "    )\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe920e0-4d3d-443d-9caa-50f984843a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/05 09:35:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Loaded months=24 rows=11974\n",
      "\n",
      "[COUNT] rows per ds\n",
      "+----------+-----+\n",
      "|ds_token  |count|\n",
      "+----------+-----+\n",
      "|2023-01-01|530  |\n",
      "|2023-02-01|501  |\n",
      "|2023-03-01|506  |\n",
      "|2023-04-01|510  |\n",
      "|2023-05-01|521  |\n",
      "|2023-06-01|517  |\n",
      "|2023-07-01|471  |\n",
      "|2023-08-01|481  |\n",
      "|2023-09-01|454  |\n",
      "|2023-10-01|487  |\n",
      "|2023-11-01|491  |\n",
      "|2023-12-01|489  |\n",
      "|2024-01-01|485  |\n",
      "|2024-02-01|518  |\n",
      "|2024-03-01|511  |\n",
      "|2024-04-01|513  |\n",
      "|2024-05-01|491  |\n",
      "|2024-06-01|498  |\n",
      "|2024-07-01|505  |\n",
      "|2024-08-01|543  |\n",
      "|2024-09-01|493  |\n",
      "|2024-10-01|456  |\n",
      "|2024-11-01|488  |\n",
      "|2024-12-01|515  |\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "[DUP] duplicate (customer_id, snapshot_date) groups (should be 0):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate groups: 0\n",
      "\n",
      "[NULL%] key columns (by month)\n",
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "|ds_token  |annual_income|monthly_inhand_salary|outstanding_debt|interest_rate|credit_utilization_ratio|num_of_loan|num_of_delayed_payment|credit_history_age|credit_mix        |payment_behaviour |payment_of_min_amount|\n",
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "|2023-01-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.754716981132077|7.169811320754717 |0.0                  |\n",
      "|2023-02-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |19.36127744510978 |6.986027944111776 |0.0                  |\n",
      "|2023-03-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |24.703557312252965|8.49802371541502  |0.0                  |\n",
      "|2023-04-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.588235294117645|10.392156862745098|0.0                  |\n",
      "|2023-05-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.072936660268713|8.253358925143955 |0.0                  |\n",
      "|2023-06-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.889748549323016|8.317214700193423 |0.0                  |\n",
      "|2023-07-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.080679405520172|8.70488322717622  |0.0                  |\n",
      "|2023-08-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |19.95841995841996 |9.563409563409564 |0.0                  |\n",
      "|2023-09-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |24.88986784140969 |7.929515418502203 |0.0                  |\n",
      "|2023-10-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.533880903490758|7.802874743326489 |0.0                  |\n",
      "|2023-11-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |15.478615071283095|6.720977596741344 |0.0                  |\n",
      "|2023-12-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.813905930470348|7.361963190184049 |0.0                  |\n",
      "|2024-01-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |21.443298969072163|7.216494845360824 |0.0                  |\n",
      "|2024-02-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.91891891891892 |7.5289575289575295|0.0                  |\n",
      "|2024-03-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.504892367906066|7.240704500978473 |0.0                  |\n",
      "|2024-04-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.22222222222222 |9.161793372319687 |0.0                  |\n",
      "|2024-05-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.606924643584524|7.128309572301425 |0.0                  |\n",
      "|2024-06-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.690763052208833|6.626506024096386 |0.0                  |\n",
      "|2024-07-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.574257425742577|6.732673267326733 |0.0                  |\n",
      "|2024-08-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |15.101289134438305|9.760589318600369 |0.0                  |\n",
      "|2024-09-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.515212981744423|6.896551724137931 |0.0                  |\n",
      "|2024-10-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |18.640350877192983|7.894736842105263 |0.0                  |\n",
      "|2024-11-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |22.74590163934426 |7.991803278688526 |0.0                  |\n",
      "|2024-12-01|0.0          |0.0                  |0.0             |0.0          |0.0                     |0.0        |0.0                   |0.0               |20.58252427184466 |8.932038834951456 |0.0                  |\n",
      "+----------+-------------+---------------------+----------------+-------------+------------------------+-----------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "\n",
      "\n",
      "[RANGE] interest_rate % (expect 0..100)\n",
      "+----+-----+-----------------+\n",
      "|min |max  |mean             |\n",
      "+----+-----+-----------------+\n",
      "|1.01|100.0|17.48090863537665|\n",
      "+----+-----+-----------------+\n",
      "\n",
      "interest_rate out-of-range rows: 0\n",
      "\n",
      "[RANGE] credit_history_age months (expect >=0 & <=1200)\n",
      "+---+---+-----------------+\n",
      "|min|max|mean             |\n",
      "+---+---+-----------------+\n",
      "|8  |404|224.5880240521129|\n",
      "+---+---+-----------------+\n",
      "\n",
      "credit_history_age implausible rows (>100y or negative): 0\n",
      "\n",
      "[2DP%] fraction not rounded to 2 d.p. (should be ~0)\n",
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "|annual_income|monthly_inhand_salary|outstanding_debt|amount_invested_monthly|monthly_balance|\n",
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "|0.0          |0.0                  |0.0             |0.0                    |0.0            |\n",
      "+-------------+---------------------+----------------+-----------------------+---------------+\n",
      "\n",
      "\n",
      "[CREDIT_MIX] distinct values (expect Good/Standard/Bad or NULL)\n",
      "+----------+-----+\n",
      "|credit_mix|count|\n",
      "+----------+-----+\n",
      "|Standard  |4287 |\n",
      "|Good      |2918 |\n",
      "|NULL      |2505 |\n",
      "|Bad       |2264 |\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "[PAYMENT_BEHAVIOUR] invalid token '!@9#%8' (should be 0)\n",
      "rows with bad token: 0\n",
      "\n",
      "[CONSISTENCY] num_of_loan vs (commas+1) mismatches:\n",
      "mismatch rows: 0\n",
      "+--------+-----------+------------+-----------+-------------+\n",
      "|ds_token|customer_id|type_of_loan|num_of_loan|_num_from_tol|\n",
      "+--------+-----------+------------+-----------+-------------+\n",
      "+--------+-----------+------------+-----------+-------------+\n",
      "\n",
      "\n",
      "[NEG%] numeric columns with negatives (should be ~0; delay_from_due_date may be ±)\n",
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+--------------------+------------------+------------------------+\n",
      "|annual_income|monthly_inhand_salary|outstanding_debt|total_emi_per_month|amount_invested_monthly|monthly_balance|num_of_loan|num_of_delayed_payment|num_credit_inquiries|credit_history_age|credit_utilization_ratio|\n",
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+--------------------+------------------+------------------------+\n",
      "|0.0          |0.0                  |0.0             |0.0                |0.0                    |0.0            |0.0        |0.0                   |0.0                 |0.0               |0.0                     |\n",
      "+-------------+---------------------+----------------+-------------------+-----------------------+---------------+-----------+----------------------+--------------------+------------------+------------------------+\n",
      "\n",
      "\n",
      "[DELAY_FROM_DUE_DATE] share negative (early payments)\n",
      "+------------------+\n",
      "|neg_%             |\n",
      "+------------------+\n",
      "|0.7098713880073493|\n",
      "+------------------+\n",
      "\n",
      "\n",
      "[SAMPLE] interest_rate out-of-range examples\n",
      "+--------+-----------+-------------+\n",
      "|ds_token|customer_id|interest_rate|\n",
      "+--------+-----------+-------------+\n",
      "+--------+-----------+-------------+\n",
      "\n",
      "\n",
      "[OK] Sanity checks complete.\n"
     ]
    }
   ],
   "source": [
    "# sanity_financials.py\n",
    "# Quick checks for Silver financials across a date range (aligned to final Silver rules).\n",
    "#\n",
    "# - Loads each month's silver_financials_<YYYY_MM_DD>.parquet and unions them\n",
    "# - Counts rows/month, duplicate (customer_id, snapshot_date), null rates\n",
    "# - Range checks: interest_rate in 0..100 (%), credit_history_age months plausible\n",
    "# - 2 d.p. conformance for money fields\n",
    "# - Category hygiene for credit_mix and payment_behaviour\n",
    "# - Consistency: num_of_loan == (raw comma count + 1), with NULL/empty -> 0\n",
    "# - Reasonableness: negatives for non-negative numeric fields\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SILVER_FIN_DIR = \"datamart/silver/features/financials\"   # no trailing slash\n",
    "\n",
    "\n",
    "def month_range(start_str: str, end_str: str):\n",
    "    start = datetime.strptime(start_str, \"%Y-%m-%d\")\n",
    "    end   = datetime.strptime(end_str,   \"%Y-%m-%d\")\n",
    "    cur = datetime(start.year, start.month, 1)\n",
    "    out = []\n",
    "    while cur <= end:\n",
    "        out.append(cur.strftime(\"%Y-%m-%d\"))\n",
    "        cur += relativedelta(months=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_month_df(spark, silver_dir, ds):\n",
    "    token = datetime.strptime(ds, \"%Y-%m-%d\").strftime(\"%Y_%m_%d\")\n",
    "    path = os.path.join(silver_dir, f\"silver_financials_{token}.parquet\")\n",
    "    return spark.read.parquet(path).withColumn(\"ds_token\", F.lit(ds))\n",
    "\n",
    "\n",
    "def dq_for_range(spark, silver_dir, start=\"2023-01-01\", end=\"2024-12-01\"):\n",
    "    months = month_range(start, end)\n",
    "\n",
    "    # ---------- Load & union ----------\n",
    "    dfs = []\n",
    "    for ds in months:\n",
    "        try:\n",
    "            dfs.append(load_month_df(spark, silver_dir, ds))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Missing or unreadable: {ds} ({e})\")\n",
    "    if not dfs:\n",
    "        print(\"[ERROR] No Silver financials found.\")\n",
    "        return\n",
    "\n",
    "    df = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        df = df.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "    total_rows = df.count()\n",
    "    print(f\"\\n[INFO] Loaded months={len(dfs)} rows={total_rows}\")\n",
    "\n",
    "    # ---------- Basic shape & duplicates ----------\n",
    "    print(\"\\n[COUNT] rows per ds\")\n",
    "    df.groupBy(\"ds_token\").count().orderBy(\"ds_token\").show(50, False)\n",
    "\n",
    "    print(\"\\n[DUP] duplicate (customer_id, snapshot_date) groups (should be 0):\")\n",
    "    dup = (\n",
    "        df.groupBy(\"customer_id\", \"snapshot_date\", \"ds_token\")\n",
    "          .count()\n",
    "          .filter(F.col(\"count\") > 1)\n",
    "    )\n",
    "    print(f\"duplicate groups: {dup.count()}\")\n",
    "\n",
    "    # ---------- Schema presence ----------\n",
    "    expected_cols = [\n",
    "        \"customer_id\",\"snapshot_date\",\"ingest_dt\",\"snap_ym\",\"source_file\",\n",
    "        \"annual_income\",\"monthly_inhand_salary\",\"interest_rate\",\n",
    "        \"num_of_loan\",\"delay_from_due_date\",\"num_of_delayed_payment\",\n",
    "        \"changed_credit_limit\",\"num_credit_inquiries\",\"credit_mix\",\n",
    "        \"outstanding_debt\",\"credit_utilization_ratio\",\"credit_history_age\",\n",
    "        \"payment_of_min_amount\",\"total_emi_per_month\",\"amount_invested_monthly\",\n",
    "        \"payment_behaviour\",\"monthly_balance\",\"type_of_loan\"\n",
    "    ]\n",
    "    missing = [c for c in expected_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"[SCHEMA] Missing columns:\", missing)\n",
    "\n",
    "    # ---------- Null rates (by month) ----------\n",
    "    print(\"\\n[NULL%] key columns (by month)\")\n",
    "    key_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "                \"interest_rate\",\"credit_utilization_ratio\",\"num_of_loan\",\n",
    "                \"num_of_delayed_payment\",\"credit_history_age\",\"credit_mix\",\n",
    "                \"payment_behaviour\",\"payment_of_min_amount\"]\n",
    "    exprs = [\n",
    "        (F.avg(F.when(F.col(c).isNull(), 1.0).otherwise(0.0))*100.0).alias(c)\n",
    "        for c in key_cols if c in df.columns\n",
    "    ]\n",
    "    if exprs:\n",
    "        df.groupBy(\"ds_token\").agg(*exprs).orderBy(\"ds_token\").show(50, False)\n",
    "\n",
    "    # ---------- Ranges ----------\n",
    "    def show_min_max_mean(name, title=None):\n",
    "        if name in df.columns:\n",
    "            print(f\"\\n[RANGE] {title or name}\")\n",
    "            df.select(\n",
    "                F.min(F.col(name)).alias(\"min\"),\n",
    "                F.max(F.col(name)).alias(\"max\"),\n",
    "                F.avg(F.col(name)).alias(\"mean\")\n",
    "            ).show(truncate=False)\n",
    "\n",
    "    # interest_rate in 0..100\n",
    "    show_min_max_mean(\"interest_rate\", \"interest_rate % (expect 0..100)\")\n",
    "    out_ir = df.filter((F.col(\"interest_rate\") < 0) | (F.col(\"interest_rate\") > 100))\n",
    "    print(f\"interest_rate out-of-range rows: {out_ir.count()}\")\n",
    "\n",
    "    # credit_history_age months plausible (>=0, <= 1200 ~ 100y)\n",
    "    show_min_max_mean(\"credit_history_age\", \"credit_history_age months (expect >=0 & <=1200)\")\n",
    "    out_hist = df.filter((F.col(\"credit_history_age\") < 0) | (F.col(\"credit_history_age\") > 1200))\n",
    "    print(f\"credit_history_age implausible rows (>100y or negative): {out_hist.count()}\")\n",
    "\n",
    "    # ---------- “2dp” conformance for money fields ----------\n",
    "    def two_dp_nonconforming(colname):\n",
    "        return F.avg(\n",
    "            F.when(F.abs(F.col(colname) - F.round(F.col(colname), 2)) > F.lit(1e-9), 1.0).otherwise(0.0)\n",
    "        ).alias(colname)\n",
    "\n",
    "    dp_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "               \"amount_invested_monthly\",\"monthly_balance\"]\n",
    "    print(\"\\n[2DP%] fraction not rounded to 2 d.p. (should be ~0)\")\n",
    "    df.select([two_dp_nonconforming(c) for c in dp_cols if c in df.columns]).show(truncate=False)\n",
    "\n",
    "    # ---------- Category hygiene ----------\n",
    "    if \"credit_mix\" in df.columns:\n",
    "        print(\"\\n[CREDIT_MIX] distinct values (expect Good/Standard/Bad or NULL)\")\n",
    "        df.groupBy(\"credit_mix\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "    if \"payment_behaviour\" in df.columns:\n",
    "        print(\"\\n[PAYMENT_BEHAVIOUR] invalid token '!@9#%8' (should be 0)\")\n",
    "        bad_pb = df.filter(F.col(\"payment_behaviour\") == \"!@9#%8\")\n",
    "        print(f\"rows with bad token: {bad_pb.count()}\")\n",
    "\n",
    "    # ---------- Consistency: num_of_loan vs (commas + 1) on RAW type_of_loan ----------\n",
    "    if \"type_of_loan\" in df.columns and \"num_of_loan\" in df.columns:\n",
    "        tol_raw = F.col(\"type_of_loan\").cast(\"string\")\n",
    "        is_zero = (\n",
    "            tol_raw.isNull() |\n",
    "            (F.length(tol_raw) == 0) |\n",
    "            (F.upper(F.trim(tol_raw)) == F.lit(\"NULL\"))\n",
    "        )\n",
    "        comma_count = F.length(tol_raw) - F.length(F.regexp_replace(tol_raw, \",\", \"\"))\n",
    "        df_check = df.withColumn(\n",
    "            \"_num_from_tol\",\n",
    "            F.when(is_zero, F.lit(0)).otherwise((comma_count + F.lit(1)).cast(\"int\"))\n",
    "        )\n",
    "        mism = df_check.filter(F.col(\"_num_from_tol\") != F.col(\"num_of_loan\"))\n",
    "        print(\"\\n[CONSISTENCY] num_of_loan vs (commas+1) mismatches:\")\n",
    "        print(f\"mismatch rows: {mism.count()}\")\n",
    "        mism.select(\"ds_token\",\"customer_id\",\"type_of_loan\",\"num_of_loan\",\"_num_from_tol\").show(10, False)\n",
    "\n",
    "    # ---------- Reasonableness: non-negative numeric fields ----------\n",
    "    nonneg_cols = [\"annual_income\",\"monthly_inhand_salary\",\"outstanding_debt\",\n",
    "                   \"total_emi_per_month\",\"amount_invested_monthly\",\"monthly_balance\",\n",
    "                   \"num_of_loan\",\"num_of_delayed_payment\",\"num_credit_inquiries\",\n",
    "                   \"credit_history_age\",\"credit_utilization_ratio\"]\n",
    "    print(\"\\n[NEG%] numeric columns with negatives (should be ~0; delay_from_due_date may be ±)\")\n",
    "    exprs2 = [\n",
    "        (F.avg(F.when(F.col(c) < 0, 1.0).otherwise(0.0))*100.0).alias(c)\n",
    "        for c in nonneg_cols if c in df.columns\n",
    "    ]\n",
    "    if exprs2:\n",
    "        df.select(*exprs2).show(truncate=False)\n",
    "\n",
    "    # delay_from_due_date separate (can be negative for early payments)\n",
    "    if \"delay_from_due_date\" in df.columns:\n",
    "        print(\"\\n[DELAY_FROM_DUE_DATE] share negative (early payments)\")\n",
    "        df.select((F.avg(F.when(F.col(\"delay_from_due_date\") < 0, 1.0).otherwise(0.0))*100.0)\n",
    "                  .alias(\"neg_%\")).show(truncate=False)\n",
    "\n",
    "    # ---------- Spot samples ----------\n",
    "    print(\"\\n[SAMPLE] interest_rate out-of-range examples\")\n",
    "    out_ir.select(\"ds_token\",\"customer_id\",\"interest_rate\").show(10, False)\n",
    "\n",
    "    print(\"\\n[OK] Sanity checks complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"DQ-Silver-Financials\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    dq_for_range(\n",
    "        spark,\n",
    "        silver_dir=SILVER_FIN_DIR,\n",
    "        start=\"2023-01-01\",\n",
    "        end=\"2024-12-01\"\n",
    "    )\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6496ef-7a1c-4cd2-b453-89fe64c4e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q pyarrow fastparquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "149f77d6-8981-4893-95e2-99c5629ed7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/site-packages (25.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0.1\n",
      "    Uninstalling pip-25.0.1:\n",
      "      Successfully uninstalled pip-25.0.1\n",
      "Successfully installed pip-25.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1e9d79-1297-40d3-9dd6-6daf02dc5ce1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowKeyError",
     "evalue": "A type extension with name pandas.period already defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Example path (adjust to your actual directory)\u001b[39;00m\n\u001b[32m      4\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33mdatamart/silver/loan_daily/silver_loan_daily_2023_01_01.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.shape)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/parquet.py:651\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    500\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    508\u001b[39m     **kwargs,\n\u001b[32m    509\u001b[39m ) -> DataFrame:\n\u001b[32m    510\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    512\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    654\u001b[39m         msg = (\n\u001b[32m    655\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    656\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    657\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/parquet.py:63\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     65\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/parquet.py:169\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/core/arrays/arrow/extension_types.py:59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# register the type with a dummy instance\u001b[39;00m\n\u001b[32m     58\u001b[39m _period_type = ArrowPeriodType(\u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_period_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mArrowIntervalType\u001b[39;00m(pyarrow.ExtensionType):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, subtype, closed: IntervalClosedType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m# attributes need to be set first before calling\u001b[39;00m\n\u001b[32m     65\u001b[39m         \u001b[38;5;66;03m# super init (as that calls serialize)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyarrow/types.pxi:2226\u001b[39m, in \u001b[36mpyarrow.lib.register_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: A type extension with name pandas.period already defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example path (adjust to your actual directory)\n",
    "path = \"datamart/silver/loan_daily/silver_loan_daily_2023_01_01.parquet\"\n",
    "\n",
    "df = pd.read_parquet(path)\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2f941ad-c005-4a5a-a3b9-c2db1b92d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(530, 18)\n",
      "['loan_id', 'Customer_ID', 'loan_start_date', 'tenure', 'installment_num', 'loan_amt', 'due_amt', 'paid_amt', 'overdue_amt', 'balance', 'snapshot_date', 'ingest_dt', 'snap_ym', 'source_file', 'mob', 'installments_missed', 'first_missed_date', 'dpd']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_id</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>loan_start_date</th>\n",
       "      <th>tenure</th>\n",
       "      <th>installment_num</th>\n",
       "      <th>loan_amt</th>\n",
       "      <th>due_amt</th>\n",
       "      <th>paid_amt</th>\n",
       "      <th>overdue_amt</th>\n",
       "      <th>balance</th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>ingest_dt</th>\n",
       "      <th>snap_ym</th>\n",
       "      <th>source_file</th>\n",
       "      <th>mob</th>\n",
       "      <th>installments_missed</th>\n",
       "      <th>first_missed_date</th>\n",
       "      <th>dpd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUS_0x1037_2023_01_01</td>\n",
       "      <td>CUS_0x1037</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>lms_loan_daily.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUS_0x1069_2023_01_01</td>\n",
       "      <td>CUS_0x1069</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>lms_loan_daily.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUS_0x114a_2023_01_01</td>\n",
       "      <td>CUS_0x114a</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>lms_loan_daily.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUS_0x1184_2023_01_01</td>\n",
       "      <td>CUS_0x1184</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>lms_loan_daily.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUS_0x1297_2023_01_01</td>\n",
       "      <td>CUS_0x1297</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>lms_loan_daily.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loan_id Customer_ID loan_start_date  tenure  installment_num  \\\n",
       "0  CUS_0x1037_2023_01_01  CUS_0x1037      2023-01-01      10                0   \n",
       "1  CUS_0x1069_2023_01_01  CUS_0x1069      2023-01-01      10                0   \n",
       "2  CUS_0x114a_2023_01_01  CUS_0x114a      2023-01-01      10                0   \n",
       "3  CUS_0x1184_2023_01_01  CUS_0x1184      2023-01-01      10                0   \n",
       "4  CUS_0x1297_2023_01_01  CUS_0x1297      2023-01-01      10                0   \n",
       "\n",
       "   loan_amt  due_amt  paid_amt  overdue_amt  balance snapshot_date  ingest_dt  \\\n",
       "0   10000.0      0.0       0.0          0.0  10000.0    2023-01-01 2025-10-03   \n",
       "1   10000.0      0.0       0.0          0.0  10000.0    2023-01-01 2025-10-03   \n",
       "2   10000.0      0.0       0.0          0.0  10000.0    2023-01-01 2025-10-03   \n",
       "3   10000.0      0.0       0.0          0.0  10000.0    2023-01-01 2025-10-03   \n",
       "4   10000.0      0.0       0.0          0.0  10000.0    2023-01-01 2025-10-03   \n",
       "\n",
       "     snap_ym         source_file  mob  installments_missed first_missed_date  \\\n",
       "0 2023-01-01  lms_loan_daily.csv    0                    0               NaT   \n",
       "1 2023-01-01  lms_loan_daily.csv    0                    0               NaT   \n",
       "2 2023-01-01  lms_loan_daily.csv    0                    0               NaT   \n",
       "3 2023-01-01  lms_loan_daily.csv    0                    0               NaT   \n",
       "4 2023-01-01  lms_loan_daily.csv    0                    0               NaT   \n",
       "\n",
       "   dpd  \n",
       "0    0  \n",
       "1    0  \n",
       "2    0  \n",
       "3    0  \n",
       "4    0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = \"datamart/silver/loan_daily/silver_loan_daily_2023_01_01.parquet\"\n",
    "df = pd.read_parquet(path, engine=\"fastparquet\")\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6205bd8c-61ba-4f0a-8235-ae034ad1d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows, Cols: (8974, 5)\n",
      "Columns: ['loan_id', 'Customer_ID', 'label', 'label_def', 'snapshot_date']\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_id</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>label</th>\n",
       "      <th>label_def</th>\n",
       "      <th>snapshot_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUS_0x1037_2023_01_01</td>\n",
       "      <td>CUS_0x1037</td>\n",
       "      <td>0</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>2023-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUS_0x1069_2023_01_01</td>\n",
       "      <td>CUS_0x1069</td>\n",
       "      <td>0</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>2023-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUS_0x114a_2023_01_01</td>\n",
       "      <td>CUS_0x114a</td>\n",
       "      <td>0</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>2023-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUS_0x1184_2023_01_01</td>\n",
       "      <td>CUS_0x1184</td>\n",
       "      <td>0</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>2023-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUS_0x1297_2023_01_01</td>\n",
       "      <td>CUS_0x1297</td>\n",
       "      <td>1</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>2023-07-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loan_id Customer_ID  label   label_def snapshot_date\n",
       "0  CUS_0x1037_2023_01_01  CUS_0x1037      0  30dpd_6mob    2023-07-01\n",
       "1  CUS_0x1069_2023_01_01  CUS_0x1069      0  30dpd_6mob    2023-07-01\n",
       "2  CUS_0x114a_2023_01_01  CUS_0x114a      0  30dpd_6mob    2023-07-01\n",
       "3  CUS_0x1184_2023_01_01  CUS_0x1184      0  30dpd_6mob    2023-07-01\n",
       "4  CUS_0x1297_2023_01_01  CUS_0x1297      1  30dpd_6mob    2023-07-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique months: ['2023-07-01', '2023-08-01', '2023-09-01', '2023-10-01', '2023-11-01', '2023-12-01', '2024-01-01', '2024-02-01', '2024-03-01', '2024-04-01'] ... ['2024-10-01', '2024-11-01', '2024-12-01']\n",
      "\n",
      "Label defs: label_def\n",
      "30dpd_6mob    8974\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Overall class balance:\n",
      "label\n",
      "0    71.13\n",
      "1    28.87\n",
      "Name: pct, dtype: float64\n",
      "\n",
      "Delinquency % by month:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds_token</th>\n",
       "      <th>delinq_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>26.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>30.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>32.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>26.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>26.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>26.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>29.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>28.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>26.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>25.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>29.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>30.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>29.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>28.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>32.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>27.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>30.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>31.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ds_token  delinq_pct\n",
       "0   2023-07-01       26.42\n",
       "1   2023-08-01       30.74\n",
       "2   2023-09-01       32.02\n",
       "3   2023-10-01       26.86\n",
       "4   2023-11-01       26.68\n",
       "5   2023-12-01       26.50\n",
       "6   2024-01-01       29.72\n",
       "7   2024-02-01       28.90\n",
       "8   2024-03-01       26.21\n",
       "9   2024-04-01       25.67\n",
       "10  2024-05-01       29.12\n",
       "11  2024-06-01       30.88\n",
       "12  2024-07-01       29.69\n",
       "13  2024-08-01       28.38\n",
       "14  2024-09-01       32.88\n",
       "15  2024-10-01       27.68\n",
       "16  2024-11-01       30.35\n",
       "17  2024-12-01       31.12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate (loan_id, snapshot_date, label_def) rows: 0\n",
      "Non-binary label rows: 0\n",
      "\n",
      "Monthly summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds_token</th>\n",
       "      <th>rows</th>\n",
       "      <th>pos_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>530</td>\n",
       "      <td>26.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>501</td>\n",
       "      <td>30.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>506</td>\n",
       "      <td>32.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>510</td>\n",
       "      <td>26.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>521</td>\n",
       "      <td>26.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>517</td>\n",
       "      <td>26.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>471</td>\n",
       "      <td>29.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>481</td>\n",
       "      <td>28.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>454</td>\n",
       "      <td>26.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>487</td>\n",
       "      <td>25.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>491</td>\n",
       "      <td>29.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>489</td>\n",
       "      <td>30.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>485</td>\n",
       "      <td>29.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>518</td>\n",
       "      <td>28.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>511</td>\n",
       "      <td>32.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>513</td>\n",
       "      <td>27.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>491</td>\n",
       "      <td>30.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>498</td>\n",
       "      <td>31.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ds_token  rows  pos_rate\n",
       "0   2023-07-01   530     26.42\n",
       "1   2023-08-01   501     30.74\n",
       "2   2023-09-01   506     32.02\n",
       "3   2023-10-01   510     26.86\n",
       "4   2023-11-01   521     26.68\n",
       "5   2023-12-01   517     26.50\n",
       "6   2024-01-01   471     29.72\n",
       "7   2024-02-01   481     28.90\n",
       "8   2024-03-01   454     26.21\n",
       "9   2024-04-01   487     25.67\n",
       "10  2024-05-01   491     29.12\n",
       "11  2024-06-01   489     30.88\n",
       "12  2024-07-01   485     29.69\n",
       "13  2024-08-01   518     28.38\n",
       "14  2024-09-01   511     32.88\n",
       "15  2024-10-01   513     27.68\n",
       "16  2024-11-01   491     30.35\n",
       "17  2024-12-01   498     31.12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive label samples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_id</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>label_def</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>CUS_0x2b5d_2023_12_01</td>\n",
       "      <td>CUS_0x2b5d</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>CUS_0x2b29_2023_09_01</td>\n",
       "      <td>CUS_0x2b29</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8922</th>\n",
       "      <td>CUS_0xb9da_2024_06_01</td>\n",
       "      <td>CUS_0xb9da</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>CUS_0xa59_2023_03_01</td>\n",
       "      <td>CUS_0xa59</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>CUS_0x9be9_2023_05_01</td>\n",
       "      <td>CUS_0x9be9</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>CUS_0x452f_2023_02_01</td>\n",
       "      <td>CUS_0x452f</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>CUS_0x1324_2023_03_01</td>\n",
       "      <td>CUS_0x1324</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7179</th>\n",
       "      <td>CUS_0x6ae7_2024_03_01</td>\n",
       "      <td>CUS_0x6ae7</td>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>CUS_0x7694_2023_09_01</td>\n",
       "      <td>CUS_0x7694</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>CUS_0x33aa_2023_02_01</td>\n",
       "      <td>CUS_0x33aa</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    loan_id Customer_ID snapshot_date   label_def  label\n",
       "5544  CUS_0x2b5d_2023_12_01  CUS_0x2b5d    2024-06-01  30dpd_6mob      1\n",
       "4098  CUS_0x2b29_2023_09_01  CUS_0x2b29    2024-03-01  30dpd_6mob      1\n",
       "8922  CUS_0xb9da_2024_06_01  CUS_0xb9da    2024-12-01  30dpd_6mob      1\n",
       "1447   CUS_0xa59_2023_03_01   CUS_0xa59    2023-09-01  30dpd_6mob      1\n",
       "2447  CUS_0x9be9_2023_05_01  CUS_0x9be9    2023-11-01  30dpd_6mob      1\n",
       "672   CUS_0x452f_2023_02_01  CUS_0x452f    2023-08-01  30dpd_6mob      1\n",
       "1039  CUS_0x1324_2023_03_01  CUS_0x1324    2023-09-01  30dpd_6mob      1\n",
       "7179  CUS_0x6ae7_2024_03_01  CUS_0x6ae7    2024-09-01  30dpd_6mob      1\n",
       "4282  CUS_0x7694_2023_09_01  CUS_0x7694    2024-03-01  30dpd_6mob      1\n",
       "618   CUS_0x33aa_2023_02_01  CUS_0x33aa    2023-08-01  30dpd_6mob      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "GOLD_DIR = \"datamart/gold/label_store\"  # adjust if different\n",
    "\n",
    "# 1) Load all months\n",
    "paths = sorted(glob.glob(os.path.join(GOLD_DIR, \"gold_label_store_*.parquet\")))\n",
    "assert paths, f\"No gold files found under {GOLD_DIR}\"\n",
    "dfs = [pd.read_parquet(p, engine=\"fastparquet\") for p in paths]\n",
    "gold = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 2) Quick shape & schema\n",
    "print(\"Rows, Cols:\", gold.shape)\n",
    "print(\"Columns:\", gold.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "display(gold.head())\n",
    "\n",
    "# 3) What months & label definitions do we have?\n",
    "gold[\"ds_token\"] = gold[\"snapshot_date\"].astype(str)  # if already string, this is a no-op\n",
    "print(\"\\nUnique months:\", sorted(gold[\"ds_token\"].unique())[:10], \"...\",\n",
    "      sorted(gold[\"ds_token\"].unique())[-3:])\n",
    "print(\"\\nLabel defs:\", gold[\"label_def\"].value_counts())\n",
    "\n",
    "# 4) Class balance overall and by month\n",
    "print(\"\\nOverall class balance:\")\n",
    "print(gold[\"label\"].value_counts(normalize=True).mul(100).round(2).rename(\"pct\"))\n",
    "\n",
    "by_month = (gold.groupby(\"ds_token\")[\"label\"]\n",
    "            .mean().mul(100).round(2).rename(\"delinq_pct\"))\n",
    "print(\"\\nDelinquency % by month:\")\n",
    "display(by_month.reset_index().sort_values(\"ds_token\"))\n",
    "\n",
    "# 5) Uniqueness checks (should be one row per loan per month per label_def)\n",
    "dups = (gold.groupby([\"loan_id\",\"snapshot_date\",\"label_def\"])\n",
    "             .size().reset_index(name=\"n\").query(\"n > 1\"))\n",
    "print(\"\\nDuplicate (loan_id, snapshot_date, label_def) rows:\", len(dups))\n",
    "if not dups.empty:\n",
    "    display(dups.head())\n",
    "\n",
    "# 6) Sanity: label is only 0/1\n",
    "bad_labels = gold.loc[~gold[\"label\"].isin([0,1])]\n",
    "print(\"Non-binary label rows:\", len(bad_labels))\n",
    "\n",
    "# 7) Counts per month (rows, pos rate)\n",
    "summary = (gold.assign(pos=gold[\"label\"].astype(int))\n",
    "                .groupby(\"ds_token\")\n",
    "                .agg(rows=(\"loan_id\",\"size\"),\n",
    "                     pos_rate=(\"pos\",\"mean\"))\n",
    "                .assign(pos_rate=lambda d: (d[\"pos_rate\"]*100).round(2))\n",
    "                .reset_index()\n",
    "                .sort_values(\"ds_token\"))\n",
    "print(\"\\nMonthly summary:\")\n",
    "display(summary.head(20))\n",
    "\n",
    "# 8) Peek a few positive labels\n",
    "pos = gold.query(\"label == 1\").sample(min(10, (gold['label']==1).sum()), random_state=42)\n",
    "print(\"\\nPositive label samples:\")\n",
    "display(pos[[\"loan_id\",\"Customer_ID\",\"snapshot_date\",\"label_def\",\"label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c13fe4-05a2-49d8-b103-3c58f26b2a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available label_defs: {'30dpd_6mob': 8974}\n",
      "MOB=0 label rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_id</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>label</th>\n",
       "      <th>label_def</th>\n",
       "      <th>snapshot_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [loan_id, Customer_ID, label, label_def, snapshot_date]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob, pandas as pd\n",
    "\n",
    "GOLD_DIR = \"datamart/gold/label_store\"\n",
    "paths = sorted(glob.glob(os.path.join(GOLD_DIR, \"gold_label_store_*.parquet\")))\n",
    "gold = pd.concat([pd.read_parquet(p, engine=\"fastparquet\") for p in paths], ignore_index=True)\n",
    "\n",
    "print(\"Available label_defs:\", gold[\"label_def\"].value_counts().to_dict())\n",
    "\n",
    "# Filter to MOB=0 labels if they exist\n",
    "mob0 = gold[gold[\"label_def\"].str.endswith(\"_0mob\", na=False)]\n",
    "print(\"MOB=0 label rows:\", len(mob0))\n",
    "mob0.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae630de-c169-40dd-877e-d21b46d7fb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Columns in Silver Financials Table:\n",
      "['customer_id', 'snapshot_date', 'ingest_dt', 'snap_ym', 'source_file', 'annual_income', 'monthly_inhand_salary', 'interest_rate', 'num_of_loan', 'delay_from_due_date', 'num_of_delayed_payment', 'changed_credit_limit', 'num_credit_inquiries', 'credit_mix', 'outstanding_debt', 'credit_utilization_ratio', 'credit_history_age', 'payment_of_min_amount', 'total_emi_per_month', 'amount_invested_monthly', 'payment_behaviour', 'monthly_balance', 'type_of_loan']\n",
      "\n",
      "📊 Head (top 10 rows):\n",
      "+-----------+-------------+-------------------+-------+-----------------------+-------------+---------------------+-------------+-----------+-------------------+----------------------+--------------------+--------------------+----------+----------------+------------------------+------------------+---------------------+-------------------+-----------------------+--------------------------------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|customer_id|snapshot_date|ingest_dt          |snap_ym|source_file            |annual_income|monthly_inhand_salary|interest_rate|num_of_loan|delay_from_due_date|num_of_delayed_payment|changed_credit_limit|num_credit_inquiries|credit_mix|outstanding_debt|credit_utilization_ratio|credit_history_age|payment_of_min_amount|total_emi_per_month|amount_invested_monthly|payment_behaviour               |monthly_balance|type_of_loan                                                                                                                               |\n",
      "+-----------+-------------+-------------------+-------+-----------------------+-------------+---------------------+-------------+-----------+-------------------+----------------------+--------------------+--------------------+----------+----------------+------------------------+------------------+---------------------+-------------------+-----------------------+--------------------------------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CUS_0x1037 |2023-01-01   |2025-10-03 00:00:00|2023-01|features_financials.csv|15989.09     |1086.42              |2.0          |4          |13                 |15                    |0.5                 |3                   |Good      |665.82          |40.69769934536393       |237               |No                   |33.797020629881075 |80.47                  |Low_spent_Small_value_payments  |284.38         |Credit-Builder Loan, Auto Loan, Auto Loan, and Mortgage Loan                                                                               |\n",
      "|CUS_0x1069 |2023-01-01   |2025-10-03 00:00:00|2023-01|features_financials.csv|58637.34     |4799.45              |10.0         |3          |9                  |17                    |12.56               |5                   |Standard  |208.8           |25.23314363376225       |368               |Yes                  |139.8850133927109  |165.21                 |High_spent_Small_value_payments |434.85         |Personal Loan, Auto Loan, and Not Specified                                                                                                |\n",
      "|CUS_0x114a |2023-01-01   |2025-10-03 00:00:00|2023-01|features_financials.csv|15305.46     |1230.46              |2.0          |2          |14                 |2                     |15.95               |0                   |Good      |642.42          |27.52511297681479       |189               |No                   |20.301653868439384 |64.78                  |Low_spent_Small_value_payments  |327.97         |Student Loan, and Home Equity Loan                                                                                                         |\n",
      "|CUS_0x1184 |2023-01-01   |2025-10-03 00:00:00|2023-01|features_financials.csv|19867.48     |1396.62              |11.0         |3          |10                 |9                     |6.74                |4                   |Good      |707.29          |26.68978981173729       |392               |No                   |42.60688196261637  |23.46                  |NULL                            |313.59         |Student Loan, Mortgage Loan, and Payday Loan                                                                                               |\n",
      "|CUS_0x1297 |2023-01-01   |2025-10-03 00:00:00|2023-01|features_financials.csv|57738.06     |4881.51              |30.0         |9          |61                 |24                    |14.27               |11                  |Bad       |3916.47         |25.74214265617326       |164               |Yes                  |296.2841357039575  |53.82                  |High_spent_Medium_value_payments|388.05         |Payday Loan, Personal Loan, Payday Loan, Personal Loan, Mortgage Loan, Home Equity Loan, Student Loan, Credit-Builder Loan, and Payday Loan|\n",
      "|CUS_0x12fb |2023-01-01   |2025-10-03 00:00:00|2023-01|features_financials.csv|26342.91     |1949.24              |7.0          |1          |23                 |14                    |17.6                |3                   |Standard  |397.9           |33.21173803384935       |264               |Yes                  |57567.0            |48.06                  |High_spent_Large_value_payments |365.28         |Credit-Builder Loan                                                                                                                        |\n",
      "|CUS_0x1325 |2023-01-01   |2025-10-03 00:00:00|2023-01|features_financials.csv|35823.96     |2937.33              |22.0         |5          |39                 |17                    |7.35                |8                   |Bad       |3029.92         |31.56717092224887       |117               |Yes                  |112.43578465602695 |148.52                 |High_spent_Small_value_payments |292.78         |Not Specified, Personal Loan, Auto Loan, Not Specified, and Debt Consolidation Loan                                                        |\n",
      "|CUS_0x1341 |2023-01-01   |2025-10-03 00:00:00|2023-01|features_financials.csv|15341.9      |1035.49              |3.0          |1          |20                 |8                     |7.71                |2                   |NULL      |1152.71         |24.134405197385306      |256               |NM                   |6.9819930191464366 |81.56                  |Low_spent_Medium_value_payments |295.0          |Debt Consolidation Loan                                                                                                                    |\n",
      "|CUS_0x1375 |2023-01-01   |2025-10-03 00:00:00|2023-01|features_financials.csv|72524.2      |5908.68              |17.0         |3          |46                 |23                    |8.74                |9                   |Bad       |2699.4          |23.070388419207724      |184               |Yes                  |153.03345685022464 |606.46                 |Low_spent_Medium_value_payments |111.38         |Mortgage Loan, Debt Consolidation Loan, and Home Equity Loan                                                                               |\n",
      "|CUS_0x13a8 |2023-01-01   |2025-10-03 00:00:00|2023-01|features_financials.csv|97059.0      |8295.25              |9.0          |0          |9                  |10                    |9.94                |5                   |Good      |192.99          |39.9399690577729        |401               |No                   |0.0                |210.62                 |High_spent_Large_value_payments |858.91         |NULL                                                                                                                                       |\n",
      "+-----------+-------------+-------------------+-------+-----------------------+-------------+---------------------+-------------+-----------+-------------------+----------------------+--------------------+--------------------+----------+----------------+------------------------+------------------+---------------------+-------------------+-----------------------+--------------------------------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "📐 Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- ingest_dt: timestamp (nullable = true)\n",
      " |-- snap_ym: string (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      " |-- annual_income: double (nullable = true)\n",
      " |-- monthly_inhand_salary: double (nullable = true)\n",
      " |-- interest_rate: double (nullable = true)\n",
      " |-- num_of_loan: integer (nullable = true)\n",
      " |-- delay_from_due_date: integer (nullable = true)\n",
      " |-- num_of_delayed_payment: integer (nullable = true)\n",
      " |-- changed_credit_limit: double (nullable = true)\n",
      " |-- num_credit_inquiries: integer (nullable = true)\n",
      " |-- credit_mix: string (nullable = true)\n",
      " |-- outstanding_debt: double (nullable = true)\n",
      " |-- credit_utilization_ratio: double (nullable = true)\n",
      " |-- credit_history_age: integer (nullable = true)\n",
      " |-- payment_of_min_amount: string (nullable = true)\n",
      " |-- total_emi_per_month: double (nullable = true)\n",
      " |-- amount_invested_monthly: double (nullable = true)\n",
      " |-- payment_behaviour: string (nullable = true)\n",
      " |-- monthly_balance: double (nullable = true)\n",
      " |-- type_of_loan: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1️⃣ Start Spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"CheckSilverFinancials\").getOrCreate()\n",
    "\n",
    "# 2️⃣ Load one month’s silver financials file\n",
    "silver_path = \"datamart/silver/features/financials/silver_financials_2023_01_01.parquet\"  # <-- change month if needed\n",
    "df = spark.read.parquet(silver_path)\n",
    "\n",
    "# 3️⃣ Show all columns\n",
    "print(\"✅ Columns in Silver Financials Table:\")\n",
    "print(df.columns)\n",
    "\n",
    "# 4️⃣ Preview top 10 rows\n",
    "print(\"\\n📊 Head (top 10 rows):\")\n",
    "df.show(10, truncate=False)\n",
    "\n",
    "# 5️⃣ Optional: print schema for data types too\n",
    "print(\"\\n📐 Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# 6️⃣ Stop Spark (optional if script ends)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0dfe7c7-fe02-41ae-bac2-185832f6628b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowKeyError",
     "evalue": "A type extension with name pandas.period already defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1️⃣ Load the Silver financials parquet file\u001b[39;00m\n\u001b[32m      4\u001b[39m silver_path = \u001b[33m\"\u001b[39m\u001b[33mdatamart/silver/features/financials/silver_financials_2023_01_01.parquet\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# change the date if needed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msilver_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 2️⃣ List all columns\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Columns in Silver Financials Table:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/parquet.py:651\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    500\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    508\u001b[39m     **kwargs,\n\u001b[32m    509\u001b[39m ) -> DataFrame:\n\u001b[32m    510\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    512\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    654\u001b[39m         msg = (\n\u001b[32m    655\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    656\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    657\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/parquet.py:63\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     65\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/io/parquet.py:169\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pandas/core/arrays/arrow/extension_types.py:59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# register the type with a dummy instance\u001b[39;00m\n\u001b[32m     58\u001b[39m _period_type = ArrowPeriodType(\u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_period_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mArrowIntervalType\u001b[39;00m(pyarrow.ExtensionType):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, subtype, closed: IntervalClosedType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m# attributes need to be set first before calling\u001b[39;00m\n\u001b[32m     65\u001b[39m         \u001b[38;5;66;03m# super init (as that calls serialize)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyarrow/types.pxi:2226\u001b[39m, in \u001b[36mpyarrow.lib.register_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: A type extension with name pandas.period already defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Load the Silver financials parquet file\n",
    "silver_path = \"datamart/silver/features/financials/silver_financials_2023_01_01.parquet\"  # change the date if needed\n",
    "df = pd.read_parquet(silver_path)\n",
    "\n",
    "# 2️⃣ List all columns\n",
    "print(\"✅ Columns in Silver Financials Table:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# 3️⃣ Show the first 5 rows (head)\n",
    "print(\"\\n📊 Head (first 5 rows):\")\n",
    "print(df.head())\n",
    "\n",
    "# 4️⃣ (Optional) Show shape for quick overview\n",
    "print(\"\\n📐 Shape of dataframe:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "164eee5c-8141-43e8-94b5-c51c2b6e3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 silver financial files\n",
      "✅ Columns in Silver Financials:\n",
      "['customer_id', 'snapshot_date', 'ingest_dt', 'snap_ym', 'source_file', 'annual_income', 'monthly_inhand_salary', 'interest_rate', 'num_of_loan', 'delay_from_due_date', 'num_of_delayed_payment', 'changed_credit_limit', 'num_credit_inquiries', 'credit_mix', 'outstanding_debt', 'credit_utilization_ratio', 'credit_history_age', 'payment_of_min_amount', 'total_emi_per_month', 'amount_invested_monthly', 'payment_behaviour', 'monthly_balance', 'type_of_loan']\n",
      "\n",
      "📐 Shape: (11974, 23)\n",
      "\n",
      "📊 Head (first 10 rows):\n",
      "  customer_id snapshot_date  ingest_dt  snap_ym              source_file  \\\n",
      "0  CUS_0x1037    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
      "1  CUS_0x1069    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
      "2  CUS_0x114a    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
      "3  CUS_0x1184    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
      "4  CUS_0x1297    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
      "5  CUS_0x12fb    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
      "6  CUS_0x1325    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
      "7  CUS_0x1341    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
      "8  CUS_0x1375    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
      "9  CUS_0x13a8    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
      "\n",
      "   annual_income  monthly_inhand_salary  interest_rate  num_of_loan  \\\n",
      "0       15989.09                1086.42            2.0            4   \n",
      "1       58637.34                4799.45           10.0            3   \n",
      "2       15305.46                1230.46            2.0            2   \n",
      "3       19867.48                1396.62           11.0            3   \n",
      "4       57738.06                4881.51           30.0            9   \n",
      "5       26342.91                1949.24            7.0            1   \n",
      "6       35823.96                2937.33           22.0            5   \n",
      "7       15341.90                1035.49            3.0            1   \n",
      "8       72524.20                5908.68           17.0            3   \n",
      "9       97059.00                8295.25            9.0            0   \n",
      "\n",
      "   delay_from_due_date  ...  credit_mix  outstanding_debt  \\\n",
      "0                   13  ...        Good            665.82   \n",
      "1                    9  ...    Standard            208.80   \n",
      "2                   14  ...        Good            642.42   \n",
      "3                   10  ...        Good            707.29   \n",
      "4                   61  ...         Bad           3916.47   \n",
      "5                   23  ...    Standard            397.90   \n",
      "6                   39  ...         Bad           3029.92   \n",
      "7                   20  ...        None           1152.71   \n",
      "8                   46  ...         Bad           2699.40   \n",
      "9                    9  ...        Good            192.99   \n",
      "\n",
      "   credit_utilization_ratio credit_history_age  payment_of_min_amount  \\\n",
      "0                 40.697699                237                     No   \n",
      "1                 25.233144                368                    Yes   \n",
      "2                 27.525113                189                     No   \n",
      "3                 26.689790                392                     No   \n",
      "4                 25.742143                164                    Yes   \n",
      "5                 33.211738                264                    Yes   \n",
      "6                 31.567171                117                    Yes   \n",
      "7                 24.134405                256                     NM   \n",
      "8                 23.070388                184                    Yes   \n",
      "9                 39.939969                401                     No   \n",
      "\n",
      "   total_emi_per_month  amount_invested_monthly  \\\n",
      "0            33.797021                    80.47   \n",
      "1           139.885013                   165.21   \n",
      "2            20.301654                    64.78   \n",
      "3            42.606882                    23.46   \n",
      "4           296.284136                    53.82   \n",
      "5         57567.000000                    48.06   \n",
      "6           112.435785                   148.52   \n",
      "7             6.981993                    81.56   \n",
      "8           153.033457                   606.46   \n",
      "9             0.000000                   210.62   \n",
      "\n",
      "                  payment_behaviour  monthly_balance  \\\n",
      "0    Low_spent_Small_value_payments           284.38   \n",
      "1   High_spent_Small_value_payments           434.85   \n",
      "2    Low_spent_Small_value_payments           327.97   \n",
      "3                              None           313.59   \n",
      "4  High_spent_Medium_value_payments           388.05   \n",
      "5   High_spent_Large_value_payments           365.28   \n",
      "6   High_spent_Small_value_payments           292.78   \n",
      "7   Low_spent_Medium_value_payments           295.00   \n",
      "8   Low_spent_Medium_value_payments           111.38   \n",
      "9   High_spent_Large_value_payments           858.91   \n",
      "\n",
      "                                        type_of_loan  \n",
      "0  Credit-Builder Loan, Auto Loan, Auto Loan, and...  \n",
      "1        Personal Loan, Auto Loan, and Not Specified  \n",
      "2                 Student Loan, and Home Equity Loan  \n",
      "3       Student Loan, Mortgage Loan, and Payday Loan  \n",
      "4  Payday Loan, Personal Loan, Payday Loan, Perso...  \n",
      "5                                Credit-Builder Loan  \n",
      "6  Not Specified, Personal Loan, Auto Loan, Not S...  \n",
      "7                            Debt Consolidation Loan  \n",
      "8  Mortgage Loan, Debt Consolidation Loan, and Ho...  \n",
      "9                                               None  \n",
      "\n",
      "[10 rows x 23 columns]\n",
      "\n",
      "🔎 Null counts:\n",
      "credit_mix                  2505\n",
      "type_of_loan                1368\n",
      "payment_behaviour            953\n",
      "amount_invested_monthly      540\n",
      "changed_credit_limit         246\n",
      "monthly_balance                1\n",
      "customer_id                    0\n",
      "monthly_inhand_salary          0\n",
      "annual_income                  0\n",
      "source_file                    0\n",
      "snap_ym                        0\n",
      "ingest_dt                      0\n",
      "snapshot_date                  0\n",
      "interest_rate                  0\n",
      "num_of_loan                    0\n",
      "outstanding_debt               0\n",
      "num_credit_inquiries           0\n",
      "delay_from_due_date            0\n",
      "num_of_delayed_payment         0\n",
      "total_emi_per_month            0\n",
      "payment_of_min_amount          0\n",
      "credit_history_age             0\n",
      "credit_utilization_ratio       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Define the silver financials directory\n",
    "SILVER_DIR = \"datamart/silver/features/financials\"\n",
    "\n",
    "# 2️⃣ Find all monthly parquet files\n",
    "paths = sorted(glob.glob(os.path.join(SILVER_DIR, \"silver_financials_*.parquet\")))\n",
    "print(f\"Found {len(paths)} silver financial files\")\n",
    "\n",
    "# 3️⃣ Load them into a single DataFrame (using fastparquet)\n",
    "silver_financials = pd.concat([pd.read_parquet(p, engine=\"fastparquet\") for p in paths], ignore_index=True)\n",
    "\n",
    "# 4️⃣ Show available columns\n",
    "print(\"✅ Columns in Silver Financials:\")\n",
    "print(silver_financials.columns.tolist())\n",
    "\n",
    "# 5️⃣ Show shape (rows, cols)\n",
    "print(\"\\n📐 Shape:\", silver_financials.shape)\n",
    "\n",
    "# 6️⃣ Preview first 10 rows\n",
    "print(\"\\n📊 Head (first 10 rows):\")\n",
    "print(silver_financials.head(10))\n",
    "\n",
    "# 7️⃣ (Optional) Null counts for data quality\n",
    "print(\"\\n🔎 Null counts:\")\n",
    "print(silver_financials.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36b5e4e2-a8c6-43ff-a761-7caaa03cf6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>ingest_dt</th>\n",
       "      <th>snap_ym</th>\n",
       "      <th>source_file</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>monthly_inhand_salary</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>num_of_loan</th>\n",
       "      <th>delay_from_due_date</th>\n",
       "      <th>...</th>\n",
       "      <th>credit_mix</th>\n",
       "      <th>outstanding_debt</th>\n",
       "      <th>credit_utilization_ratio</th>\n",
       "      <th>credit_history_age</th>\n",
       "      <th>payment_of_min_amount</th>\n",
       "      <th>total_emi_per_month</th>\n",
       "      <th>amount_invested_monthly</th>\n",
       "      <th>payment_behaviour</th>\n",
       "      <th>monthly_balance</th>\n",
       "      <th>type_of_loan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUS_0x1037</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>15989.09</td>\n",
       "      <td>1086.42</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>665.82</td>\n",
       "      <td>40.697699</td>\n",
       "      <td>237</td>\n",
       "      <td>No</td>\n",
       "      <td>33.797021</td>\n",
       "      <td>80.47</td>\n",
       "      <td>Low_spent_Small_value_payments</td>\n",
       "      <td>284.38</td>\n",
       "      <td>Credit-Builder Loan, Auto Loan, Auto Loan, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUS_0x1069</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>58637.34</td>\n",
       "      <td>4799.45</td>\n",
       "      <td>10.00</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Standard</td>\n",
       "      <td>208.80</td>\n",
       "      <td>25.233144</td>\n",
       "      <td>368</td>\n",
       "      <td>Yes</td>\n",
       "      <td>139.885013</td>\n",
       "      <td>165.21</td>\n",
       "      <td>High_spent_Small_value_payments</td>\n",
       "      <td>434.85</td>\n",
       "      <td>Personal Loan, Auto Loan, and Not Specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUS_0x114a</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>15305.46</td>\n",
       "      <td>1230.46</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>642.42</td>\n",
       "      <td>27.525113</td>\n",
       "      <td>189</td>\n",
       "      <td>No</td>\n",
       "      <td>20.301654</td>\n",
       "      <td>64.78</td>\n",
       "      <td>Low_spent_Small_value_payments</td>\n",
       "      <td>327.97</td>\n",
       "      <td>Student Loan, and Home Equity Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUS_0x1184</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>19867.48</td>\n",
       "      <td>1396.62</td>\n",
       "      <td>11.00</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>707.29</td>\n",
       "      <td>26.689790</td>\n",
       "      <td>392</td>\n",
       "      <td>No</td>\n",
       "      <td>42.606882</td>\n",
       "      <td>23.46</td>\n",
       "      <td>None</td>\n",
       "      <td>313.59</td>\n",
       "      <td>Student Loan, Mortgage Loan, and Payday Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUS_0x1297</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>57738.06</td>\n",
       "      <td>4881.51</td>\n",
       "      <td>30.00</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>3916.47</td>\n",
       "      <td>25.742143</td>\n",
       "      <td>164</td>\n",
       "      <td>Yes</td>\n",
       "      <td>296.284136</td>\n",
       "      <td>53.82</td>\n",
       "      <td>High_spent_Medium_value_payments</td>\n",
       "      <td>388.05</td>\n",
       "      <td>Payday Loan, Personal Loan, Payday Loan, Perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CUS_0x12fb</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>26342.91</td>\n",
       "      <td>1949.24</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>Standard</td>\n",
       "      <td>397.90</td>\n",
       "      <td>33.211738</td>\n",
       "      <td>264</td>\n",
       "      <td>Yes</td>\n",
       "      <td>57567.000000</td>\n",
       "      <td>48.06</td>\n",
       "      <td>High_spent_Large_value_payments</td>\n",
       "      <td>365.28</td>\n",
       "      <td>Credit-Builder Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CUS_0x1325</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>35823.96</td>\n",
       "      <td>2937.33</td>\n",
       "      <td>22.00</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>3029.92</td>\n",
       "      <td>31.567171</td>\n",
       "      <td>117</td>\n",
       "      <td>Yes</td>\n",
       "      <td>112.435785</td>\n",
       "      <td>148.52</td>\n",
       "      <td>High_spent_Small_value_payments</td>\n",
       "      <td>292.78</td>\n",
       "      <td>Not Specified, Personal Loan, Auto Loan, Not S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CUS_0x1341</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>15341.90</td>\n",
       "      <td>1035.49</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1152.71</td>\n",
       "      <td>24.134405</td>\n",
       "      <td>256</td>\n",
       "      <td>NM</td>\n",
       "      <td>6.981993</td>\n",
       "      <td>81.56</td>\n",
       "      <td>Low_spent_Medium_value_payments</td>\n",
       "      <td>295.00</td>\n",
       "      <td>Debt Consolidation Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CUS_0x1375</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>72524.20</td>\n",
       "      <td>5908.68</td>\n",
       "      <td>17.00</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>2699.40</td>\n",
       "      <td>23.070388</td>\n",
       "      <td>184</td>\n",
       "      <td>Yes</td>\n",
       "      <td>153.033457</td>\n",
       "      <td>606.46</td>\n",
       "      <td>Low_spent_Medium_value_payments</td>\n",
       "      <td>111.38</td>\n",
       "      <td>Mortgage Loan, Debt Consolidation Loan, and Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CUS_0x13a8</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>97059.00</td>\n",
       "      <td>8295.25</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>192.99</td>\n",
       "      <td>39.939969</td>\n",
       "      <td>401</td>\n",
       "      <td>No</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>210.62</td>\n",
       "      <td>High_spent_Large_value_payments</td>\n",
       "      <td>858.91</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CUS_0x13ef</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>18666.56</td>\n",
       "      <td>1323.55</td>\n",
       "      <td>11.00</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>770.81</td>\n",
       "      <td>35.323932</td>\n",
       "      <td>387</td>\n",
       "      <td>No</td>\n",
       "      <td>58.886044</td>\n",
       "      <td>106.09</td>\n",
       "      <td>Low_spent_Small_value_payments</td>\n",
       "      <td>257.38</td>\n",
       "      <td>Student Loan, Student Loan, Payday Loan, and C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CUS_0x1440</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>7780.49</td>\n",
       "      <td>368.37</td>\n",
       "      <td>33.00</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>4108.42</td>\n",
       "      <td>35.147265</td>\n",
       "      <td>153</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38.107871</td>\n",
       "      <td>19.93</td>\n",
       "      <td>Low_spent_Large_value_payments</td>\n",
       "      <td>248.80</td>\n",
       "      <td>Personal Loan, Debt Consolidation Loan, Person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CUS_0x1443</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>88120.12</td>\n",
       "      <td>7377.34</td>\n",
       "      <td>15.00</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>Standard</td>\n",
       "      <td>534.85</td>\n",
       "      <td>26.828954</td>\n",
       "      <td>404</td>\n",
       "      <td>No</td>\n",
       "      <td>164.978777</td>\n",
       "      <td>99.25</td>\n",
       "      <td>High_spent_Large_value_payments</td>\n",
       "      <td>713.51</td>\n",
       "      <td>Student Loan, Personal Loan, Auto Loan, and St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CUS_0x145a</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>41587.37</td>\n",
       "      <td>3704.61</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>1353.86</td>\n",
       "      <td>26.198926</td>\n",
       "      <td>198</td>\n",
       "      <td>No</td>\n",
       "      <td>68.539914</td>\n",
       "      <td>45.80</td>\n",
       "      <td>High_spent_Medium_value_payments</td>\n",
       "      <td>506.12</td>\n",
       "      <td>Not Specified, and Mortgage Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CUS_0x1492</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>9134.42</td>\n",
       "      <td>1051.20</td>\n",
       "      <td>15.00</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>Standard</td>\n",
       "      <td>1621.28</td>\n",
       "      <td>33.778794</td>\n",
       "      <td>194</td>\n",
       "      <td>Yes</td>\n",
       "      <td>15.953677</td>\n",
       "      <td>92.91</td>\n",
       "      <td>Low_spent_Small_value_payments</td>\n",
       "      <td>286.26</td>\n",
       "      <td>Payday Loan, Mortgage Loan, and Student Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CUS_0x153d</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>8538.41</td>\n",
       "      <td>992.53</td>\n",
       "      <td>30.00</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>Bad</td>\n",
       "      <td>4171.09</td>\n",
       "      <td>29.328462</td>\n",
       "      <td>21</td>\n",
       "      <td>Yes</td>\n",
       "      <td>27.175783</td>\n",
       "      <td>35.10</td>\n",
       "      <td>Low_spent_Small_value_payments</td>\n",
       "      <td>326.97</td>\n",
       "      <td>Payday Loan, Not Specified, Payday Loan, Perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CUS_0x1567</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>19363.33</td>\n",
       "      <td>1904.61</td>\n",
       "      <td>50.59</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>Standard</td>\n",
       "      <td>111.66</td>\n",
       "      <td>24.394828</td>\n",
       "      <td>174</td>\n",
       "      <td>NM</td>\n",
       "      <td>30.693931</td>\n",
       "      <td>49.63</td>\n",
       "      <td>High_spent_Medium_value_payments</td>\n",
       "      <td>360.13</td>\n",
       "      <td>Credit-Builder Loan, and Home Equity Loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CUS_0x15ad</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>82316.18</td>\n",
       "      <td>6625.68</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>42.98</td>\n",
       "      <td>31.788692</td>\n",
       "      <td>220</td>\n",
       "      <td>No</td>\n",
       "      <td>133.893759</td>\n",
       "      <td>161.21</td>\n",
       "      <td>High_spent_Large_value_payments</td>\n",
       "      <td>607.47</td>\n",
       "      <td>Not Specified, Debt Consolidation Loan, and Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CUS_0x1630</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>17706148.00</td>\n",
       "      <td>6943.13</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>Standard</td>\n",
       "      <td>377.05</td>\n",
       "      <td>32.086623</td>\n",
       "      <td>134</td>\n",
       "      <td>Yes</td>\n",
       "      <td>340.287730</td>\n",
       "      <td>396.25</td>\n",
       "      <td>Low_spent_Medium_value_payments</td>\n",
       "      <td>237.78</td>\n",
       "      <td>Not Specified, Student Loan, Personal Loan, No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CUS_0x169c</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-03</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>features_financials.csv</td>\n",
       "      <td>11646.93</td>\n",
       "      <td>1135.58</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>Good</td>\n",
       "      <td>782.26</td>\n",
       "      <td>24.488122</td>\n",
       "      <td>395</td>\n",
       "      <td>No</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.29</td>\n",
       "      <td>Low_spent_Large_value_payments</td>\n",
       "      <td>311.27</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id snapshot_date  ingest_dt  snap_ym              source_file  \\\n",
       "0   CUS_0x1037    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "1   CUS_0x1069    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "2   CUS_0x114a    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "3   CUS_0x1184    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "4   CUS_0x1297    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "5   CUS_0x12fb    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "6   CUS_0x1325    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "7   CUS_0x1341    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "8   CUS_0x1375    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "9   CUS_0x13a8    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "10  CUS_0x13ef    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "11  CUS_0x1440    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "12  CUS_0x1443    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "13  CUS_0x145a    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "14  CUS_0x1492    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "15  CUS_0x153d    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "16  CUS_0x1567    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "17  CUS_0x15ad    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "18  CUS_0x1630    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "19  CUS_0x169c    2023-01-01 2025-10-03  2023-01  features_financials.csv   \n",
       "\n",
       "    annual_income  monthly_inhand_salary  interest_rate  num_of_loan  \\\n",
       "0        15989.09                1086.42           2.00            4   \n",
       "1        58637.34                4799.45          10.00            3   \n",
       "2        15305.46                1230.46           2.00            2   \n",
       "3        19867.48                1396.62          11.00            3   \n",
       "4        57738.06                4881.51          30.00            9   \n",
       "5        26342.91                1949.24           7.00            1   \n",
       "6        35823.96                2937.33          22.00            5   \n",
       "7        15341.90                1035.49           3.00            1   \n",
       "8        72524.20                5908.68          17.00            3   \n",
       "9        97059.00                8295.25           9.00            0   \n",
       "10       18666.56                1323.55          11.00            4   \n",
       "11        7780.49                 368.37          33.00            7   \n",
       "12       88120.12                7377.34          15.00            4   \n",
       "13       41587.37                3704.61           3.00            2   \n",
       "14        9134.42                1051.20          15.00            3   \n",
       "15        8538.41                 992.53          30.00            6   \n",
       "16       19363.33                1904.61          50.59            2   \n",
       "17       82316.18                6625.68           4.00            3   \n",
       "18    17706148.00                6943.13          10.00            6   \n",
       "19       11646.93                1135.58           7.00            0   \n",
       "\n",
       "    delay_from_due_date  ...  credit_mix  outstanding_debt  \\\n",
       "0                    13  ...        Good            665.82   \n",
       "1                     9  ...    Standard            208.80   \n",
       "2                    14  ...        Good            642.42   \n",
       "3                    10  ...        Good            707.29   \n",
       "4                    61  ...         Bad           3916.47   \n",
       "5                    23  ...    Standard            397.90   \n",
       "6                    39  ...         Bad           3029.92   \n",
       "7                    20  ...        None           1152.71   \n",
       "8                    46  ...         Bad           2699.40   \n",
       "9                     9  ...        Good            192.99   \n",
       "10                    3  ...        None            770.81   \n",
       "11                   59  ...         Bad           4108.42   \n",
       "12                    6  ...    Standard            534.85   \n",
       "13                    5  ...        Good           1353.86   \n",
       "14                   18  ...    Standard           1621.28   \n",
       "15                   31  ...         Bad           4171.09   \n",
       "16                    7  ...    Standard            111.66   \n",
       "17                   23  ...        Good             42.98   \n",
       "18                    9  ...    Standard            377.05   \n",
       "19                   26  ...        Good            782.26   \n",
       "\n",
       "    credit_utilization_ratio credit_history_age  payment_of_min_amount  \\\n",
       "0                  40.697699                237                     No   \n",
       "1                  25.233144                368                    Yes   \n",
       "2                  27.525113                189                     No   \n",
       "3                  26.689790                392                     No   \n",
       "4                  25.742143                164                    Yes   \n",
       "5                  33.211738                264                    Yes   \n",
       "6                  31.567171                117                    Yes   \n",
       "7                  24.134405                256                     NM   \n",
       "8                  23.070388                184                    Yes   \n",
       "9                  39.939969                401                     No   \n",
       "10                 35.323932                387                     No   \n",
       "11                 35.147265                153                    Yes   \n",
       "12                 26.828954                404                     No   \n",
       "13                 26.198926                198                     No   \n",
       "14                 33.778794                194                    Yes   \n",
       "15                 29.328462                 21                    Yes   \n",
       "16                 24.394828                174                     NM   \n",
       "17                 31.788692                220                     No   \n",
       "18                 32.086623                134                    Yes   \n",
       "19                 24.488122                395                     No   \n",
       "\n",
       "    total_emi_per_month  amount_invested_monthly  \\\n",
       "0             33.797021                    80.47   \n",
       "1            139.885013                   165.21   \n",
       "2             20.301654                    64.78   \n",
       "3             42.606882                    23.46   \n",
       "4            296.284136                    53.82   \n",
       "5          57567.000000                    48.06   \n",
       "6            112.435785                   148.52   \n",
       "7              6.981993                    81.56   \n",
       "8            153.033457                   606.46   \n",
       "9              0.000000                   210.62   \n",
       "10            58.886044                   106.09   \n",
       "11            38.107871                    19.93   \n",
       "12           164.978777                    99.25   \n",
       "13            68.539914                    45.80   \n",
       "14            15.953677                    92.91   \n",
       "15            27.175783                    35.10   \n",
       "16            30.693931                    49.63   \n",
       "17           133.893759                   161.21   \n",
       "18           340.287730                   396.25   \n",
       "19             0.000000                    72.29   \n",
       "\n",
       "                   payment_behaviour  monthly_balance  \\\n",
       "0     Low_spent_Small_value_payments           284.38   \n",
       "1    High_spent_Small_value_payments           434.85   \n",
       "2     Low_spent_Small_value_payments           327.97   \n",
       "3                               None           313.59   \n",
       "4   High_spent_Medium_value_payments           388.05   \n",
       "5    High_spent_Large_value_payments           365.28   \n",
       "6    High_spent_Small_value_payments           292.78   \n",
       "7    Low_spent_Medium_value_payments           295.00   \n",
       "8    Low_spent_Medium_value_payments           111.38   \n",
       "9    High_spent_Large_value_payments           858.91   \n",
       "10    Low_spent_Small_value_payments           257.38   \n",
       "11    Low_spent_Large_value_payments           248.80   \n",
       "12   High_spent_Large_value_payments           713.51   \n",
       "13  High_spent_Medium_value_payments           506.12   \n",
       "14    Low_spent_Small_value_payments           286.26   \n",
       "15    Low_spent_Small_value_payments           326.97   \n",
       "16  High_spent_Medium_value_payments           360.13   \n",
       "17   High_spent_Large_value_payments           607.47   \n",
       "18   Low_spent_Medium_value_payments           237.78   \n",
       "19    Low_spent_Large_value_payments           311.27   \n",
       "\n",
       "                                         type_of_loan  \n",
       "0   Credit-Builder Loan, Auto Loan, Auto Loan, and...  \n",
       "1         Personal Loan, Auto Loan, and Not Specified  \n",
       "2                  Student Loan, and Home Equity Loan  \n",
       "3        Student Loan, Mortgage Loan, and Payday Loan  \n",
       "4   Payday Loan, Personal Loan, Payday Loan, Perso...  \n",
       "5                                 Credit-Builder Loan  \n",
       "6   Not Specified, Personal Loan, Auto Loan, Not S...  \n",
       "7                             Debt Consolidation Loan  \n",
       "8   Mortgage Loan, Debt Consolidation Loan, and Ho...  \n",
       "9                                                None  \n",
       "10  Student Loan, Student Loan, Payday Loan, and C...  \n",
       "11  Personal Loan, Debt Consolidation Loan, Person...  \n",
       "12  Student Loan, Personal Loan, Auto Loan, and St...  \n",
       "13                   Not Specified, and Mortgage Loan  \n",
       "14       Payday Loan, Mortgage Loan, and Student Loan  \n",
       "15  Payday Loan, Not Specified, Payday Loan, Perso...  \n",
       "16          Credit-Builder Loan, and Home Equity Loan  \n",
       "17  Not Specified, Debt Consolidation Loan, and Pa...  \n",
       "18  Not Specified, Student Loan, Personal Loan, No...  \n",
       "19                                               None  \n",
       "\n",
       "[20 rows x 23 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_financials.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d58b4764-2efd-4766-8e07-e086cdd83226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  9, 14, 10, 61, 23, 39, 20, 46,  3, 59,  6,  5, 18, 31,  7, 26,\n",
       "       12, 22, 56,  0, 19, 30, 54, 29,  8, 17, 24,  1, 25, 27, 16, 15, 35,\n",
       "       36, 44,  2, 11, 42, 28, 62, 47, -3, 21, 53, 40, 60, -5, 43, 45, 65,\n",
       "       33, 57,  4, 58, -4, 34, 32, 50, 52, 51, 41, 49, 48, 55, 38, -2, 37,\n",
       "       63, -1, 64, 66, 67], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_financials[\"delay_from_due_date\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7ff6a42-8cd5-47c4-8c42-8252bc3dc1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(141)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_financials[\"delay_from_due_date\"].eq(0).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afda673b-e826-4e4b-bdc4-3ee01005518e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delay_from_due_date\n",
       " 13    435\n",
       " 15    435\n",
       " 14    411\n",
       " 7     411\n",
       " 8     402\n",
       "      ... \n",
       " 63      7\n",
       "-5       5\n",
       " 64      4\n",
       " 66      4\n",
       " 67      3\n",
       "Name: count, Length: 73, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_financials[\"delay_from_due_date\"].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89e8bd87-8b61-4429-b52f-4b95cb70b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCHEMA (Attributes) ===\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- ingest_dt: date (nullable = true)\n",
      " |-- snap_ym: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n",
      "=== ROWS & DISTINCT KEYS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows=11,974  distinct_keys=11,974\n",
      "=== ROWS PER SNAPSHOT ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|snapshot_date|count|\n",
      "+-------------+-----+\n",
      "|2023-01-01   |530  |\n",
      "|2023-02-01   |501  |\n",
      "|2023-03-01   |506  |\n",
      "|2023-04-01   |510  |\n",
      "|2023-05-01   |521  |\n",
      "|2023-06-01   |517  |\n",
      "|2023-07-01   |471  |\n",
      "|2023-08-01   |481  |\n",
      "|2023-09-01   |454  |\n",
      "|2023-10-01   |487  |\n",
      "|2023-11-01   |491  |\n",
      "|2023-12-01   |489  |\n",
      "|2024-01-01   |485  |\n",
      "|2024-02-01   |518  |\n",
      "|2024-03-01   |511  |\n",
      "|2024-04-01   |513  |\n",
      "|2024-05-01   |491  |\n",
      "|2024-06-01   |498  |\n",
      "|2024-07-01   |505  |\n",
      "|2024-08-01   |543  |\n",
      "|2024-09-01   |493  |\n",
      "|2024-10-01   |456  |\n",
      "|2024-11-01   |488  |\n",
      "|2024-12-01   |515  |\n",
      "+-------------+-----+\n",
      "\n",
      "=== DUPLICATES ON (customer_id, snapshot_date) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate key rows = 0\n",
      "=== NULLS & CARDINALITY (occupation) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occupation_nulls=846  occupation_distinct=16\n",
      "=== TOP 20 OCCUPATIONS (by distinct customers per month snapshot) ===\n",
      "+-------------+-----+\n",
      "|occupation   |count|\n",
      "+-------------+-----+\n",
      "|NULL         |846  |\n",
      "|LAWYER       |785  |\n",
      "|ARCHITECT    |766  |\n",
      "|ACCOUNTANT   |763  |\n",
      "|ENGINEER     |754  |\n",
      "|TEACHER      |751  |\n",
      "|MECHANIC     |750  |\n",
      "|MEDIA_MANAGER|750  |\n",
      "|DEVELOPER    |746  |\n",
      "|SCIENTIST    |746  |\n",
      "|ENTREPRENEUR |744  |\n",
      "|JOURNALIST   |741  |\n",
      "|DOCTOR       |729  |\n",
      "|MUSICIAN     |709  |\n",
      "|MANAGER      |700  |\n",
      "|WRITER       |694  |\n",
      "+-------------+-----+\n",
      "\n",
      "=== VALUE QUALITY CHECKS (length/pattern) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|max_len|p99_len|\n",
      "+-------+-------+\n",
      "|     13|     13|\n",
      "+-------+-------+\n",
      "\n",
      "=== SNAPSHOT COVERAGE VS LABEL STORE (mob=0) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_mob0=11,974   with_attributes=11,974   coverage=100.00%\n",
      "=== OCCUPATION COVERAGE WITHIN MOB=0 LABELED SET (TOP 15) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:==================>                                     (8 + 16) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|occupation   |count|\n",
      "+-------------+-----+\n",
      "|NULL         |846  |\n",
      "|LAWYER       |785  |\n",
      "|ARCHITECT    |766  |\n",
      "|ACCOUNTANT   |763  |\n",
      "|ENGINEER     |754  |\n",
      "|TEACHER      |751  |\n",
      "|MECHANIC     |750  |\n",
      "|MEDIA_MANAGER|750  |\n",
      "|DEVELOPER    |746  |\n",
      "|SCIENTIST    |746  |\n",
      "|ENTREPRENEUR |744  |\n",
      "|JOURNALIST   |741  |\n",
      "|DOCTOR       |729  |\n",
      "|MUSICIAN     |709  |\n",
      "|MANAGER      |700  |\n",
      "+-------------+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# --- Paths ---\n",
    "ATTR_DIR  = \"datamart/silver/features/attributes\"\n",
    "LMS_DIR   = \"datamart/silver/loan_daily\"   # your Silver LMS location\n",
    "\n",
    "# --- Load ALL months of attributes ---\n",
    "attr_paths = sorted(glob.glob(os.path.join(ATTR_DIR, \"silver_attributes_*.parquet\")))\n",
    "attr = spark.read.parquet(*attr_paths)\n",
    "\n",
    "print(\"=== SCHEMA (Attributes) ===\")\n",
    "attr.printSchema()\n",
    "\n",
    "print(\"=== ROWS & DISTINCT KEYS ===\")\n",
    "n_rows = attr.count()\n",
    "n_keys = attr.select(\"customer_id\",\"snapshot_date\").distinct().count()\n",
    "print(f\"rows={n_rows:,}  distinct_keys={n_keys:,}\")\n",
    "\n",
    "print(\"=== ROWS PER SNAPSHOT ===\")\n",
    "attr.groupBy(\"snapshot_date\").count().orderBy(\"snapshot_date\").show(30, False)\n",
    "\n",
    "print(\"=== DUPLICATES ON (customer_id, snapshot_date) ===\")\n",
    "dups = (attr\n",
    "    .groupBy(\"customer_id\",\"snapshot_date\")\n",
    "    .count().filter(\"count > 1\"))\n",
    "dups_count = dups.count()\n",
    "print(f\"duplicate key rows = {dups_count}\")\n",
    "if dups_count > 0:\n",
    "    dups.show(20, False)\n",
    "\n",
    "print(\"=== NULLS & CARDINALITY (occupation) ===\")\n",
    "nulls = attr.filter(F.col(\"occupation\").isNull()).count()\n",
    "distinct_occ = attr.select(\"occupation\").distinct().count()\n",
    "print(f\"occupation_nulls={nulls:,}  occupation_distinct={distinct_occ:,}\")\n",
    "\n",
    "print(\"=== TOP 20 OCCUPATIONS (by distinct customers per month snapshot) ===\")\n",
    "attr.groupBy(\"occupation\").count().orderBy(F.desc(\"count\")).show(20, False)\n",
    "\n",
    "print(\"=== VALUE QUALITY CHECKS (length/pattern) ===\")\n",
    "attr.select(\n",
    "    F.max(F.length(\"occupation\")).alias(\"max_len\"),\n",
    "    F.expr(\"percentile_approx(length(occupation), 0.99)\").alias(\"p99_len\")\n",
    ").show()\n",
    "\n",
    "print(\"=== SNAPSHOT COVERAGE VS LABEL STORE (mob=0) ===\")\n",
    "# Load all silver_loan_daily_* and keep mob=0\n",
    "lms_paths = sorted(glob.glob(os.path.join(LMS_DIR, \"silver_loan_daily_*.parquet\")))\n",
    "lms = spark.read.parquet(*lms_paths) \\\n",
    "    .select(F.col(\"Customer_ID\").alias(\"customer_id\"), \"snapshot_date\", \"mob\").distinct()\n",
    "\n",
    "lbl_m0 = lms.filter(F.col(\"mob\") == 0).select(\"customer_id\",\"snapshot_date\").distinct()\n",
    "attr_keys = attr.select(\"customer_id\",\"snapshot_date\").distinct()\n",
    "\n",
    "usable = lbl_m0.join(attr_keys, [\"customer_id\",\"snapshot_date\"], \"inner\").count()\n",
    "print(f\"labels_mob0={lbl_m0.count():,}   with_attributes={usable:,}   coverage={(usable/max(lbl_m0.count(),1)):.2%}\")\n",
    "\n",
    "print(\"=== OCCUPATION COVERAGE WITHIN MOB=0 LABELED SET (TOP 15) ===\")\n",
    "(lbl_m0.join(attr, [\"customer_id\",\"snapshot_date\"], \"left\")\n",
    " .groupBy(\"occupation\")\n",
    " .count().orderBy(F.desc(\"count\")).show(15, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d0302e4-b646-451d-a7a5-a5bbe08bd799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark OK: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "# (Re)start Spark if needed\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def ensure_spark(app_name=\"MLE-Inspect\"):\n",
    "    global spark\n",
    "    try:\n",
    "        sc = spark.sparkContext  # may fail if spark undefined\n",
    "        # If Spark exists but is stopped, _jsc.sc().isStopped() will be True\n",
    "        if sc._jsc.sc().isStopped():\n",
    "            raise RuntimeError(\"SparkContext is stopped\")\n",
    "    except Exception:\n",
    "        spark = (SparkSession.builder\n",
    "                 .appName(app_name)\n",
    "                 .master(\"local[*]\")\n",
    "                 # add any configs you typically use:\n",
    "                 .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "                 .getOrCreate())\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    return spark\n",
    "\n",
    "spark = ensure_spark()\n",
    "print(\"Spark OK:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3476da1-cd60-4be3-884d-bf9902f4b15e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
