{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe1fd5b-2c47-4edb-9b66-3661082bc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, classification_report,\n",
    "    confusion_matrix, precision_recall_curve\n",
    ")\n",
    "import xgboost as xgb\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6075286d-e044-41be-aa8b-16177160b5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 snapshots under: datamart/gold/model_dataset\n",
      "Rows: 8974 Cols: 59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>loan_id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_def</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>num_credit_inquiries</th>\n",
       "      <th>credit_mix</th>\n",
       "      <th>num_of_loan</th>\n",
       "      <th>credit_history_age</th>\n",
       "      <th>...</th>\n",
       "      <th>occ_ENTREPRENEUR</th>\n",
       "      <th>occ_JOURNALIST</th>\n",
       "      <th>occ_LAWYER</th>\n",
       "      <th>occ_MANAGER</th>\n",
       "      <th>occ_MEDIA_MANAGER</th>\n",
       "      <th>occ_MECHANIC</th>\n",
       "      <th>occ_MUSICIAN</th>\n",
       "      <th>occ_SCIENTIST</th>\n",
       "      <th>occ_TEACHER</th>\n",
       "      <th>occ_WRITER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUS_0X16F4</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>CUS_0x16f4_2023_01_01</td>\n",
       "      <td>0</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Standard</td>\n",
       "      <td>3</td>\n",
       "      <td>362</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUS_0X4340</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>CUS_0x4340_2023_01_01</td>\n",
       "      <td>0</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17</td>\n",
       "      <td>Bad</td>\n",
       "      <td>3</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUS_0X7C66</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>CUS_0x7c66_2023_01_01</td>\n",
       "      <td>0</td>\n",
       "      <td>30dpd_6mob</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Bad</td>\n",
       "      <td>8</td>\n",
       "      <td>242</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id snapshot_date                loan_id  label   label_def  \\\n",
       "0  CUS_0X16F4    2023-07-01  CUS_0x16f4_2023_01_01      0  30dpd_6mob   \n",
       "1  CUS_0X4340    2023-07-01  CUS_0x4340_2023_01_01      0  30dpd_6mob   \n",
       "2  CUS_0X7C66    2023-07-01  CUS_0x7c66_2023_01_01      0  30dpd_6mob   \n",
       "\n",
       "   interest_rate  num_credit_inquiries credit_mix  num_of_loan  \\\n",
       "0            6.0                     1   Standard            3   \n",
       "1           22.0                    17        Bad            3   \n",
       "2           17.0                     6        Bad            8   \n",
       "\n",
       "   credit_history_age  ...  occ_ENTREPRENEUR  occ_JOURNALIST  occ_LAWYER  \\\n",
       "0                 362  ...                 0               0           0   \n",
       "1                 153  ...                 0               0           0   \n",
       "2                 242  ...                 0               0           0   \n",
       "\n",
       "   occ_MANAGER  occ_MEDIA_MANAGER  occ_MECHANIC  occ_MUSICIAN occ_SCIENTIST  \\\n",
       "0            0                  1             0             0             0   \n",
       "1            0                  0             0             0             0   \n",
       "2            0                  0             0             0             0   \n",
       "\n",
       "   occ_TEACHER  occ_WRITER  \n",
       "0            0           0  \n",
       "1            1           0  \n",
       "2            0           0  \n",
       "\n",
       "[3 rows x 59 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "CANDIDATES = [\n",
    "    Path(\"gold/model_dataset\"),\n",
    "    Path(\"datamart/gold/model_dataset\"),\n",
    "]\n",
    "\n",
    "paths = []\n",
    "for base in CANDIDATES:\n",
    "    pat = str(base / \"gold_model_dataset_*.parquet\")\n",
    "    found = sorted(glob.glob(pat))\n",
    "    if found:\n",
    "        print(f\"Found {len(found)} snapshots under: {base}\")\n",
    "        paths = found\n",
    "        break\n",
    "\n",
    "if not paths:\n",
    "    raise FileNotFoundError(\"Couldn’t find any gold_model_dataset_*.parquet under \"\n",
    "                            \"'gold/model_dataset' or 'datamart/gold/model_dataset'. \"\n",
    "                            \"Check your CWD from the diagnostics above.\")\n",
    "\n",
    "# Read each snapshot (file or parquet directory)\n",
    "dfs = []\n",
    "for p in paths:\n",
    "    dfs.append(pd.read_parquet(p))\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Coerce types\n",
    "df[\"snapshot_date\"] = pd.to_datetime(df[\"snapshot_date\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(\"Rows:\", len(df), \"Cols:\", df.shape[1])\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ce0984-41ad-473a-9fc6-0bdf1b71d2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4978, Valid: 980, Test: 2027, OOT: 989\n",
      "Feature columns: 54 | Train shape: (4978, 54)\n"
     ]
    }
   ],
   "source": [
    "# --- Time-based train/valid/test/oot split -----------------------------------\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the temporal boundaries\n",
    "train_start = pd.Timestamp(\"2023-07-01\")\n",
    "train_end   = pd.Timestamp(\"2024-04-30\")\n",
    "valid_start = pd.Timestamp(\"2024-05-01\")\n",
    "valid_end   = pd.Timestamp(\"2024-06-30\")\n",
    "test_start  = pd.Timestamp(\"2024-07-01\")\n",
    "test_end    = pd.Timestamp(\"2024-10-31\")\n",
    "oot_start   = pd.Timestamp(\"2024-11-01\")\n",
    "oot_end     = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "# Assign subsets based on snapshot_date\n",
    "train_df = df[(df[\"snapshot_date\"] >= train_start) & (df[\"snapshot_date\"] <= train_end)]\n",
    "valid_df = df[(df[\"snapshot_date\"] >= valid_start) & (df[\"snapshot_date\"] <= valid_end)]\n",
    "test_df  = df[(df[\"snapshot_date\"] >= test_start)  & (df[\"snapshot_date\"] <= test_end)]\n",
    "oot_df   = df[(df[\"snapshot_date\"] >= oot_start)   & (df[\"snapshot_date\"] <= oot_end)]\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}, OOT: {len(oot_df)}\")\n",
    "\n",
    "# Drop ID-like columns\n",
    "drop_cols = [\"customer_id\", \"loan_id\", \"label_def\"]\n",
    "train_df = train_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "valid_df = valid_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "test_df  = test_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "oot_df   = oot_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "# Separate features and labels\n",
    "feature_cols = [c for c in df.columns if c not in [\"label\",\"snapshot_date\",\"customer_id\",\"loan_id\",\"label_def\"]]\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[\"label\"].astype(int)\n",
    "X_valid, y_valid = valid_df[feature_cols], valid_df[\"label\"].astype(int)\n",
    "X_test,  y_test  = test_df[feature_cols],  test_df[\"label\"].astype(int)\n",
    "X_oot,   y_oot   = oot_df[feature_cols],   oot_df[\"label\"].astype(int)\n",
    "\n",
    "print(f\"Feature columns: {len(feature_cols)} | Train shape: {X_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6060fed7-0d06-44cf-a38d-4e2e14a28375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: models/logreg_pipeline_20251108_165734.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# === Train Logistic Regression (Train+Valid) ================================\n",
    "# Rebuild preprocessing from the detected columns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re, joblib, numpy as np, pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ARTIFACT_DIR = Path(\"models\"); ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# Columns from training frame\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "\n",
    "# Preprocess\n",
    "numeric_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "categorical_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                           (\"ohe\", make_ohe())])\n",
    "\n",
    "preprocess_lr = ColumnTransformer([(\"num\", numeric_tf, num_cols),\n",
    "                                   (\"cat\", categorical_tf, cat_cols)])\n",
    "\n",
    "pipe_lr = Pipeline([(\"prep\", preprocess_lr),\n",
    "                    (\"clf\", LogisticRegression(max_iter=1000, solver=\"lbfgs\"))])\n",
    "\n",
    "# Fit on Train+Valid\n",
    "X_trv = pd.concat([X_train, X_valid], axis=0)\n",
    "y_trv = pd.concat([y_train, y_valid], axis=0)\n",
    "pipe_lr.fit(X_trv, y_trv)\n",
    "\n",
    "# Versioned save\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "lr_path = ARTIFACT_DIR / f\"logreg_pipeline_{ts}.joblib\"\n",
    "joblib.dump(pipe_lr, lr_path)\n",
    "print(\"✅ Saved:\", lr_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c0335e-a0d3-45b5-93bd-99043741d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded latest LR (versioned): logreg_pipeline_20251108_165734.joblib\n",
      "\n",
      "=== Logistic Regression — Train (thr=0.500) ===\n",
      "ROC-AUC: 0.717 | PR-AUC: 0.453\n",
      "Precision (1): 0.488 | Recall (1): 0.196 | F1 (1): 0.280\n",
      "Confusion matrix:\n",
      "TN=3300  FP= 286\n",
      "FN=1119  TP= 273\n",
      "\n",
      "=== Logistic Regression — Valid (thr=0.500) ===\n",
      "ROC-AUC: 0.684 | PR-AUC: 0.423\n",
      "Precision (1): 0.381 | Recall (1): 0.136 | F1 (1): 0.201\n",
      "Confusion matrix:\n",
      "TN= 621  FP=  65\n",
      "FN= 254  TP=  40\n",
      "\n",
      "=== Logistic Regression — Test (thr=0.500) ===\n",
      "ROC-AUC: 0.704 | PR-AUC: 0.441\n",
      "Precision (1): 0.453 | Recall (1): 0.170 | F1 (1): 0.247\n",
      "Confusion matrix:\n",
      "TN=1303  FP= 123\n",
      "FN= 499  TP= 102\n",
      "\n",
      "=== Logistic Regression — OOT (thr=0.500) ===\n",
      "ROC-AUC: 0.743 | PR-AUC: 0.530\n",
      "Precision (1): 0.518 | Recall (1): 0.188 | F1 (1): 0.275\n",
      "Confusion matrix:\n",
      "TN= 632  FP=  53\n",
      "FN= 247  TP=  57\n",
      "\n",
      ">>> Best threshold from VALID (max F1): 0.241 | Precision=0.477, Recall=0.704, F1=0.569\n",
      "\n",
      "=== Logistic Regression — Train_tuned (thr=0.241) ===\n",
      "ROC-AUC: 0.717 | PR-AUC: 0.453\n",
      "Precision (1): 0.445 | Recall (1): 0.740 | F1 (1): 0.556\n",
      "Confusion matrix:\n",
      "TN=2301  FP=1285\n",
      "FN= 362  TP=1030\n",
      "\n",
      "=== Logistic Regression — Valid_tuned (thr=0.241) ===\n",
      "ROC-AUC: 0.684 | PR-AUC: 0.423\n",
      "Precision (1): 0.477 | Recall (1): 0.704 | F1 (1): 0.569\n",
      "Confusion matrix:\n",
      "TN= 459  FP= 227\n",
      "FN=  87  TP= 207\n",
      "\n",
      "=== Logistic Regression — Test_tuned (thr=0.241) ===\n",
      "ROC-AUC: 0.704 | PR-AUC: 0.441\n",
      "Precision (1): 0.460 | Recall (1): 0.715 | F1 (1): 0.560\n",
      "Confusion matrix:\n",
      "TN= 921  FP= 505\n",
      "FN= 171  TP= 430\n",
      "\n",
      "=== Logistic Regression — OOT_tuned (thr=0.241) ===\n",
      "ROC-AUC: 0.743 | PR-AUC: 0.530\n",
      "Precision (1): 0.496 | Recall (1): 0.763 | F1 (1): 0.601\n",
      "Confusion matrix:\n",
      "TN= 449  FP= 236\n",
      "FN=  72  TP= 232\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>threshold</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.716992</td>\n",
       "      <td>0.452506</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.196121</td>\n",
       "      <td>0.279856</td>\n",
       "      <td>3300</td>\n",
       "      <td>286</td>\n",
       "      <td>1119</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Valid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.684462</td>\n",
       "      <td>0.422762</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.136054</td>\n",
       "      <td>0.200501</td>\n",
       "      <td>621</td>\n",
       "      <td>65</td>\n",
       "      <td>254</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.703522</td>\n",
       "      <td>0.440685</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.169717</td>\n",
       "      <td>0.246973</td>\n",
       "      <td>1303</td>\n",
       "      <td>123</td>\n",
       "      <td>499</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOT</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.742802</td>\n",
       "      <td>0.529626</td>\n",
       "      <td>0.518182</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>632</td>\n",
       "      <td>53</td>\n",
       "      <td>247</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_tuned</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.716992</td>\n",
       "      <td>0.452506</td>\n",
       "      <td>0.444924</td>\n",
       "      <td>0.739943</td>\n",
       "      <td>0.555705</td>\n",
       "      <td>2301</td>\n",
       "      <td>1285</td>\n",
       "      <td>362</td>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Valid_tuned</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.684462</td>\n",
       "      <td>0.422762</td>\n",
       "      <td>0.476959</td>\n",
       "      <td>0.704082</td>\n",
       "      <td>0.568681</td>\n",
       "      <td>459</td>\n",
       "      <td>227</td>\n",
       "      <td>87</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Test_tuned</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.703522</td>\n",
       "      <td>0.440685</td>\n",
       "      <td>0.459893</td>\n",
       "      <td>0.715474</td>\n",
       "      <td>0.559896</td>\n",
       "      <td>921</td>\n",
       "      <td>505</td>\n",
       "      <td>171</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OOT_tuned</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.742802</td>\n",
       "      <td>0.529626</td>\n",
       "      <td>0.495726</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.601036</td>\n",
       "      <td>449</td>\n",
       "      <td>236</td>\n",
       "      <td>72</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         split  threshold   roc_auc    pr_auc  precision_1  recall_1  \\\n",
       "0        Train   0.500000  0.716992  0.452506     0.488372  0.196121   \n",
       "1        Valid   0.500000  0.684462  0.422762     0.380952  0.136054   \n",
       "2         Test   0.500000  0.703522  0.440685     0.453333  0.169717   \n",
       "3          OOT   0.500000  0.742802  0.529626     0.518182  0.187500   \n",
       "4  Train_tuned   0.240939  0.716992  0.452506     0.444924  0.739943   \n",
       "5  Valid_tuned   0.240939  0.684462  0.422762     0.476959  0.704082   \n",
       "6   Test_tuned   0.240939  0.703522  0.440685     0.459893  0.715474   \n",
       "7    OOT_tuned   0.240939  0.742802  0.529626     0.495726  0.763158   \n",
       "\n",
       "       f1_1    tn    fp    fn    tp  \n",
       "0  0.279856  3300   286  1119   273  \n",
       "1  0.200501   621    65   254    40  \n",
       "2  0.246973  1303   123   499   102  \n",
       "3  0.275362   632    53   247    57  \n",
       "4  0.555705  2301  1285   362  1030  \n",
       "5  0.568681   459   227    87   207  \n",
       "6  0.559896   921   505   171   430  \n",
       "7  0.601036   449   236    72   232  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved metrics to models/logreg_metrics_all_splits.csv\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate Logistic Regression across splits===================\n",
    "# Load the saved pipeline\n",
    "ARTIFACT_DIR = Path(\"models\")\n",
    "\n",
    "def load_latest_logreg():\n",
    "    # Prefer timestamped artifacts like: logreg_pipeline_YYYYMMDD_HHMMSS.joblib\n",
    "    files = sorted(ARTIFACT_DIR.glob(\"logreg_pipeline_*.joblib\"))\n",
    "    if files:\n",
    "        latest = files[-1]\n",
    "        print(\"Loaded latest LR (versioned):\", latest.name)\n",
    "        return joblib.load(latest)\n",
    "    # Fallback to non-versioned\n",
    "    fallback = ARTIFACT_DIR / \"logreg_pipeline.joblib\"\n",
    "    if fallback.exists():\n",
    "        print(\"Loaded fallback LR:\", fallback.name)\n",
    "        return joblib.load(fallback)\n",
    "    raise FileNotFoundError(\"No Logistic Regression artifacts found in models/.\")\n",
    "\n",
    "pipe_lr = load_latest_logreg()\n",
    "\n",
    "def eval_split(model, X, y, split_name, thr=0.5):\n",
    "    \"\"\"Return metrics dict and print a text confusion matrix.\"\"\"\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    pred  = (proba >= thr).astype(int)\n",
    "\n",
    "    roc = roc_auc_score(y, proba)\n",
    "    pr  = average_precision_score(y, proba)\n",
    "    rpt = classification_report(y, pred, digits=3, output_dict=True)\n",
    "    cm  = confusion_matrix(y, pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    print(f\"\\n=== Logistic Regression — {split_name} (thr={thr:.3f}) ===\")\n",
    "    print(f\"ROC-AUC: {roc:.3f} | PR-AUC: {pr:.3f}\")\n",
    "    print(f\"Precision (1): {rpt['1']['precision']:.3f} | Recall (1): {rpt['1']['recall']:.3f} | F1 (1): {rpt['1']['f1-score']:.3f}\")\n",
    "    print(\"Confusion matrix:\\n\"\n",
    "          f\"TN={tn:>4}  FP={fp:>4}\\n\"\n",
    "          f\"FN={fn:>4}  TP={tp:>4}\")\n",
    "\n",
    "    return {\n",
    "        \"split\": split_name,\n",
    "        \"threshold\": thr,\n",
    "        \"roc_auc\": roc,\n",
    "        \"pr_auc\": pr,\n",
    "        \"precision_1\": rpt[\"1\"][\"precision\"],\n",
    "        \"recall_1\": rpt[\"1\"][\"recall\"],\n",
    "        \"f1_1\": rpt[\"1\"][\"f1-score\"],\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)\n",
    "    }\n",
    "\n",
    "# 1) Evaluate at default threshold = 0.5\n",
    "metrics = []\n",
    "metrics.append(eval_split(pipe_lr, X_train, y_train, \"Train\", thr=0.5))\n",
    "metrics.append(eval_split(pipe_lr, X_valid, y_valid, \"Valid\", thr=0.5))\n",
    "metrics.append(eval_split(pipe_lr, X_test,  y_test,  \"Test\",  thr=0.5))\n",
    "metrics.append(eval_split(pipe_lr, X_oot,   y_oot,   \"OOT\",   thr=0.5))\n",
    "\n",
    "# 2) Tune threshold on VALID (maximize F1), then re-evaluate all splits\n",
    "proba_valid = pipe_lr.predict_proba(X_valid)[:, 1]\n",
    "prec_v, rec_v, thr_v = precision_recall_curve(y_valid, proba_valid)\n",
    "f1_v = 2 * (prec_v * rec_v) / (prec_v + rec_v + 1e-9)\n",
    "best_idx = np.nanargmax(f1_v)\n",
    "best_thr = float(thr_v[best_idx])\n",
    "print(f\"\\n>>> Best threshold from VALID (max F1): {best_thr:.3f} \"\n",
    "      f\"| Precision={prec_v[best_idx]:.3f}, Recall={rec_v[best_idx]:.3f}, F1={f1_v[best_idx]:.3f}\")\n",
    "\n",
    "metrics.append(eval_split(pipe_lr, X_train, y_train, \"Train_tuned\", thr=best_thr))\n",
    "metrics.append(eval_split(pipe_lr, X_valid, y_valid, \"Valid_tuned\", thr=best_thr))\n",
    "metrics.append(eval_split(pipe_lr, X_test,  y_test,  \"Test_tuned\",  thr=best_thr))\n",
    "metrics.append(eval_split(pipe_lr, X_oot,   y_oot,   \"OOT_tuned\",   thr=best_thr))\n",
    "\n",
    "# 3) Save metrics (optional)\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "display(metrics_df)\n",
    "metrics_df.to_csv(\"models/logreg_metrics_all_splits.csv\", index=False)\n",
    "print(\"\\nSaved metrics to models/logreg_metrics_all_splits.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d173be32-1bf1-4919-8753-4ca419137477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 70\n",
      "✅ Saved: models/xgb_bundle_20251108_170307.joblib\n"
     ]
    }
   ],
   "source": [
    "# === Train XGBoost (DMatrix API with early stopping), then refit on Train+Valid ===\n",
    "ARTIFACT_DIR = Path(\"models\"); ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# Columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "\n",
    "# Preprocess pieces\n",
    "numeric_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "categorical_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                           (\"ohe\", make_ohe())])\n",
    "\n",
    "# ---- Stage 1: early stopping on VALID\n",
    "pre_stage1 = ColumnTransformer([(\"num\", numeric_tf, num_cols),\n",
    "                                (\"cat\", categorical_tf, cat_cols)])\n",
    "Xtr = pre_stage1.fit_transform(X_train, y_train)\n",
    "Xva = pre_stage1.transform(X_valid)\n",
    "\n",
    "dtr = xgb.DMatrix(Xtr, label=y_train.to_numpy())\n",
    "dva = xgb.DMatrix(Xva, label=y_valid.to_numpy())\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"eta\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"seed\": 42,\n",
    "}\n",
    "bst_es = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva, \"valid\")],\n",
    "                   early_stopping_rounds=50, verbose_eval=False)\n",
    "best_iter = int(bst_es.best_iteration + 1)\n",
    "print(\"Best iteration:\", best_iter)\n",
    "\n",
    "# ---- Stage 2: refit on Train+Valid\n",
    "pre_final = ColumnTransformer([(\"num\", numeric_tf, num_cols),\n",
    "                               (\"cat\", categorical_tf, cat_cols)])\n",
    "X_trv = pd.concat([X_train, X_valid], axis=0)\n",
    "y_trv = pd.concat([y_train, y_valid], axis=0)\n",
    "\n",
    "Xtrv = pre_final.fit_transform(X_trv, y_trv)\n",
    "dtrv = xgb.DMatrix(Xtrv, label=y_trv.to_numpy())\n",
    "\n",
    "bst_final = xgb.train(params, dtrv, num_boost_round=best_iter, verbose_eval=False)\n",
    "\n",
    "# Versioned save\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "xgb_path = ARTIFACT_DIR / f\"xgb_bundle_{ts}.joblib\"\n",
    "bundle = {\"preprocess\": pre_final, \"booster\": bst_final,\n",
    "          \"num_cols\": num_cols, \"cat_cols\": cat_cols, \"best_iter\": best_iter}\n",
    "joblib.dump(bundle, xgb_path)\n",
    "print(\"✅ Saved:\", xgb_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e585b2fc-3650-4929-9471-054ac5cd5c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded latest XGB (versioned): xgb_bundle_20251108_170307.joblib\n",
      "\n",
      "=== XGBoost — Train (thr=0.500) ===\n",
      "ROC-AUC: 0.902 | PR-AUC: 0.801\n",
      "Precision (1): 0.768 | Recall (1): 0.622 | F1 (1): 0.688\n",
      "Confusion matrix:\n",
      "TN=3325  FP= 261\n",
      "FN= 526  TP= 866\n",
      "\n",
      "=== XGBoost — Valid (thr=0.500) ===\n",
      "ROC-AUC: 0.896 | PR-AUC: 0.810\n",
      "Precision (1): 0.789 | Recall (1): 0.599 | F1 (1): 0.681\n",
      "Confusion matrix:\n",
      "TN= 639  FP=  47\n",
      "FN= 118  TP= 176\n",
      "\n",
      "=== XGBoost — Test (thr=0.500) ===\n",
      "ROC-AUC: 0.757 | PR-AUC: 0.555\n",
      "Precision (1): 0.600 | Recall (1): 0.507 | F1 (1): 0.550\n",
      "Confusion matrix:\n",
      "TN=1223  FP= 203\n",
      "FN= 296  TP= 305\n",
      "\n",
      "=== XGBoost — OOT (thr=0.500) ===\n",
      "ROC-AUC: 0.781 | PR-AUC: 0.615\n",
      "Precision (1): 0.634 | Recall (1): 0.497 | F1 (1): 0.557\n",
      "Confusion matrix:\n",
      "TN= 598  FP=  87\n",
      "FN= 153  TP= 151\n",
      "\n",
      ">>> Best threshold from VALID (max F1): 0.199 | Precision=0.610, Recall=0.878, F1=0.720\n",
      "\n",
      "=== XGBoost — Train_tuned (thr=0.199) ===\n",
      "ROC-AUC: 0.902 | PR-AUC: 0.801\n",
      "Precision (1): 0.579 | Recall (1): 0.881 | F1 (1): 0.699\n",
      "Confusion matrix:\n",
      "TN=2692  FP= 894\n",
      "FN= 165  TP=1227\n",
      "\n",
      "=== XGBoost — Valid_tuned (thr=0.199) ===\n",
      "ROC-AUC: 0.896 | PR-AUC: 0.810\n",
      "Precision (1): 0.610 | Recall (1): 0.878 | F1 (1): 0.720\n",
      "Confusion matrix:\n",
      "TN= 521  FP= 165\n",
      "FN=  36  TP= 258\n",
      "\n",
      "=== XGBoost — Test_tuned (thr=0.199) ===\n",
      "ROC-AUC: 0.757 | PR-AUC: 0.555\n",
      "Precision (1): 0.493 | Recall (1): 0.727 | F1 (1): 0.588\n",
      "Confusion matrix:\n",
      "TN= 977  FP= 449\n",
      "FN= 164  TP= 437\n",
      "\n",
      "=== XGBoost — OOT_tuned (thr=0.199) ===\n",
      "ROC-AUC: 0.781 | PR-AUC: 0.615\n",
      "Precision (1): 0.519 | Recall (1): 0.763 | F1 (1): 0.618\n",
      "Confusion matrix:\n",
      "TN= 470  FP= 215\n",
      "FN=  72  TP= 232\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>threshold</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.901888</td>\n",
       "      <td>0.800592</td>\n",
       "      <td>0.768412</td>\n",
       "      <td>0.622126</td>\n",
       "      <td>0.687574</td>\n",
       "      <td>3325</td>\n",
       "      <td>261</td>\n",
       "      <td>526</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Valid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.895911</td>\n",
       "      <td>0.810426</td>\n",
       "      <td>0.789238</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>639</td>\n",
       "      <td>47</td>\n",
       "      <td>118</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.757078</td>\n",
       "      <td>0.554729</td>\n",
       "      <td>0.600394</td>\n",
       "      <td>0.507488</td>\n",
       "      <td>0.550045</td>\n",
       "      <td>1223</td>\n",
       "      <td>203</td>\n",
       "      <td>296</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOT</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781060</td>\n",
       "      <td>0.614728</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.496711</td>\n",
       "      <td>0.557196</td>\n",
       "      <td>598</td>\n",
       "      <td>87</td>\n",
       "      <td>153</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_tuned</td>\n",
       "      <td>0.198991</td>\n",
       "      <td>0.901888</td>\n",
       "      <td>0.800592</td>\n",
       "      <td>0.578501</td>\n",
       "      <td>0.881466</td>\n",
       "      <td>0.698548</td>\n",
       "      <td>2692</td>\n",
       "      <td>894</td>\n",
       "      <td>165</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Valid_tuned</td>\n",
       "      <td>0.198991</td>\n",
       "      <td>0.895911</td>\n",
       "      <td>0.810426</td>\n",
       "      <td>0.609929</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>521</td>\n",
       "      <td>165</td>\n",
       "      <td>36</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Test_tuned</td>\n",
       "      <td>0.198991</td>\n",
       "      <td>0.757078</td>\n",
       "      <td>0.554729</td>\n",
       "      <td>0.493228</td>\n",
       "      <td>0.727121</td>\n",
       "      <td>0.587761</td>\n",
       "      <td>977</td>\n",
       "      <td>449</td>\n",
       "      <td>164</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OOT_tuned</td>\n",
       "      <td>0.198991</td>\n",
       "      <td>0.781060</td>\n",
       "      <td>0.614728</td>\n",
       "      <td>0.519016</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.617843</td>\n",
       "      <td>470</td>\n",
       "      <td>215</td>\n",
       "      <td>72</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         split  threshold   roc_auc    pr_auc  precision_1  recall_1  \\\n",
       "0        Train   0.500000  0.901888  0.800592     0.768412  0.622126   \n",
       "1        Valid   0.500000  0.895911  0.810426     0.789238  0.598639   \n",
       "2         Test   0.500000  0.757078  0.554729     0.600394  0.507488   \n",
       "3          OOT   0.500000  0.781060  0.614728     0.634454  0.496711   \n",
       "4  Train_tuned   0.198991  0.901888  0.800592     0.578501  0.881466   \n",
       "5  Valid_tuned   0.198991  0.895911  0.810426     0.609929  0.877551   \n",
       "6   Test_tuned   0.198991  0.757078  0.554729     0.493228  0.727121   \n",
       "7    OOT_tuned   0.198991  0.781060  0.614728     0.519016  0.763158   \n",
       "\n",
       "       f1_1    tn   fp   fn    tp  \n",
       "0  0.687574  3325  261  526   866  \n",
       "1  0.680851   639   47  118   176  \n",
       "2  0.550045  1223  203  296   305  \n",
       "3  0.557196   598   87  153   151  \n",
       "4  0.698548  2692  894  165  1227  \n",
       "5  0.719665   521  165   36   258  \n",
       "6  0.587761   977  449  164   437  \n",
       "7  0.617843   470  215   72   232  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved metrics to models/xgb_metrics_all_splits.csv\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate XGBoost (Booster + preprocess) across splits, no plots ===========\n",
    "ARTIFACT_DIR = Path(\"models\")\n",
    "\n",
    "def load_latest_xgb():\n",
    "    \"\"\"Load the most recent XGBoost bundle from models/.\"\"\"\n",
    "    files = sorted(ARTIFACT_DIR.glob(\"xgb_bundle_*.joblib\"))\n",
    "    if files:\n",
    "        latest = files[-1]\n",
    "        print(\"Loaded latest XGB (versioned):\", latest.name)\n",
    "        return joblib.load(latest)\n",
    "    fallback = ARTIFACT_DIR / \"xgb_bundle.joblib\"\n",
    "    if fallback.exists():\n",
    "        print(\"Loaded fallback XGB:\", fallback.name)\n",
    "        return joblib.load(fallback)\n",
    "    raise FileNotFoundError(\"No XGBoost artifacts found in models/.\")\n",
    "\n",
    "# Load latest model bundle\n",
    "bundle = load_latest_xgb()\n",
    "prep = bundle[\"preprocess\"]\n",
    "booster = bundle[\"booster\"]\n",
    "\n",
    "def predict_proba(prep, booster, X):\n",
    "    Xt = prep.transform(X)\n",
    "    d = xgb.DMatrix(Xt)\n",
    "    return booster.predict(d)  # probability for class 1\n",
    "\n",
    "def eval_split(prep, booster, X, y, split_name, thr=0.5):\n",
    "    proba = predict_proba(prep, booster, X)\n",
    "    pred  = (proba >= thr).astype(int)\n",
    "\n",
    "    roc = roc_auc_score(y, proba)\n",
    "    pr  = average_precision_score(y, proba)\n",
    "    rpt = classification_report(y, pred, digits=3, output_dict=True)\n",
    "    cm  = confusion_matrix(y, pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    print(f\"\\n=== XGBoost — {split_name} (thr={thr:.3f}) ===\")\n",
    "    print(f\"ROC-AUC: {roc:.3f} | PR-AUC: {pr:.3f}\")\n",
    "    print(f\"Precision (1): {rpt['1']['precision']:.3f} | Recall (1): {rpt['1']['recall']:.3f} | F1 (1): {rpt['1']['f1-score']:.3f}\")\n",
    "    print(\"Confusion matrix:\\n\"\n",
    "          f\"TN={tn:>4}  FP={fp:>4}\\n\"\n",
    "          f\"FN={fn:>4}  TP={tp:>4}\")\n",
    "\n",
    "    return {\n",
    "        \"split\": split_name, \"threshold\": thr,\n",
    "        \"roc_auc\": roc, \"pr_auc\": pr,\n",
    "        \"precision_1\": rpt[\"1\"][\"precision\"],\n",
    "        \"recall_1\": rpt[\"1\"][\"recall\"],\n",
    "        \"f1_1\": rpt[\"1\"][\"f1-score\"],\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)\n",
    "    }\n",
    "\n",
    "# 1) Evaluate at threshold = 0.5\n",
    "metrics_xgb = []\n",
    "metrics_xgb.append(eval_split(prep, booster, X_train, y_train, \"Train\", thr=0.5))\n",
    "metrics_xgb.append(eval_split(prep, booster, X_valid, y_valid, \"Valid\", thr=0.5))\n",
    "metrics_xgb.append(eval_split(prep, booster, X_test,  y_test,  \"Test\",  thr=0.5))\n",
    "metrics_xgb.append(eval_split(prep, booster, X_oot,   y_oot,   \"OOT\",   thr=0.5))\n",
    "\n",
    "# 2) Tune threshold on VALID (maximize F1), then re-evaluate all splits\n",
    "proba_valid = predict_proba(prep, booster, X_valid)\n",
    "prec_v, rec_v, thr_v = precision_recall_curve(y_valid, proba_valid)\n",
    "f1_v = 2 * (prec_v * rec_v) / (prec_v + rec_v + 1e-9)\n",
    "best_idx = np.nanargmax(f1_v)\n",
    "best_thr = float(thr_v[best_idx])\n",
    "print(f\"\\n>>> Best threshold from VALID (max F1): {best_thr:.3f} \"\n",
    "      f\"| Precision={prec_v[best_idx]:.3f}, Recall={rec_v[best_idx]:.3f}, F1={f1_v[best_idx]:.3f}\")\n",
    "\n",
    "metrics_xgb.append(eval_split(prep, booster, X_train, y_train, \"Train_tuned\", thr=best_thr))\n",
    "metrics_xgb.append(eval_split(prep, booster, X_valid, y_valid, \"Valid_tuned\", thr=best_thr))\n",
    "metrics_xgb.append(eval_split(prep, booster, X_test,  y_test,  \"Test_tuned\",  thr=best_thr))\n",
    "metrics_xgb.append(eval_split(prep, booster, X_oot,   y_oot,   \"OOT_tuned\",   thr=best_thr))\n",
    "\n",
    "# 3) Save metrics\n",
    "metrics_xgb_df = pd.DataFrame(metrics_xgb)\n",
    "display(metrics_xgb_df)\n",
    "metrics_xgb_df.to_csv(\"models/xgb_metrics_all_splits.csv\", index=False)\n",
    "print(\"\\nSaved metrics to models/xgb_metrics_all_splits.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55872ba9-5436-4f16-9bfd-300e7616c74b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
