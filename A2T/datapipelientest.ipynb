{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e88dcfd-b60e-45b9-9fa2-4a5b09b45f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/08 06:51:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .appName(\"MLE-A2-EDA\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbd18b3-95ad-418d-8ad4-052124c8f703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['datamart/gold/model_dataset/gold_model_dataset_2023_01_01.parquet',\n",
       "  'datamart/gold/model_dataset/gold_model_dataset_2023_02_01.parquet',\n",
       "  'datamart/gold/model_dataset/gold_model_dataset_2023_03_01.parquet',\n",
       "  'datamart/gold/model_dataset/gold_model_dataset_2023_04_01.parquet',\n",
       "  'datamart/gold/model_dataset/gold_model_dataset_2023_05_01.parquet'],\n",
       " 24)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "GOLD_MD_DIR = \"datamart/gold/model_dataset\"\n",
    "\n",
    "paths = sorted([str(p) for p in Path(GOLD_MD_DIR).glob(\"gold_model_dataset_*.parquet\")])\n",
    "paths[:5], len(paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74bacc10-6dcc-475f-be65-1413d96e7259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 8974 cols = 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|snapshot_date|count|\n",
      "+-------------+-----+\n",
      "|2023-07-01   |530  |\n",
      "|2023-08-01   |501  |\n",
      "|2023-09-01   |506  |\n",
      "|2023-10-01   |510  |\n",
      "|2023-11-01   |521  |\n",
      "|2023-12-01   |517  |\n",
      "|2024-01-01   |471  |\n",
      "|2024-02-01   |481  |\n",
      "|2024-03-01   |454  |\n",
      "|2024-04-01   |487  |\n",
      "|2024-05-01   |491  |\n",
      "|2024-06-01   |489  |\n",
      "|2024-07-01   |485  |\n",
      "|2024-08-01   |518  |\n",
      "|2024-09-01   |511  |\n",
      "|2024-10-01   |513  |\n",
      "|2024-11-01   |491  |\n",
      "|2024-12-01   |498  |\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(*paths)\n",
    "print(\"rows =\", df.count(), \"cols =\", len(df.columns))\n",
    "df.groupBy(\"snapshot_date\").count().orderBy(\"snapshot_date\").show(30, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf60c6c-bad2-4d3d-9329-1b41a77f8e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label files: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 8974 cols = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                       (0 + 24) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|snapshot_date|count|\n",
      "+-------------+-----+\n",
      "|2023-07-01   |530  |\n",
      "|2023-08-01   |501  |\n",
      "|2023-09-01   |506  |\n",
      "|2023-10-01   |510  |\n",
      "|2023-11-01   |521  |\n",
      "|2023-12-01   |517  |\n",
      "|2024-01-01   |471  |\n",
      "|2024-02-01   |481  |\n",
      "|2024-03-01   |454  |\n",
      "|2024-04-01   |487  |\n",
      "|2024-05-01   |491  |\n",
      "|2024-06-01   |489  |\n",
      "|2024-07-01   |485  |\n",
      "|2024-08-01   |518  |\n",
      "|2024-09-01   |511  |\n",
      "|2024-10-01   |513  |\n",
      "|2024-11-01   |491  |\n",
      "|2024-12-01   |498  |\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "LABEL_DIR = \"datamart/gold/label_store\"\n",
    "label_paths = sorted([str(p) for p in Path(LABEL_DIR).glob(\"gold_label_store_*.parquet\")])\n",
    "print(\"Label files:\", len(label_paths))\n",
    "\n",
    "lbl = spark.read.parquet(*label_paths)\n",
    "print(\"rows =\", lbl.count(), \"cols =\", len(lbl.columns))\n",
    "lbl.groupBy(\"snapshot_date\").count().orderBy(\"snapshot_date\").show(40, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "231265a7-eadd-4c27-b035-5de141fac41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 6383|\n",
      "|    1| 2591|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|label|           percent|\n",
      "+-----+------------------+\n",
      "|    0| 71.12770225094718|\n",
      "|    1|28.872297749052816|\n",
      "+-----+------------------+\n",
      "\n",
      "+-----------+-------------+-----+----+-------------+----------------+-----------+\n",
      "|customer_id|snapshot_date|label| dti|emi_to_income|pay_behav_coarse|util_bucket|\n",
      "+-----------+-------------+-----+----+-------------+----------------+-----------+\n",
      "| CUS_0x1037|   2023-07-01|    0|NULL|         NULL|            NULL|       NULL|\n",
      "| CUS_0x1069|   2023-07-01|    0|NULL|         NULL|            NULL|       NULL|\n",
      "| CUS_0x114a|   2023-07-01|    0|NULL|         NULL|            NULL|       NULL|\n",
      "| CUS_0x1184|   2023-07-01|    0|NULL|         NULL|            NULL|       NULL|\n",
      "| CUS_0x1297|   2023-07-01|    1|NULL|         NULL|            NULL|       NULL|\n",
      "| CUS_0x12fb|   2023-07-01|    0|NULL|         NULL|            NULL|       NULL|\n",
      "| CUS_0x1325|   2023-07-01|    0|NULL|         NULL|            NULL|       NULL|\n",
      "| CUS_0x1341|   2023-07-01|    0|NULL|         NULL|            NULL|       NULL|\n",
      "| CUS_0x1375|   2023-07-01|    1|NULL|         NULL|            NULL|       NULL|\n",
      "| CUS_0x13a8|   2023-07-01|    0|NULL|         NULL|            NULL|       NULL|\n",
      "+-----------+-------------+-----+----+-------------+----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Label distribution\n",
    "df.groupBy(\"label\").count().orderBy(\"label\").show()\n",
    "\n",
    "# Percentage breakdown\n",
    "total = df.count()\n",
    "df.groupBy(\"label\").agg(\n",
    "    (F.count(\"*\") / F.lit(total) * 100).alias(\"percent\")\n",
    ").orderBy(\"label\").show()\n",
    "\n",
    "# Sample a few columns\n",
    "df.select(\"customer_id\", \"snapshot_date\", \"label\", \"dti\", \"emi_to_income\", \"pay_behav_coarse\", \"util_bucket\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634ae604-8310-4b95-8d31-25cd86223bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>nulls</th>\n",
       "      <th>null_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>credit_mix</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>num_credit_inquiries</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>interest_rate</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>snap_ym</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>credit_history_age</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>num_of_loan</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>credit_utilization_ratio</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>util_capped</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>invest_to_income</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>emi_to_balance</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>emi_to_income</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dti</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>emi_x_util</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>interest_x_loans</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hist_bucket</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hist_bucket_idx</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>inq_bucket</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>util_sq</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>util_bucket</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>util_bucket_idx</td>\n",
       "      <td>8974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      column  nulls  null_rate\n",
       "8                 credit_mix   8974      100.0\n",
       "7       num_credit_inquiries   8974      100.0\n",
       "6              interest_rate   8974      100.0\n",
       "5                    snap_ym   8974      100.0\n",
       "10        credit_history_age   8974      100.0\n",
       "9                num_of_loan   8974      100.0\n",
       "11  credit_utilization_ratio   8974      100.0\n",
       "16               util_capped   8974      100.0\n",
       "15          invest_to_income   8974      100.0\n",
       "14            emi_to_balance   8974      100.0\n",
       "13             emi_to_income   8974      100.0\n",
       "12                       dti   8974      100.0\n",
       "20                emi_x_util   8974      100.0\n",
       "21          interest_x_loans   8974      100.0\n",
       "22               hist_bucket   8974      100.0\n",
       "23           hist_bucket_idx   8974      100.0\n",
       "24                inq_bucket   8974      100.0\n",
       "17                   util_sq   8974      100.0\n",
       "18               util_bucket   8974      100.0\n",
       "19           util_bucket_idx   8974      100.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = df.count()\n",
    "null_rates = (\n",
    "    df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "      .toPandas()\n",
    "      .T.reset_index()\n",
    "      .rename(columns={\"index\": \"column\", 0: \"nulls\"})\n",
    ")\n",
    "null_rates[\"null_rate\"] = (null_rates[\"nulls\"] / total) * 100\n",
    "null_rates.sort_values(\"null_rate\", ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b4002f-e6e2-4510-b204-be9e92265511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial feature files: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 11974 cols = 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                       (0 + 24) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|snapshot_date|\n",
      "+-------------+\n",
      "|   2023-01-01|\n",
      "|   2023-02-01|\n",
      "|   2023-03-01|\n",
      "|   2023-04-01|\n",
      "|   2023-05-01|\n",
      "|   2023-06-01|\n",
      "|   2023-07-01|\n",
      "|   2023-08-01|\n",
      "|   2023-09-01|\n",
      "|   2023-10-01|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "fin_dir = \"datamart/gold/features/financials\"\n",
    "fin_paths = sorted([str(p) for p in Path(fin_dir).glob(\"gold_financials_features_*.parquet\")])\n",
    "print(\"Financial feature files:\", len(fin_paths))\n",
    "\n",
    "fin_df = spark.read.parquet(*fin_paths)\n",
    "print(\"rows =\", fin_df.count(), \"cols =\", len(fin_df.columns))\n",
    "fin_df.select(\"snapshot_date\").distinct().orderBy(\"snapshot_date\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9e86b4-65b8-4fc3-95af-b2e2c70e9b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      "\n",
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "| CUS_0x1037|\n",
      "| CUS_0x1069|\n",
      "| CUS_0x114a|\n",
      "| CUS_0x1184|\n",
      "| CUS_0x1297|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|snapshot_date|\n",
      "+-------------+\n",
      "|   2023-07-01|\n",
      "|   2023-08-01|\n",
      "|   2023-09-01|\n",
      "|   2023-10-01|\n",
      "|   2023-11-01|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      "\n",
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "| CUS_0x10ac|\n",
      "| CUS_0x10c5|\n",
      "| CUS_0x1145|\n",
      "| CUS_0x11ac|\n",
      "| CUS_0x122c|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                       (0 + 24) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|snapshot_date|\n",
      "+-------------+\n",
      "|   2023-01-01|\n",
      "|   2023-02-01|\n",
      "|   2023-03-01|\n",
      "|   2023-04-01|\n",
      "|   2023-05-01|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Label store keys\n",
    "lbl.select(\"customer_id\", \"snapshot_date\").printSchema()\n",
    "lbl.select(\"customer_id\").show(5)\n",
    "lbl.select(\"snapshot_date\").distinct().orderBy(\"snapshot_date\").show(5)\n",
    "\n",
    "# Financial features keys\n",
    "fin_df.select(\"customer_id\", \"snapshot_date\").printSchema()\n",
    "fin_df.select(\"customer_id\").show(5)\n",
    "fin_df.select(\"snapshot_date\").distinct().orderBy(\"snapshot_date\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b43a3bd-5795-4ced-8e51-bcf096799485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                       (0 + 24) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact-key overlaps (raw): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "overlap_now = lbl.join(fin_df, [\"customer_id\",\"snapshot_date\"], \"inner\").count()\n",
    "print(\"Exact-key overlaps (raw):\", overlap_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d33743-7db0-4604-9d39-b4d8b95b951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                       (0 + 24) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlaps after TRIM+UPPER: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "lbl_n  = (lbl\n",
    "          .withColumn(\"cid_norm\",  F.upper(F.trim(F.col(\"customer_id\"))))\n",
    "          .withColumn(\"sdate_norm\", F.to_date(\"snapshot_date\")))\n",
    "\n",
    "fin_n  = (fin_df\n",
    "          .withColumn(\"cid_norm\",  F.upper(F.trim(F.col(\"customer_id\"))))\n",
    "          .withColumn(\"sdate_norm\", F.to_date(\"snapshot_date\")))\n",
    "\n",
    "overlap_norm = (lbl_n\n",
    "                .join(fin_n, [\"cid_norm\",\"sdate_norm\"], \"inner\")\n",
    "                .count())\n",
    "print(\"Overlaps after TRIM+UPPER:\", overlap_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e49d850-c5a5-4c6b-b611-a26c07148e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW columns:\n",
      "LMS : ['loan_id', 'Customer_ID', 'loan_start_date', 'tenure', 'installment_num', 'loan_amt', 'due_amt', 'paid_amt', 'overdue_amt', 'balance', 'snapshot_date']\n",
      "FIN : ['Customer_ID', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Credit_History_Age', 'Payment_of_Min_Amount', 'Total_EMI_per_month', 'Amount_invested_monthly', 'Payment_Behaviour']\n",
      "ATTR: ['Customer_ID', 'Name', 'Age', 'SSN', 'Occupation', 'snapshot_date']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- Read raw CSVs (correct file names & safe quote syntax) ---\n",
    "lms_raw = spark.read.option(\"header\", True).csv(\"data/lms_loan_daily.csv\")\n",
    "\n",
    "fin_raw = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"multiLine\", True)\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .csv(\"data/features_financials.csv\")\n",
    ")\n",
    "\n",
    "att_raw = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"multiLine\", True)\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .csv(\"data/features_attributes.csv\")\n",
    ")\n",
    "\n",
    "print(\"RAW columns:\")\n",
    "print(\"LMS :\", lms_raw.columns[:20])\n",
    "print(\"FIN :\", fin_raw.columns[:20])\n",
    "print(\"ATTR:\", att_raw.columns[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b40542-510e-4062-99b5-cc2932bf6544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected IDs: {'lms': 'Customer_ID', 'fin': 'Customer_ID', 'att': 'Customer_ID'}\n"
     ]
    }
   ],
   "source": [
    "def pick_id(df):\n",
    "    for c in [\"Customer_ID\",\"customer_id\",\"CUSTOMER_ID\",\"cust_id\",\"id\",\"ID\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(\"No obvious ID column found.\")\n",
    "\n",
    "lms_id = pick_id(lms_raw)\n",
    "fin_id = pick_id(fin_raw)\n",
    "att_id = pick_id(att_raw)\n",
    "\n",
    "def with_norm_id(df, col):\n",
    "    return (df\n",
    "            .withColumn(\"cid_norm\", F.upper(F.trim(F.col(col))))\n",
    "            .withColumn(\"snapshot_norm\",\n",
    "                        F.coalesce(\n",
    "                            F.to_date(\"snapshot_date\",\"d/M/yy\"),\n",
    "                            F.to_date(\"snapshot_date\",\"dd/MM/yy\"),\n",
    "                            F.to_date(\"snapshot_date\",\"d/M/yyyy\"),\n",
    "                            F.to_date(\"snapshot_date\",\"dd/MM/yyyy\"),\n",
    "                            F.to_date(\"snapshot_date\",\"M/d/yyyy\"),\n",
    "                            F.to_date(\"snapshot_date\",\"yyyy-MM-dd\"),\n",
    "                            F.to_date(\"snapshot_date\")\n",
    "                        )))\n",
    "\n",
    "lmsN = with_norm_id(lms_raw, lms_id)\n",
    "finN = with_norm_id(fin_raw, fin_id)\n",
    "attN = with_norm_id(att_raw, att_id)\n",
    "\n",
    "print(\"Detected IDs:\", {\"lms\": lms_id, \"fin\": fin_id, \"att\": att_id})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "573418b0-56b0-40a6-8ce9-9f216a2833d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct IDs → LMS/FIN/ATTR: 12500 12500 12500\n",
      "LMS ∩ FIN : 12500\n",
      "LMS ∩ ATTR: 12500\n",
      "FIN ∩ ATTR: 12500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "July-2023 → counts LMS/FIN/ATTR: 3556 471 471\n",
      "July-2023 LMS ∩ FIN : 471\n",
      "July-2023 LMS ∩ ATTR: 471\n",
      "July-2023 FIN ∩ ATTR: 471\n"
     ]
    }
   ],
   "source": [
    "# Overall distinct ID counts\n",
    "print(\"Distinct IDs → LMS/FIN/ATTR:\",\n",
    "      lmsN.select(\"cid_norm\").distinct().count(),\n",
    "      finN.select(\"cid_norm\").distinct().count(),\n",
    "      attN.select(\"cid_norm\").distinct().count())\n",
    "\n",
    "# Overall intersections\n",
    "print(\"LMS ∩ FIN :\", lmsN.select(\"cid_norm\").distinct()\n",
    "                      .join(finN.select(\"cid_norm\").distinct(), \"cid_norm\", \"inner\").count())\n",
    "print(\"LMS ∩ ATTR:\", lmsN.select(\"cid_norm\").distinct()\n",
    "                      .join(attN.select(\"cid_norm\").distinct(), \"cid_norm\", \"inner\").count())\n",
    "print(\"FIN ∩ ATTR:\", finN.select(\"cid_norm\").distinct()\n",
    "                      .join(attN.select(\"cid_norm\").distinct(), \"cid_norm\", \"inner\").count())\n",
    "\n",
    "# Restrict to July 2023 (matches how the pipeline filters)\n",
    "lms_jul = lmsN.where(F.date_format(\"snapshot_norm\",\"yyyy-MM\")==\"2023-07\").select(\"cid_norm\").distinct()\n",
    "fin_jul = finN.where(F.date_format(\"snapshot_norm\",\"yyyy-MM\")==\"2023-07\").select(\"cid_norm\").distinct()\n",
    "att_jul = attN.where(F.date_format(\"snapshot_norm\",\"yyyy-MM\")==\"2023-07\").select(\"cid_norm\").distinct()\n",
    "\n",
    "print(\"July-2023 → counts LMS/FIN/ATTR:\", lms_jul.count(), fin_jul.count(), att_jul.count())\n",
    "print(\"July-2023 LMS ∩ FIN :\", lms_jul.join(fin_jul,\"cid_norm\",\"inner\").count())\n",
    "print(\"July-2023 LMS ∩ ATTR:\", lms_jul.join(att_jul,\"cid_norm\",\"inner\").count())\n",
    "print(\"July-2023 FIN ∩ ATTR:\", fin_jul.join(att_jul,\"cid_norm\",\"inner\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43ff089c-5462-4c37-a902-119ad04b2a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold-only ID overlap: 8974\n"
     ]
    }
   ],
   "source": [
    "# Distinct IDs in Gold label store vs Gold financial features\n",
    "lbl_ids  = lbl.select(F.upper(F.trim(\"customer_id\")).alias(\"cid\")).distinct()\n",
    "fin_ids  = fin_df.select(F.upper(F.trim(\"customer_id\")).alias(\"cid\")).distinct()\n",
    "\n",
    "print(\"Gold-only ID overlap:\", lbl_ids.join(fin_ids, \"cid\", \"inner\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c9c01bb-60a6-4f7b-ba3a-3f1b22df2f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_07_01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_08_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_09_01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_10_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_11_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_12_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_01_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_02_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_03_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_04_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_05_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_06_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_07_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_08_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_09_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_10_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_11_01.parquet\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_12_01.parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "LABEL_DIR = \"datamart/gold/label_store\"\n",
    "FIN_DIR   = \"datamart/gold/features/financials\"\n",
    "ATT_DIR   = \"datamart/gold/features/attributes\"\n",
    "OUT_DIR   = \"datamart/gold/model_dataset\"   # overwrite in-place\n",
    "\n",
    "def token(ds: str) -> str:\n",
    "    return datetime.strptime(ds, \"%Y-%m-%d\").strftime(\"%Y_%m_%d\")\n",
    "\n",
    "def rebuild_one_month(ds: str):\n",
    "    tok = token(ds)\n",
    "    p_lbl = f\"{LABEL_DIR}/gold_label_store_{tok}.parquet\"\n",
    "    p_fin = f\"{FIN_DIR}/gold_financials_features_{tok}.parquet\"\n",
    "    p_att = f\"{ATT_DIR}/gold_attributes_features_{tok}.parquet\"\n",
    "\n",
    "    # Read\n",
    "    lbl = (spark.read.parquet(p_lbl)\n",
    "           .withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "           .select(\"customer_id\",\"snapshot_date\",\"loan_id\",\"label\",\"label_def\"))\n",
    "    fin = (spark.read.parquet(p_fin)\n",
    "           .withColumnRenamed(\"Customer_ID\",\"customer_id\"))\n",
    "    att = (spark.read.parquet(p_att)\n",
    "           .withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "           .drop(\"snap_ym\"))\n",
    "\n",
    "    # Normalize join keys: TRIM+UPPER for ID and MONTH key for dates\n",
    "    def norm(d):\n",
    "        return (d\n",
    "                .withColumn(\"cid_norm\", F.upper(F.trim(\"customer_id\")))\n",
    "                .withColumn(\"ym_norm\", F.date_format(F.to_date(\"snapshot_date\"), \"yyyy-MM\")))\n",
    "\n",
    "    lbln, finn, attn = norm(lbl), norm(fin), norm(att)\n",
    "\n",
    "    # Join by (customer_id, month) via the normalized columns\n",
    "    ds_joined = (lbln\n",
    "                 .join(finn.drop(\"customer_id\",\"snapshot_date\"), [\"cid_norm\",\"ym_norm\"], \"left\")\n",
    "                 .join(attn.drop(\"customer_id\",\"snapshot_date\"), [\"cid_norm\",\"ym_norm\"], \"left\"))\n",
    "\n",
    "    # Restore clean columns: keep label’s first-of-month snapshot_date\n",
    "    out_cols_front = [\"customer_id\",\"snapshot_date\",\"loan_id\",\"label\",\"label_def\"]\n",
    "    ds_out = (ds_joined\n",
    "              .withColumn(\"customer_id\", F.col(\"cid_norm\"))   # already UPPER+TRIM\n",
    "              .drop(\"cid_norm\",\"ym_norm\"))\n",
    "\n",
    "    # Reorder: label keys first, then the rest\n",
    "    rest = [c for c in ds_out.columns if c not in out_cols_front]\n",
    "    ds_out = ds_out.select(*out_cols_front, *rest)\n",
    "\n",
    "    out_path = f\"{OUT_DIR}/gold_model_dataset_{tok}.parquet\"\n",
    "    ds_out.write.mode(\"overwrite\").parquet(out_path)\n",
    "    print(\"[REBUILT]\", out_path)\n",
    "\n",
    "# Rebuild for all months present in label store\n",
    "months = [r[\"snapshot_date\"].strftime(\"%Y-%m-%d\")\n",
    "          for r in spark.read.parquet(f\"{LABEL_DIR}/gold_label_store_*.parquet\")\n",
    "                            .select(\"snapshot_date\").distinct()\n",
    "                            .orderBy(\"snapshot_date\").collect()]\n",
    "\n",
    "for m in months:\n",
    "    rebuild_one_month(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97a1e9bf-dde6-4d43-a971-5a54c5be0e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|dti_non_null_rate|\n",
      "+-----------------+\n",
      "|              0.0|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|emi_to_income_non_null_rate|\n",
      "+---------------------------+\n",
      "|                        0.0|\n",
      "+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|credit_utilization_ratio_non_null_rate|\n",
      "+--------------------------------------+\n",
      "|                                   0.0|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|pay_behav_coarse_non_null_rate|\n",
      "+------------------------------+\n",
      "|                           0.0|\n",
      "+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|util_bucket_non_null_rate|\n",
      "+-------------------------+\n",
      "|                      0.0|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 289:>                                                      (0 + 24) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|snapshot_date|count|\n",
      "+-------------+-----+\n",
      "|2023-07-01   |530  |\n",
      "|2023-08-01   |501  |\n",
      "|2023-09-01   |506  |\n",
      "|2023-10-01   |510  |\n",
      "|2023-11-01   |521  |\n",
      "|2023-12-01   |517  |\n",
      "|2024-01-01   |471  |\n",
      "|2024-02-01   |481  |\n",
      "|2024-03-01   |454  |\n",
      "|2024-04-01   |487  |\n",
      "|2024-05-01   |491  |\n",
      "|2024-06-01   |489  |\n",
      "|2024-07-01   |485  |\n",
      "|2024-08-01   |518  |\n",
      "|2024-09-01   |511  |\n",
      "|2024-10-01   |513  |\n",
      "|2024-11-01   |491  |\n",
      "|2024-12-01   |498  |\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "md = spark.read.parquet(\"datamart/gold/model_dataset/gold_model_dataset_*.parquet\")\n",
    "\n",
    "# Check a few key features’ non-null rates\n",
    "for c in [\"dti\",\"emi_to_income\",\"credit_utilization_ratio\",\"pay_behav_coarse\",\"util_bucket\"]:\n",
    "    if c in md.columns:\n",
    "        md.agg((1 - F.avg(F.col(c).isNull().cast(\"double\"))).alias(f\"{c}_non_null_rate\")).show()\n",
    "\n",
    "# Month counts (should match your label counts)\n",
    "md.groupBy(\"snapshot_date\").count().orderBy(\"snapshot_date\").show(30, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e576b1a-5153-45e1-a6fd-471ac758aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|dti_non_null_in_fin|\n",
      "+-------------------+\n",
      "|                1.0|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|emi_to_income_non_null_in_fin|\n",
      "+-----------------------------+\n",
      "|                          1.0|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|credit_utilization_ratio_non_null_in_fin|\n",
      "+----------------------------------------+\n",
      "|                                     1.0|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|pay_behav_coarse_non_null_in_fin|\n",
      "+--------------------------------+\n",
      "|                             1.0|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|util_bucket_non_null_in_fin|\n",
      "+---------------------------+\n",
      "|                        1.0|\n",
      "+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 307:>                                                      (0 + 24) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-------------------+\n",
      "|ym_try_nulls|ym_from_snap_nulls|ym_norm_probe_nulls|\n",
      "+------------+------------------+-------------------+\n",
      "|           0|                 0|                  0|\n",
      "+------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# A) Are the financial feature columns actually populated in GOLD (before any join)?\n",
    "for c in [\"dti\",\"emi_to_income\",\"credit_utilization_ratio\",\"pay_behav_coarse\",\"util_bucket\"]:\n",
    "    if c in fin_df.columns:\n",
    "        fin_df.agg((1 - F.avg(F.col(c).isNull().cast(\"double\"))).alias(f\"{c}_non_null_in_fin\")).show()\n",
    "\n",
    "# B) Is ym_norm becoming NULL inside GOLD features? (if yes, no join can match)\n",
    "fin_ym = (fin_df\n",
    "          .withColumn(\"ym_try\",\n",
    "                      F.date_format(F.to_date(\"snapshot_date\"), \"yyyy-MM\"))\n",
    "          .withColumn(\"ym_from_snap\",\n",
    "                      F.when(F.length(\"snap_ym\")==7, F.col(\"snap_ym\"))\n",
    "                       .otherwise(F.date_format(F.to_timestamp(\"snap_ym\"), \"yyyy-MM\")))\n",
    "          .withColumn(\"ym_norm_probe\", F.coalesce(\"ym_try\",\"ym_from_snap\")))\n",
    "fin_ym.agg(\n",
    "    F.sum(F.col(\"ym_try\").isNull().cast(\"int\")).alias(\"ym_try_nulls\"),\n",
    "    F.sum(F.col(\"ym_from_snap\").isNull().cast(\"int\")).alias(\"ym_from_snap_nulls\"),\n",
    "    F.sum(F.col(\"ym_norm_probe\").isNull().cast(\"int\")).alias(\"ym_norm_probe_nulls\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53539851-b3b4-4cd1-9f6f-c51f8304b1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_07_01.parquet\n",
      "[2023-08-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_08_01.parquet\n",
      "[2023-09-01] overlap keys  label↔fin = 0   label↔att = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_09_01.parquet\n",
      "[2023-10-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_10_01.parquet\n",
      "[2023-11-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_11_01.parquet\n",
      "[2023-12-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2023_12_01.parquet\n",
      "[2024-01-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_01_01.parquet\n",
      "[2024-02-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_02_01.parquet\n",
      "[2024-03-01] overlap keys  label↔fin = 0   label↔att = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_03_01.parquet\n",
      "[2024-04-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_04_01.parquet\n",
      "[2024-05-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_05_01.parquet\n",
      "[2024-06-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_06_01.parquet\n",
      "[2024-07-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_07_01.parquet\n",
      "[2024-08-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_08_01.parquet\n",
      "[2024-09-01] overlap keys  label↔fin = 0   label↔att = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_09_01.parquet\n",
      "[2024-10-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_10_01.parquet\n",
      "[2024-11-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_11_01.parquet\n",
      "[2024-12-01] overlap keys  label↔fin = 0   label↔att = 0\n",
      "[REBUILT] datamart/gold/model_dataset/gold_model_dataset_2024_12_01.parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "LABEL_DIR = \"datamart/gold/label_store\"\n",
    "FIN_DIR   = \"datamart/gold/features/financials\"\n",
    "ATT_DIR   = \"datamart/gold/features/attributes\"\n",
    "OUT_DIR   = \"datamart/gold/model_dataset\"\n",
    "\n",
    "def token(ds: str) -> str:\n",
    "    return datetime.strptime(ds, \"%Y-%m-%d\").strftime(\"%Y_%m_%d\")\n",
    "\n",
    "def normalize_id(df):\n",
    "    if \"Customer_ID\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"Customer_ID\", \"customer_id\")\n",
    "    return df.withColumn(\"customer_id\", F.upper(F.trim(F.col(\"customer_id\"))))\n",
    "\n",
    "def add_ym(df):\n",
    "    # Always try from snapshot_date → 'yyyy-MM'\n",
    "    ym_try = F.date_format(F.to_date(\"snapshot_date\"), \"yyyy-MM\")\n",
    "    # Only fall back to snap_ym if the column exists in this DF\n",
    "    if \"snap_ym\" in df.columns:\n",
    "        ym_from_snap = F.when(F.length(\"snap_ym\")==7, F.col(\"snap_ym\")) \\\n",
    "                        .otherwise(F.date_format(F.to_timestamp(\"snap_ym\"), \"yyyy-MM\"))\n",
    "        return df.withColumn(\"ym_norm\", F.coalesce(ym_try, ym_from_snap))\n",
    "    else:\n",
    "        return df.withColumn(\"ym_norm\", ym_try)\n",
    "\n",
    "def rebuild_one_month(ds: str):\n",
    "    tok = token(ds)\n",
    "    p_lbl = f\"{LABEL_DIR}/gold_label_store_{tok}.parquet\"\n",
    "    p_fin = f\"{FIN_DIR}/gold_financials_features_{tok}.parquet\"\n",
    "    p_att = f\"{ATT_DIR}/gold_attributes_features_{tok}.parquet\"\n",
    "    p_out = f\"{OUT_DIR}/gold_model_dataset_{tok}.parquet\"\n",
    "\n",
    "    lbl_raw = spark.read.parquet(p_lbl)\n",
    "    fin_raw = spark.read.parquet(p_fin)\n",
    "    att_raw = spark.read.parquet(p_att)\n",
    "\n",
    "    lbl = add_ym(normalize_id(lbl_raw)) \\\n",
    "            .select(\"customer_id\",\"snapshot_date\",\"ym_norm\",\"loan_id\",\"label\",\"label_def\")\n",
    "    fin = add_ym(normalize_id(fin_raw))\n",
    "    att = add_ym(normalize_id(att_raw)).drop(\"snap_ym\")\n",
    "\n",
    "    # diagnostics: check monthly key overlap before join\n",
    "    lbl_keys = lbl.select(\"customer_id\",\"ym_norm\").distinct()\n",
    "    fin_keys = fin.select(\"customer_id\",\"ym_norm\").distinct()\n",
    "    att_keys = att.select(\"customer_id\",\"ym_norm\").distinct()\n",
    "    o_lbl_fin = lbl_keys.join(fin_keys, [\"customer_id\",\"ym_norm\"], \"inner\").count()\n",
    "    o_lbl_att = lbl_keys.join(att_keys, [\"customer_id\",\"ym_norm\"], \"inner\").count()\n",
    "    print(f\"[{ds}] overlap keys  label↔fin = {o_lbl_fin:,}   label↔att = {o_lbl_att:,}\")\n",
    "\n",
    "    # left-join on normalized keys\n",
    "    ds_join = (lbl\n",
    "               .join(fin.drop(\"snapshot_date\"), [\"customer_id\",\"ym_norm\"], \"left\")\n",
    "               .join(att.drop(\"snapshot_date\"), [\"customer_id\",\"ym_norm\"], \"left\"))\n",
    "\n",
    "    # keep label's first-of-month snapshot_date\n",
    "    front = [\"customer_id\",\"snapshot_date\",\"loan_id\",\"label\",\"label_def\"]\n",
    "    rest  = [c for c in ds_join.columns if c not in front + [\"ym_norm\"]]\n",
    "    out   = ds_join.select(*front, *rest)\n",
    "\n",
    "    out.write.mode(\"overwrite\").parquet(p_out)\n",
    "    print(\"[REBUILT]\", p_out)\n",
    "\n",
    "# Rebuild all months\n",
    "months = [r[\"snapshot_date\"].strftime(\"%Y-%m-%d\")\n",
    "          for r in spark.read.parquet(f\"{LABEL_DIR}/gold_label_store_*.parquet\")\n",
    "                             .select(\"snapshot_date\").distinct()\n",
    "                             .orderBy(\"snapshot_date\").collect()]\n",
    "for m in months:\n",
    "    rebuild_one_month(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a53f1d53-67e9-4500-a249-8910da92fc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|ym_norm|\n",
      "+-------+\n",
      "|2023-07|\n",
      "+-------+\n",
      "\n",
      "Label ym_norm distinct: None\n",
      "+-------+-------+\n",
      "|ym_try |ym_snap|\n",
      "+-------+-------+\n",
      "|2023-07|2023-07|\n",
      "+-------+-------+\n",
      "\n",
      "Fin ym_try, ym_snap distinct: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|ym_try|ym_snap|\n",
      "+------+-------+\n",
      "|NULL  |2023-07|\n",
      "+------+-------+\n",
      "\n",
      "Att ym_try, ym_snap distinct: None\n",
      "Examples missing in FIN:\n",
      "+-----------+-------+\n",
      "|customer_id|ym_norm|\n",
      "+-----------+-------+\n",
      "|CUS_0X53F4 |2023-07|\n",
      "|CUS_0X69BE |2023-07|\n",
      "|CUS_0X1037 |2023-07|\n",
      "|CUS_0X42C6 |2023-07|\n",
      "|CUS_0X9BEF |2023-07|\n",
      "+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "def token(ds: str) -> str:\n",
    "    return datetime.strptime(ds, \"%Y-%m-%d\").strftime(\"%Y_%m_%d\")\n",
    "\n",
    "ds = \"2023-07-01\"\n",
    "tok = token(ds)\n",
    "\n",
    "p_lbl = f\"datamart/gold/label_store/gold_label_store_{tok}.parquet\"\n",
    "p_fin = f\"datamart/gold/features/financials/gold_financials_features_{tok}.parquet\"\n",
    "p_att = f\"datamart/gold/features/attributes/gold_attributes_features_{tok}.parquet\"\n",
    "\n",
    "lbl = spark.read.parquet(p_lbl).withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "fin = spark.read.parquet(p_fin).withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "att = spark.read.parquet(p_att).withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "\n",
    "# Normalize\n",
    "lbl = lbl.withColumn(\"customer_id\", F.upper(F.trim(\"customer_id\")))\\\n",
    "         .withColumn(\"ym_norm\", F.date_format(F.to_date(\"snapshot_date\"), \"yyyy-MM\"))\n",
    "fin = fin.withColumn(\"customer_id\", F.upper(F.trim(\"customer_id\")))\\\n",
    "         .withColumn(\"ym_try\", F.date_format(F.to_date(\"snapshot_date\"), \"yyyy-MM\"))\\\n",
    "         .withColumn(\"ym_snap\", F.when(F.length(\"snap_ym\")==7, F.col(\"snap_ym\"))\n",
    "                                  .otherwise(F.date_format(F.to_timestamp(\"snap_ym\"), \"yyyy-MM\")))\n",
    "att = att.withColumn(\"customer_id\", F.upper(F.trim(\"customer_id\")))\\\n",
    "         .withColumn(\"ym_try\", F.date_format(F.to_date(\"snapshot_date\"), \"yyyy-MM\"))\\\n",
    "         .withColumn(\"ym_snap\", F.when(F.length(\"snap_ym\")==7, F.col(\"snap_ym\"))\n",
    "                                  .otherwise(F.date_format(F.to_timestamp(\"snap_ym\"), \"yyyy-MM\")))\n",
    "\n",
    "print(\"Label ym_norm distinct:\", lbl.select(\"ym_norm\").distinct().orderBy(\"ym_norm\").show())\n",
    "print(\"Fin ym_try, ym_snap distinct:\", fin.select(\"ym_try\",\"ym_snap\").distinct().orderBy(\"ym_try\",\"ym_snap\").show(10, False))\n",
    "print(\"Att ym_try, ym_snap distinct:\", att.select(\"ym_try\",\"ym_snap\").distinct().orderBy(\"ym_try\",\"ym_snap\").show(10, False))\n",
    "\n",
    "# Show a few non-matching examples (anti-join)\n",
    "lbl_keys = lbl.select(\"customer_id\",\"ym_norm\").distinct()\n",
    "fin_keys = fin.select(F.col(\"customer_id\"), F.coalesce(\"ym_try\",\"ym_snap\").alias(\"ym_norm\")).distinct()\n",
    "\n",
    "not_in_fin = lbl_keys.join(fin_keys, [\"customer_id\",\"ym_norm\"], \"left_anti\")\n",
    "print(\"Examples missing in FIN:\")\n",
    "not_in_fin.show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c94cf58e-c913-47f2-aff2-890d5092815d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `snap_ym` cannot be resolved. Did you mean one of the following? [`label`, `cid_key`, `loan_id`, `label_def`, `snapshot_date`].;\n'Project [loan_id#28498, customer_id#28628, label#28500, label_def#28501, snapshot_date#28502, cid_key#28635, coalesce(date_format(cast(to_date(snapshot_date#28502, None, Some(Etc/UTC), false) as timestamp), yyyy-MM, Some(Etc/UTC)), CASE WHEN isnotnull('snap_ym) THEN CASE WHEN (length('snap_ym) = 7) THEN 'snap_ym ELSE date_format(to_timestamp('snap_ym, None, TimestampType, Some(Etc/UTC), false), yyyy-MM, Some(Etc/UTC)) END END) AS ym_key#28642]\n+- Project [loan_id#28498, customer_id#28628, label#28500, label_def#28501, snapshot_date#28502, regexp_replace(regexp_replace(upper(trim(customer_id#28628, None)), \\s+, , 1), [^A-Z0-9_], , 1) AS cid_key#28635]\n   +- Project [loan_id#28498, Customer_ID#28499 AS customer_id#28628, label#28500, label_def#28501, snapshot_date#28502]\n      +- Relation [loan_id#28498,Customer_ID#28499,label#28500,label_def#28501,snapshot_date#28502] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m fin = spark.read.parquet(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdatamart/gold/features/financials/gold_financials_features_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtok\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m att = spark.read.parquet(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdatamart/gold/features/attributes/gold_attributes_features_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtok\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m lblK = \u001b[43mmk_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlbl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomer_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msnapshot_date\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m finK = mk_keys(fin, id_col=\u001b[33m\"\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m\"\u001b[39m, date_col=\u001b[33m\"\u001b[39m\u001b[33msnapshot_date\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m attK = mk_keys(att, id_col=\u001b[33m\"\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m\"\u001b[39m, date_col=\u001b[33m\"\u001b[39m\u001b[33msnapshot_date\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mmk_keys\u001b[39m\u001b[34m(df, id_col, date_col, literal_ym)\u001b[39m\n\u001b[32m     17\u001b[39m     ym_snap = F.when(F.col(\u001b[33m\"\u001b[39m\u001b[33msnap_ym\u001b[39m\u001b[33m\"\u001b[39m).isNotNull(),\n\u001b[32m     18\u001b[39m                      F.when(F.length(\u001b[33m\"\u001b[39m\u001b[33msnap_ym\u001b[39m\u001b[33m\"\u001b[39m)==\u001b[32m7\u001b[39m, F.col(\u001b[33m\"\u001b[39m\u001b[33msnap_ym\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     19\u001b[39m                       .otherwise(F.date_format(F.to_timestamp(\u001b[33m\"\u001b[39m\u001b[33msnap_ym\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33myyyy-MM\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     20\u001b[39m     ym = F.coalesce(ym_try, ym_snap)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mdf\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcid_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mym_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mym\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/dataframe.py:5176\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   5171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   5172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   5173\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5174\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   5175\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `snap_ym` cannot be resolved. Did you mean one of the following? [`label`, `cid_key`, `loan_id`, `label_def`, `snapshot_date`].;\n'Project [loan_id#28498, customer_id#28628, label#28500, label_def#28501, snapshot_date#28502, cid_key#28635, coalesce(date_format(cast(to_date(snapshot_date#28502, None, Some(Etc/UTC), false) as timestamp), yyyy-MM, Some(Etc/UTC)), CASE WHEN isnotnull('snap_ym) THEN CASE WHEN (length('snap_ym) = 7) THEN 'snap_ym ELSE date_format(to_timestamp('snap_ym, None, TimestampType, Some(Etc/UTC), false), yyyy-MM, Some(Etc/UTC)) END END) AS ym_key#28642]\n+- Project [loan_id#28498, customer_id#28628, label#28500, label_def#28501, snapshot_date#28502, regexp_replace(regexp_replace(upper(trim(customer_id#28628, None)), \\s+, , 1), [^A-Z0-9_], , 1) AS cid_key#28635]\n   +- Project [loan_id#28498, Customer_ID#28499 AS customer_id#28628, label#28500, label_def#28501, snapshot_date#28502]\n      +- Relation [loan_id#28498,Customer_ID#28499,label#28500,label_def#28501,snapshot_date#28502] parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def mk_keys(df, id_col=\"customer_id\", date_col=\"snapshot_date\", literal_ym=None):\n",
    "    # unify ID column name\n",
    "    if \"Customer_ID\" in df.columns and id_col != \"Customer_ID\":\n",
    "        df = df.withColumnRenamed(\"Customer_ID\", id_col)\n",
    "    # normalize ID: trim, upper, remove all whitespace, then strip any non [A-Z0-9_]\n",
    "    cid = F.upper(F.trim(F.col(id_col)))\n",
    "    cid = F.regexp_replace(cid, r\"\\s+\", \"\")                 # remove spaces/tabs/nbspaces\n",
    "    cid = F.regexp_replace(cid, r\"[^A-Z0-9_]\", \"\")          # remove hidden punctuation, zero-widths, etc.\n",
    "\n",
    "    # month key from literal (file month) OR from columns\n",
    "    if literal_ym is not None:\n",
    "        ym = F.lit(literal_ym)\n",
    "    else:\n",
    "        ym_try  = F.date_format(F.to_date(date_col), \"yyyy-MM\")\n",
    "        ym_snap = F.when(F.col(\"snap_ym\").isNotNull(),\n",
    "                         F.when(F.length(\"snap_ym\")==7, F.col(\"snap_ym\"))\n",
    "                          .otherwise(F.date_format(F.to_timestamp(\"snap_ym\"), \"yyyy-MM\")))\n",
    "        ym = F.coalesce(ym_try, ym_snap)\n",
    "\n",
    "    return (df\n",
    "            .withColumn(\"cid_key\", cid)\n",
    "            .withColumn(\"ym_key\", ym))\n",
    "\n",
    "# --- July 2023 only (Gold paths) ---\n",
    "ds  = \"2023-07-01\"\n",
    "tok = \"2023_07_01\"\n",
    "\n",
    "lbl = spark.read.parquet(f\"datamart/gold/label_store/gold_label_store_{tok}.parquet\")\n",
    "fin = spark.read.parquet(f\"datamart/gold/features/financials/gold_financials_features_{tok}.parquet\")\n",
    "att = spark.read.parquet(f\"datamart/gold/features/attributes/gold_attributes_features_{tok}.parquet\")\n",
    "\n",
    "lblK = mk_keys(lbl, id_col=\"customer_id\", date_col=\"snapshot_date\")\n",
    "finK = mk_keys(fin, id_col=\"customer_id\", date_col=\"snapshot_date\")\n",
    "attK = mk_keys(att, id_col=\"customer_id\", date_col=\"snapshot_date\")\n",
    "\n",
    "# Inspect a few keys\n",
    "print(\"Sample label IDs:\", [r[0] for r in lblK.select(\"cid_key\").distinct().limit(5).collect()])\n",
    "print(\"Sample fin   IDs:\", [r[0] for r in finK.select(\"cid_key\").distinct().limit(5).collect()])\n",
    "\n",
    "# Overlaps on aggressive keys (should be > 0)\n",
    "ov_fin = (lblK.select(\"cid_key\",\"ym_key\").distinct()\n",
    "               .join(finK.select(\"cid_key\",\"ym_key\").distinct(), [\"cid_key\",\"ym_key\"], \"inner\").count())\n",
    "ov_att = (lblK.select(\"cid_key\",\"ym_key\").distinct()\n",
    "               .join(attK.select(\"cid_key\",\"ym_key\").distinct(), [\"cid_key\",\"ym_key\"], \"inner\").count())\n",
    "print(\"Aggressive overlap July → label↔fin:\", ov_fin, \" label↔att:\", ov_att)\n",
    "\n",
    "# If still 0, show a few anti-join samples side-by-side (first 10)\n",
    "miss = (lblK.select(\"cid_key\",\"ym_key\").distinct()\n",
    "             .join(finK.select(\"cid_key\",\"ym_key\").distinct(), [\"cid_key\",\"ym_key\"], \"left_anti\"))\n",
    "print(\"Examples missing in FIN after aggressive norm:\")\n",
    "miss.show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6802740b-a506-49e9-a50a-1929d1f0a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def mk_keys(df, id_col=\"customer_id\", date_col=\"snapshot_date\", literal_ym=None):\n",
    "    # unify ID column name\n",
    "    if \"Customer_ID\" in df.columns and id_col != \"Customer_ID\":\n",
    "        df = df.withColumnRenamed(\"Customer_ID\", id_col)\n",
    "\n",
    "    # aggressive ID normalization: trim, upper, remove whitespace & non [A-Z0-9_]\n",
    "    cid = F.upper(F.trim(F.col(id_col)))\n",
    "    cid = F.regexp_replace(cid, r\"\\s+\", \"\")\n",
    "    cid = F.regexp_replace(cid, r\"[^A-Z0-9_]\", \"\")\n",
    "\n",
    "    # month key: prefer explicit literal (file month), else snapshot_date, else snap_ym (if present)\n",
    "    if literal_ym is not None:\n",
    "        ym = F.lit(literal_ym)  # e.g. '2023-07'\n",
    "    else:\n",
    "        ym_try = F.date_format(F.to_date(F.col(date_col)), \"yyyy-MM\")\n",
    "        if \"snap_ym\" in df.columns:\n",
    "            ym_snap = F.when(F.length(\"snap_ym\")==7, F.col(\"snap_ym\")) \\\n",
    "                       .otherwise(F.date_format(F.to_timestamp(\"snap_ym\"), \"yyyy-MM\"))\n",
    "            ym = F.coalesce(ym_try, ym_snap)\n",
    "        else:\n",
    "            ym = ym_try\n",
    "\n",
    "    return df.withColumn(\"cid_key\", cid).withColumn(\"ym_key\", ym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc315e40-b2d2-44b4-849e-98b14198f515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "July robust overlap → label↔fin: 0  label↔att: 0\n",
      "+----------+-------+\n",
      "|cid_key   |ym_key |\n",
      "+----------+-------+\n",
      "|CUS_0X53F4|2023-07|\n",
      "|CUS_0X69BE|2023-07|\n",
      "|CUS_0X1037|2023-07|\n",
      "|CUS_0X42C6|2023-07|\n",
      "|CUS_0X9BEF|2023-07|\n",
      "|CUS_0X71A0|2023-07|\n",
      "|CUS_0XA146|2023-07|\n",
      "|CUS_0XA2C0|2023-07|\n",
      "|CUS_0XC2E2|2023-07|\n",
      "|CUS_0X3BD0|2023-07|\n",
      "+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds  = \"2023-07-01\"\n",
    "tok = \"2023_07_01\"\n",
    "ym  = \"2023-07\"  # file month token\n",
    "\n",
    "lbl = spark.read.parquet(f\"datamart/gold/label_store/gold_label_store_{tok}.parquet\")\n",
    "fin = spark.read.parquet(f\"datamart/gold/features/financials/gold_financials_features_{tok}.parquet\")\n",
    "att = spark.read.parquet(f\"datamart/gold/features/attributes/gold_attributes_features_{tok}.parquet\")\n",
    "\n",
    "lblK = mk_keys(lbl, id_col=\"customer_id\", date_col=\"snapshot_date\", literal_ym=ym)\n",
    "finK = mk_keys(fin, id_col=\"customer_id\", date_col=\"snapshot_date\", literal_ym=ym)\n",
    "attK = mk_keys(att, id_col=\"customer_id\", date_col=\"snapshot_date\", literal_ym=ym)\n",
    "\n",
    "# Overlaps on robust keys (should be > 0 if customers align)\n",
    "ov_fin = (lblK.select(\"cid_key\",\"ym_key\").distinct()\n",
    "               .join(finK.select(\"cid_key\",\"ym_key\").distinct(), [\"cid_key\",\"ym_key\"], \"inner\").count())\n",
    "ov_att = (lblK.select(\"cid_key\",\"ym_key\").distinct()\n",
    "               .join(attK.select(\"cid_key\",\"ym_key\").distinct(), [\"cid_key\",\"ym_key\"], \"inner\").count())\n",
    "print(\"July robust overlap → label↔fin:\", ov_fin, \" label↔att:\", ov_att)\n",
    "\n",
    "# If still 0, show a few anti-join samples\n",
    "miss = (lblK.select(\"cid_key\",\"ym_key\").distinct()\n",
    "             .join(finK.select(\"cid_key\",\"ym_key\").distinct(), [\"cid_key\",\"ym_key\"], \"left_anti\"))\n",
    "miss.show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9594fac-1f6d-45b2-8dfa-4abdcfa07bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label↔Fin ANY-month overlap: 8974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 824:==================>                                    (8 + 16) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label↔Att ANY-month overlap: 8974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load all months at once\n",
    "lbl_all = spark.read.parquet(\"datamart/gold/label_store/gold_label_store_*.parquet\") \\\n",
    "    .withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "fin_all = spark.read.parquet(\"datamart/gold/features/financials/gold_financials_features_*.parquet\") \\\n",
    "    .withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "att_all = spark.read.parquet(\"datamart/gold/features/attributes/gold_attributes_features_*.parquet\") \\\n",
    "    .withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "\n",
    "def cid_key(col):\n",
    "    c = F.upper(F.trim(F.col(col)))\n",
    "    c = F.regexp_replace(c, r\"\\s+\", \"\")\n",
    "    c = F.regexp_replace(c, r\"[^A-Z0-9_]\", \"\")\n",
    "    return c\n",
    "\n",
    "lbl_ids = lbl_all.select(cid_key(\"customer_id\").alias(\"cid\")).distinct()\n",
    "fin_ids = fin_all.select(cid_key(\"customer_id\").alias(\"cid\")).distinct()\n",
    "att_ids = att_all.select(cid_key(\"customer_id\").alias(\"cid\")).distinct()\n",
    "\n",
    "print(\"Label↔Fin ANY-month overlap:\", lbl_ids.join(fin_ids, \"cid\", \"inner\").count())\n",
    "print(\"Label↔Att ANY-month overlap:\", lbl_ids.join(att_ids, \"cid\", \"inner\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd3002ab-e5ab-41e6-916c-5776990ed86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ASOF] wrote: datamart/gold/model_dataset_asof\n",
      "rows = 8974 cols = 59\n",
      "+-------------+-----+\n",
      "|snapshot_date|count|\n",
      "+-------------+-----+\n",
      "|2023-07-01   |530  |\n",
      "|2023-08-01   |501  |\n",
      "|2023-09-01   |506  |\n",
      "|2023-10-01   |510  |\n",
      "|2023-11-01   |521  |\n",
      "|2023-12-01   |517  |\n",
      "|2024-01-01   |471  |\n",
      "|2024-02-01   |481  |\n",
      "|2024-03-01   |454  |\n",
      "|2024-04-01   |487  |\n",
      "|2024-05-01   |491  |\n",
      "|2024-06-01   |489  |\n",
      "|2024-07-01   |485  |\n",
      "|2024-08-01   |518  |\n",
      "|2024-09-01   |511  |\n",
      "|2024-10-01   |513  |\n",
      "|2024-11-01   |491  |\n",
      "|2024-12-01   |498  |\n",
      "+-------------+-----+\n",
      "\n",
      "+-----------------+\n",
      "|dti_non_null_rate|\n",
      "+-----------------+\n",
      "|              1.0|\n",
      "+-----------------+\n",
      "\n",
      "+---------------------------+\n",
      "|emi_to_income_non_null_rate|\n",
      "+---------------------------+\n",
      "|                        1.0|\n",
      "+---------------------------+\n",
      "\n",
      "+--------------------------------------+\n",
      "|credit_utilization_ratio_non_null_rate|\n",
      "+--------------------------------------+\n",
      "|                                   1.0|\n",
      "+--------------------------------------+\n",
      "\n",
      "+------------------------------+\n",
      "|pay_behav_coarse_non_null_rate|\n",
      "+------------------------------+\n",
      "|                           1.0|\n",
      "+------------------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|util_bucket_non_null_rate|\n",
      "+-------------------------+\n",
      "|                      1.0|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# ---------- paths ----------\n",
    "LBL = \"datamart/gold/label_store/gold_label_store_*.parquet\"\n",
    "FIN = \"datamart/gold/features/financials/gold_financials_features_*.parquet\"\n",
    "ATT = \"datamart/gold/features/attributes/gold_attributes_features_*.parquet\"\n",
    "OUT = \"datamart/gold/model_dataset_asof\"\n",
    "\n",
    "# ---------- load & normalize ----------\n",
    "def cid_norm(c):\n",
    "    c = F.upper(F.trim(F.col(c)))\n",
    "    c = F.regexp_replace(c, r\"\\s+\", \"\")\n",
    "    c = F.regexp_replace(c, r\"[^A-Z0-9_]\", \"\")\n",
    "    return c\n",
    "\n",
    "lbl = (spark.read.parquet(LBL)\n",
    "       .withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "       .withColumn(\"customer_id\", cid_norm(\"customer_id\"))\n",
    "       .withColumn(\"label_date\", F.to_date(\"snapshot_date\"))\n",
    "       .select(\"customer_id\",\"label_date\",\"loan_id\",\"label\",\"label_def\"))\n",
    "\n",
    "fin = (spark.read.parquet(FIN)\n",
    "       .withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "       .withColumn(\"customer_id\", cid_norm(\"customer_id\"))\n",
    "       .withColumn(\"feat_date\",\n",
    "                   F.coalesce(\n",
    "                       F.to_date(\"snapshot_date\"),\n",
    "                       F.to_date(F.to_timestamp(\"snap_ym\"))  # guard if only snap_ym exists\n",
    "                   )))\n",
    "\n",
    "att = (spark.read.parquet(ATT)\n",
    "       .withColumnRenamed(\"Customer_ID\",\"customer_id\")\n",
    "       .withColumn(\"customer_id\", cid_norm(\"customer_id\"))\n",
    "       .withColumn(\"feat_date\",\n",
    "                   F.coalesce(\n",
    "                       F.to_date(\"snapshot_date\"),\n",
    "                       F.to_date(F.to_timestamp(\"snap_ym\"))\n",
    "                   )))\n",
    "\n",
    "# Keep only columns you actually want from fin/att to avoid duplicate names\n",
    "fin_keep = [c for c in fin.columns if c not in {\"snapshot_date\",\"snap_ym\",\"customer_id\",\"feat_date\"}]\n",
    "att_keep = [c for c in att.columns if c not in {\"snapshot_date\",\"snap_ym\",\"customer_id\",\"feat_date\"}]\n",
    "\n",
    "# ---------- AS-OF join: latest feature <= label_date ----------\n",
    "# 1) Join label with FIN on customer_id then filter to feat_date <= label_date\n",
    "fin_join = (lbl.join(F.broadcast(fin.select(\"customer_id\",\"feat_date\", *fin_keep)), \"customer_id\", \"left\")\n",
    "              .where(F.col(\"feat_date\").isNotNull() & (F.col(\"feat_date\") <= F.col(\"label_date\"))))\n",
    "\n",
    "w_fin = Window.partitionBy(\"customer_id\",\"label_date\").orderBy(F.col(\"feat_date\").desc())\n",
    "fin_ranked = fin_join.withColumn(\"rn\", F.row_number().over(w_fin)).where(F.col(\"rn\")==1).drop(\"rn\")\n",
    "\n",
    "# 2) Same for ATTR\n",
    "att_join = (lbl.join(F.broadcast(att.select(\"customer_id\",\"feat_date\", *att_keep)), \"customer_id\", \"left\")\n",
    "              .where(F.col(\"feat_date\").isNotNull() & (F.col(\"feat_date\") <= F.col(\"label_date\"))))\n",
    "\n",
    "w_att = Window.partitionBy(\"customer_id\",\"label_date\").orderBy(F.col(\"feat_date\").desc())\n",
    "att_ranked = att_join.withColumn(\"rn\", F.row_number().over(w_att)).where(F.col(\"rn\")==1).drop(\"rn\")\n",
    "\n",
    "# 3) Merge the two as-of results back onto *label* keys\n",
    "# safer drop: exclude only columns that clash, but KEEP join keys\n",
    "base_keys = [\"customer_id\", \"label_date\", \"loan_id\", \"label\", \"label_def\"]\n",
    "\n",
    "fin_clean = fin_ranked.drop(*[c for c in base_keys if c not in [\"customer_id\", \"label_date\"]])\n",
    "att_clean = att_ranked.drop(*[c for c in base_keys if c not in [\"customer_id\", \"label_date\"]])\n",
    "\n",
    "md_asof = (\n",
    "    lbl\n",
    "    .join(fin_clean, [\"customer_id\", \"label_date\"], \"left\")\n",
    "    .join(att_clean, [\"customer_id\", \"label_date\"], \"left\")\n",
    "    .drop(\"feat_date\")        # <--- remove duplicate helper columns\n",
    "    .withColumnRenamed(\"label_date\", \"snapshot_date\")\n",
    ")\n",
    "\n",
    "md_asof.write.mode(\"overwrite\").parquet(OUT)\n",
    "print(\"[ASOF] wrote:\", OUT)\n",
    "\n",
    "# ---------- quick sanity ----------\n",
    "md = spark.read.parquet(OUT)\n",
    "print(\"rows =\", md.count(), \"cols =\", len(md.columns))\n",
    "md.groupBy(\"snapshot_date\").count().orderBy(\"snapshot_date\").show(30, False)\n",
    "\n",
    "for c in [\"dti\",\"emi_to_income\",\"credit_utilization_ratio\",\"pay_behav_coarse\",\"util_bucket\"]:\n",
    "    if c in md.columns:\n",
    "        md.agg((1 - F.avg(F.col(c).isNull().cast(\"double\"))).alias(f\"{c}_non_null_rate\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13b6a208-ce30-46a3-974d-3849bf67c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 8974 cols = 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|snapshot_date|count|\n",
      "+-------------+-----+\n",
      "|2023-07-01   |530  |\n",
      "|2023-08-01   |501  |\n",
      "|2023-09-01   |506  |\n",
      "|2023-10-01   |510  |\n",
      "|2023-11-01   |521  |\n",
      "|2023-12-01   |517  |\n",
      "|2024-01-01   |471  |\n",
      "|2024-02-01   |481  |\n",
      "|2024-03-01   |454  |\n",
      "|2024-04-01   |487  |\n",
      "|2024-05-01   |491  |\n",
      "|2024-06-01   |489  |\n",
      "|2024-07-01   |485  |\n",
      "|2024-08-01   |518  |\n",
      "|2024-09-01   |511  |\n",
      "|2024-10-01   |513  |\n",
      "|2024-11-01   |491  |\n",
      "|2024-12-01   |498  |\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|dti_non_null_rate|\n",
      "+-----------------+\n",
      "|              1.0|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|emi_to_income_non_null_rate|\n",
      "+---------------------------+\n",
      "|                        1.0|\n",
      "+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|credit_utilization_ratio_non_null_rate|\n",
      "+--------------------------------------+\n",
      "|                                   1.0|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|pay_behav_coarse_non_null_rate|\n",
      "+------------------------------+\n",
      "|                           1.0|\n",
      "+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|util_bucket_non_null_rate|\n",
      "+-------------------------+\n",
      "|                      1.0|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>nulls</th>\n",
       "      <th>null_rate_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>credit_mix</td>\n",
       "      <td>1896</td>\n",
       "      <td>21.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>minpay_flag</td>\n",
       "      <td>1069</td>\n",
       "      <td>11.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>invest_to_income</td>\n",
       "      <td>396</td>\n",
       "      <td>4.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>emi_to_balance</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loan_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interest_rate</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>label_def</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>num_credit_inquiries</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>label</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snapshot_date</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>credit_utilization_ratio</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>credit_history_age</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_of_loan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>emi_to_income</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      column  nulls  null_rate_%\n",
       "7                 credit_mix   1896       21.128\n",
       "26               minpay_flag   1069       11.912\n",
       "14          invest_to_income    396        4.413\n",
       "13            emi_to_balance      1        0.011\n",
       "2                    loan_id      0        0.000\n",
       "5              interest_rate      0        0.000\n",
       "4                  label_def      0        0.000\n",
       "6       num_credit_inquiries      0        0.000\n",
       "3                      label      0        0.000\n",
       "1              snapshot_date      0        0.000\n",
       "0                customer_id      0        0.000\n",
       "10  credit_utilization_ratio      0        0.000\n",
       "9         credit_history_age      0        0.000\n",
       "8                num_of_loan      0        0.000\n",
       "12             emi_to_income      0        0.000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read the model datasets built by main.py\n",
    "md = spark.read.parquet(\"datamart/gold/model_dataset/gold_model_dataset_*.parquet\")\n",
    "\n",
    "print(\"rows =\", md.count(), \"cols =\", len(md.columns))\n",
    "md.groupBy(\"snapshot_date\").count().orderBy(\"snapshot_date\").show(30, False)\n",
    "\n",
    "# Check non-null ratio for a few key engineered features\n",
    "for c in [\"dti\",\"emi_to_income\",\"credit_utilization_ratio\",\"pay_behav_coarse\",\"util_bucket\"]:\n",
    "    if c in md.columns:\n",
    "        md.agg(\n",
    "            (1 - F.avg(F.col(c).isNull().cast(\"double\"))).alias(f\"{c}_non_null_rate\")\n",
    "        ).show()\n",
    "\n",
    "# Optional — find columns that still have any missing values\n",
    "total = md.count()\n",
    "nulls = (\n",
    "    md.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in md.columns])\n",
    "      .toPandas()\n",
    "      .T.reset_index()\n",
    "      .rename(columns={\"index\":\"column\", 0:\"nulls\"})\n",
    ")\n",
    "nulls[\"null_rate_%\"] = (nulls[\"nulls\"]/total*100).round(3)\n",
    "nulls.sort_values(\"null_rate_%\", ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aed2e6-5603-4ad1-bb0f-8d4e646d758b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
